commit c374a19215c339d1f847bc158efce780b734c116
Author: Michael Zeller <mike@mikezeller.net>
Date:   2019-06-19T22:13:48+00:00 (4 months ago)
    
    XXX - pre gerrit

diff --git a/.cargo/config b/.cargo/config
new file mode 100644
index 0000000..4ab32f9
--- /dev/null
+++ b/.cargo/config
@@ -0,0 +1 @@
+RUSTFLAGS = "-C force-frame-pointers"
diff --git a/.gitignore b/.gitignore
new file mode 100644
index 0000000..5f07233
--- /dev/null
+++ b/.gitignore
@@ -0,0 +1,27 @@
+# Generated by Cargo
+# will have compiled files and executables
+/build/
+target
+
+# These are backup files generated by rustfmt
+**/*.rs.bk
+**/node_modules
+
+# The usual Joyent agent stuff
+/make_stamps
+/tmp
+/cache/
+bits
+bin
+/make_stamps
+docs/*.json
+docs/*.html
+cscope.in.out
+cscope.po.out
+cscope.out
+man
+smf/manifests/bapi.xml
+*.vim
+*.swp
+
+
diff --git a/.gitmodules b/.gitmodules
new file mode 100644
index 0000000..6cbac63
--- /dev/null
+++ b/.gitmodules
@@ -0,0 +1,3 @@
+[submodule "deps/eng"]
+	path = deps/eng
+	url = https://github.com/joyent/eng.git
diff --git a/Cargo.lock b/Cargo.lock
new file mode 100644
index 0000000..c982ab6
--- /dev/null
+++ b/Cargo.lock
@@ -0,0 +1,1299 @@
+[[package]]
+name = "aho-corasick"
+version = "0.7.3"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+dependencies = [
+ "memchr 2.2.0 (registry+https://github.com/rust-lang/crates.io-index)",
+]
+
+[[package]]
+name = "arc-swap"
+version = "0.3.11"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+
+[[package]]
+name = "arrayvec"
+version = "0.4.10"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+dependencies = [
+ "nodrop 0.1.13 (registry+https://github.com/rust-lang/crates.io-index)",
+]
+
+[[package]]
+name = "atty"
+version = "0.2.11"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+dependencies = [
+ "libc 0.2.58 (registry+https://github.com/rust-lang/crates.io-index)",
+ "termion 1.5.3 (registry+https://github.com/rust-lang/crates.io-index)",
+ "winapi 0.3.7 (registry+https://github.com/rust-lang/crates.io-index)",
+]
+
+[[package]]
+name = "autocfg"
+version = "0.1.4"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+
+[[package]]
+name = "backtrace"
+version = "0.3.30"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+dependencies = [
+ "autocfg 0.1.4 (registry+https://github.com/rust-lang/crates.io-index)",
+ "backtrace-sys 0.1.28 (registry+https://github.com/rust-lang/crates.io-index)",
+ "cfg-if 0.1.9 (registry+https://github.com/rust-lang/crates.io-index)",
+ "libc 0.2.58 (registry+https://github.com/rust-lang/crates.io-index)",
+ "rustc-demangle 0.1.15 (registry+https://github.com/rust-lang/crates.io-index)",
+]
+
+[[package]]
+name = "backtrace-sys"
+version = "0.1.28"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+dependencies = [
+ "cc 1.0.37 (registry+https://github.com/rust-lang/crates.io-index)",
+ "libc 0.2.58 (registry+https://github.com/rust-lang/crates.io-index)",
+]
+
+[[package]]
+name = "bitflags"
+version = "1.1.0"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+
+[[package]]
+name = "byteorder"
+version = "1.3.2"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+
+[[package]]
+name = "bytes"
+version = "0.4.12"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+dependencies = [
+ "byteorder 1.3.2 (registry+https://github.com/rust-lang/crates.io-index)",
+ "either 1.5.2 (registry+https://github.com/rust-lang/crates.io-index)",
+ "iovec 0.1.2 (registry+https://github.com/rust-lang/crates.io-index)",
+]
+
+[[package]]
+name = "cc"
+version = "1.0.37"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+
+[[package]]
+name = "cfg-if"
+version = "0.1.9"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+
+[[package]]
+name = "cfwlogd"
+version = "0.1.0"
+dependencies = [
+ "bytes 0.4.12 (registry+https://github.com/rust-lang/crates.io-index)",
+ "chrono 0.4.6 (registry+https://github.com/rust-lang/crates.io-index)",
+ "crossbeam 0.7.1 (registry+https://github.com/rust-lang/crates.io-index)",
+ "failure 0.1.5 (registry+https://github.com/rust-lang/crates.io-index)",
+ "libc 0.2.58 (registry+https://github.com/rust-lang/crates.io-index)",
+ "log 0.4.6 (registry+https://github.com/rust-lang/crates.io-index)",
+ "nom 4.2.3 (registry+https://github.com/rust-lang/crates.io-index)",
+ "pretty_env_logger 0.3.0 (registry+https://github.com/rust-lang/crates.io-index)",
+ "serde 1.0.92 (registry+https://github.com/rust-lang/crates.io-index)",
+ "serde_json 1.0.39 (registry+https://github.com/rust-lang/crates.io-index)",
+ "signal-hook 0.1.9 (registry+https://github.com/rust-lang/crates.io-index)",
+ "testutils 0.1.0",
+ "uuid 0.7.4 (registry+https://github.com/rust-lang/crates.io-index)",
+ "vminfod 0.1.0",
+]
+
+[[package]]
+name = "chrono"
+version = "0.4.6"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+dependencies = [
+ "num-integer 0.1.41 (registry+https://github.com/rust-lang/crates.io-index)",
+ "num-traits 0.2.8 (registry+https://github.com/rust-lang/crates.io-index)",
+ "serde 1.0.92 (registry+https://github.com/rust-lang/crates.io-index)",
+ "time 0.1.42 (registry+https://github.com/rust-lang/crates.io-index)",
+]
+
+[[package]]
+name = "cloudabi"
+version = "0.0.3"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+dependencies = [
+ "bitflags 1.1.0 (registry+https://github.com/rust-lang/crates.io-index)",
+]
+
+[[package]]
+name = "crossbeam"
+version = "0.7.1"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+dependencies = [
+ "cfg-if 0.1.9 (registry+https://github.com/rust-lang/crates.io-index)",
+ "crossbeam-channel 0.3.8 (registry+https://github.com/rust-lang/crates.io-index)",
+ "crossbeam-deque 0.7.1 (registry+https://github.com/rust-lang/crates.io-index)",
+ "crossbeam-epoch 0.7.1 (registry+https://github.com/rust-lang/crates.io-index)",
+ "crossbeam-queue 0.1.2 (registry+https://github.com/rust-lang/crates.io-index)",
+ "crossbeam-utils 0.6.5 (registry+https://github.com/rust-lang/crates.io-index)",
+]
+
+[[package]]
+name = "crossbeam-channel"
+version = "0.3.8"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+dependencies = [
+ "crossbeam-utils 0.6.5 (registry+https://github.com/rust-lang/crates.io-index)",
+ "smallvec 0.6.10 (registry+https://github.com/rust-lang/crates.io-index)",
+]
+
+[[package]]
+name = "crossbeam-deque"
+version = "0.7.1"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+dependencies = [
+ "crossbeam-epoch 0.7.1 (registry+https://github.com/rust-lang/crates.io-index)",
+ "crossbeam-utils 0.6.5 (registry+https://github.com/rust-lang/crates.io-index)",
+]
+
+[[package]]
+name = "crossbeam-epoch"
+version = "0.7.1"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+dependencies = [
+ "arrayvec 0.4.10 (registry+https://github.com/rust-lang/crates.io-index)",
+ "cfg-if 0.1.9 (registry+https://github.com/rust-lang/crates.io-index)",
+ "crossbeam-utils 0.6.5 (registry+https://github.com/rust-lang/crates.io-index)",
+ "lazy_static 1.3.0 (registry+https://github.com/rust-lang/crates.io-index)",
+ "memoffset 0.2.1 (registry+https://github.com/rust-lang/crates.io-index)",
+ "scopeguard 0.3.3 (registry+https://github.com/rust-lang/crates.io-index)",
+]
+
+[[package]]
+name = "crossbeam-queue"
+version = "0.1.2"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+dependencies = [
+ "crossbeam-utils 0.6.5 (registry+https://github.com/rust-lang/crates.io-index)",
+]
+
+[[package]]
+name = "crossbeam-utils"
+version = "0.6.5"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+dependencies = [
+ "cfg-if 0.1.9 (registry+https://github.com/rust-lang/crates.io-index)",
+ "lazy_static 1.3.0 (registry+https://github.com/rust-lang/crates.io-index)",
+]
+
+[[package]]
+name = "either"
+version = "1.5.2"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+
+[[package]]
+name = "env_logger"
+version = "0.6.1"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+dependencies = [
+ "atty 0.2.11 (registry+https://github.com/rust-lang/crates.io-index)",
+ "humantime 1.2.0 (registry+https://github.com/rust-lang/crates.io-index)",
+ "log 0.4.6 (registry+https://github.com/rust-lang/crates.io-index)",
+ "regex 1.1.7 (registry+https://github.com/rust-lang/crates.io-index)",
+ "termcolor 1.0.5 (registry+https://github.com/rust-lang/crates.io-index)",
+]
+
+[[package]]
+name = "failure"
+version = "0.1.5"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+dependencies = [
+ "backtrace 0.3.30 (registry+https://github.com/rust-lang/crates.io-index)",
+ "failure_derive 0.1.5 (registry+https://github.com/rust-lang/crates.io-index)",
+]
+
+[[package]]
+name = "failure_derive"
+version = "0.1.5"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+dependencies = [
+ "proc-macro2 0.4.30 (registry+https://github.com/rust-lang/crates.io-index)",
+ "quote 0.6.12 (registry+https://github.com/rust-lang/crates.io-index)",
+ "syn 0.15.36 (registry+https://github.com/rust-lang/crates.io-index)",
+ "synstructure 0.10.2 (registry+https://github.com/rust-lang/crates.io-index)",
+]
+
+[[package]]
+name = "fnv"
+version = "1.0.6"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+
+[[package]]
+name = "fuchsia-cprng"
+version = "0.1.1"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+
+[[package]]
+name = "fuchsia-zircon"
+version = "0.3.3"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+dependencies = [
+ "bitflags 1.1.0 (registry+https://github.com/rust-lang/crates.io-index)",
+ "fuchsia-zircon-sys 0.3.3 (registry+https://github.com/rust-lang/crates.io-index)",
+]
+
+[[package]]
+name = "fuchsia-zircon-sys"
+version = "0.3.3"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+
+[[package]]
+name = "futures"
+version = "0.1.27"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+
+[[package]]
+name = "futures-cpupool"
+version = "0.1.8"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+dependencies = [
+ "futures 0.1.27 (registry+https://github.com/rust-lang/crates.io-index)",
+ "num_cpus 1.10.1 (registry+https://github.com/rust-lang/crates.io-index)",
+]
+
+[[package]]
+name = "h2"
+version = "0.1.24"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+dependencies = [
+ "byteorder 1.3.2 (registry+https://github.com/rust-lang/crates.io-index)",
+ "bytes 0.4.12 (registry+https://github.com/rust-lang/crates.io-index)",
+ "fnv 1.0.6 (registry+https://github.com/rust-lang/crates.io-index)",
+ "futures 0.1.27 (registry+https://github.com/rust-lang/crates.io-index)",
+ "http 0.1.17 (registry+https://github.com/rust-lang/crates.io-index)",
+ "indexmap 1.0.2 (registry+https://github.com/rust-lang/crates.io-index)",
+ "log 0.4.6 (registry+https://github.com/rust-lang/crates.io-index)",
+ "slab 0.4.2 (registry+https://github.com/rust-lang/crates.io-index)",
+ "string 0.2.0 (registry+https://github.com/rust-lang/crates.io-index)",
+ "tokio-io 0.1.12 (registry+https://github.com/rust-lang/crates.io-index)",
+]
+
+[[package]]
+name = "http"
+version = "0.1.17"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+dependencies = [
+ "bytes 0.4.12 (registry+https://github.com/rust-lang/crates.io-index)",
+ "fnv 1.0.6 (registry+https://github.com/rust-lang/crates.io-index)",
+ "itoa 0.4.4 (registry+https://github.com/rust-lang/crates.io-index)",
+]
+
+[[package]]
+name = "http-body"
+version = "0.1.0"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+dependencies = [
+ "bytes 0.4.12 (registry+https://github.com/rust-lang/crates.io-index)",
+ "futures 0.1.27 (registry+https://github.com/rust-lang/crates.io-index)",
+ "http 0.1.17 (registry+https://github.com/rust-lang/crates.io-index)",
+ "tokio-buf 0.1.1 (registry+https://github.com/rust-lang/crates.io-index)",
+]
+
+[[package]]
+name = "httparse"
+version = "1.3.3"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+
+[[package]]
+name = "humantime"
+version = "1.2.0"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+dependencies = [
+ "quick-error 1.2.2 (registry+https://github.com/rust-lang/crates.io-index)",
+]
+
+[[package]]
+name = "hyper"
+version = "0.12.30"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+dependencies = [
+ "bytes 0.4.12 (registry+https://github.com/rust-lang/crates.io-index)",
+ "futures 0.1.27 (registry+https://github.com/rust-lang/crates.io-index)",
+ "futures-cpupool 0.1.8 (registry+https://github.com/rust-lang/crates.io-index)",
+ "h2 0.1.24 (registry+https://github.com/rust-lang/crates.io-index)",
+ "http 0.1.17 (registry+https://github.com/rust-lang/crates.io-index)",
+ "http-body 0.1.0 (registry+https://github.com/rust-lang/crates.io-index)",
+ "httparse 1.3.3 (registry+https://github.com/rust-lang/crates.io-index)",
+ "iovec 0.1.2 (registry+https://github.com/rust-lang/crates.io-index)",
+ "itoa 0.4.4 (registry+https://github.com/rust-lang/crates.io-index)",
+ "log 0.4.6 (registry+https://github.com/rust-lang/crates.io-index)",
+ "net2 0.2.33 (registry+https://github.com/rust-lang/crates.io-index)",
+ "rustc_version 0.2.3 (registry+https://github.com/rust-lang/crates.io-index)",
+ "time 0.1.42 (registry+https://github.com/rust-lang/crates.io-index)",
+ "tokio 0.1.21 (registry+https://github.com/rust-lang/crates.io-index)",
+ "tokio-buf 0.1.1 (registry+https://github.com/rust-lang/crates.io-index)",
+ "tokio-executor 0.1.7 (registry+https://github.com/rust-lang/crates.io-index)",
+ "tokio-io 0.1.12 (registry+https://github.com/rust-lang/crates.io-index)",
+ "tokio-reactor 0.1.9 (registry+https://github.com/rust-lang/crates.io-index)",
+ "tokio-tcp 0.1.3 (registry+https://github.com/rust-lang/crates.io-index)",
+ "tokio-threadpool 0.1.14 (registry+https://github.com/rust-lang/crates.io-index)",
+ "tokio-timer 0.2.11 (registry+https://github.com/rust-lang/crates.io-index)",
+ "want 0.0.6 (registry+https://github.com/rust-lang/crates.io-index)",
+]
+
+[[package]]
+name = "indexmap"
+version = "1.0.2"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+
+[[package]]
+name = "iovec"
+version = "0.1.2"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+dependencies = [
+ "libc 0.2.58 (registry+https://github.com/rust-lang/crates.io-index)",
+ "winapi 0.2.8 (registry+https://github.com/rust-lang/crates.io-index)",
+]
+
+[[package]]
+name = "itoa"
+version = "0.4.4"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+
+[[package]]
+name = "kernel32-sys"
+version = "0.2.2"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+dependencies = [
+ "winapi 0.2.8 (registry+https://github.com/rust-lang/crates.io-index)",
+ "winapi-build 0.1.1 (registry+https://github.com/rust-lang/crates.io-index)",
+]
+
+[[package]]
+name = "lazy_static"
+version = "1.3.0"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+
+[[package]]
+name = "libc"
+version = "0.2.58"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+
+[[package]]
+name = "lock_api"
+version = "0.1.5"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+dependencies = [
+ "owning_ref 0.4.0 (registry+https://github.com/rust-lang/crates.io-index)",
+ "scopeguard 0.3.3 (registry+https://github.com/rust-lang/crates.io-index)",
+]
+
+[[package]]
+name = "log"
+version = "0.4.6"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+dependencies = [
+ "cfg-if 0.1.9 (registry+https://github.com/rust-lang/crates.io-index)",
+]
+
+[[package]]
+name = "memchr"
+version = "2.2.0"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+
+[[package]]
+name = "memoffset"
+version = "0.2.1"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+
+[[package]]
+name = "mio"
+version = "0.6.19"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+dependencies = [
+ "fuchsia-zircon 0.3.3 (registry+https://github.com/rust-lang/crates.io-index)",
+ "fuchsia-zircon-sys 0.3.3 (registry+https://github.com/rust-lang/crates.io-index)",
+ "iovec 0.1.2 (registry+https://github.com/rust-lang/crates.io-index)",
+ "kernel32-sys 0.2.2 (registry+https://github.com/rust-lang/crates.io-index)",
+ "libc 0.2.58 (registry+https://github.com/rust-lang/crates.io-index)",
+ "log 0.4.6 (registry+https://github.com/rust-lang/crates.io-index)",
+ "miow 0.2.1 (registry+https://github.com/rust-lang/crates.io-index)",
+ "net2 0.2.33 (registry+https://github.com/rust-lang/crates.io-index)",
+ "slab 0.4.2 (registry+https://github.com/rust-lang/crates.io-index)",
+ "winapi 0.2.8 (registry+https://github.com/rust-lang/crates.io-index)",
+]
+
+[[package]]
+name = "mio-uds"
+version = "0.6.7"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+dependencies = [
+ "iovec 0.1.2 (registry+https://github.com/rust-lang/crates.io-index)",
+ "libc 0.2.58 (registry+https://github.com/rust-lang/crates.io-index)",
+ "mio 0.6.19 (registry+https://github.com/rust-lang/crates.io-index)",
+]
+
+[[package]]
+name = "miow"
+version = "0.2.1"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+dependencies = [
+ "kernel32-sys 0.2.2 (registry+https://github.com/rust-lang/crates.io-index)",
+ "net2 0.2.33 (registry+https://github.com/rust-lang/crates.io-index)",
+ "winapi 0.2.8 (registry+https://github.com/rust-lang/crates.io-index)",
+ "ws2_32-sys 0.2.1 (registry+https://github.com/rust-lang/crates.io-index)",
+]
+
+[[package]]
+name = "net2"
+version = "0.2.33"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+dependencies = [
+ "cfg-if 0.1.9 (registry+https://github.com/rust-lang/crates.io-index)",
+ "libc 0.2.58 (registry+https://github.com/rust-lang/crates.io-index)",
+ "winapi 0.3.7 (registry+https://github.com/rust-lang/crates.io-index)",
+]
+
+[[package]]
+name = "nodrop"
+version = "0.1.13"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+
+[[package]]
+name = "nom"
+version = "4.2.3"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+dependencies = [
+ "memchr 2.2.0 (registry+https://github.com/rust-lang/crates.io-index)",
+ "version_check 0.1.5 (registry+https://github.com/rust-lang/crates.io-index)",
+]
+
+[[package]]
+name = "num-integer"
+version = "0.1.41"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+dependencies = [
+ "autocfg 0.1.4 (registry+https://github.com/rust-lang/crates.io-index)",
+ "num-traits 0.2.8 (registry+https://github.com/rust-lang/crates.io-index)",
+]
+
+[[package]]
+name = "num-traits"
+version = "0.2.8"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+dependencies = [
+ "autocfg 0.1.4 (registry+https://github.com/rust-lang/crates.io-index)",
+]
+
+[[package]]
+name = "num_cpus"
+version = "1.10.1"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+dependencies = [
+ "libc 0.2.58 (registry+https://github.com/rust-lang/crates.io-index)",
+]
+
+[[package]]
+name = "numtoa"
+version = "0.1.0"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+
+[[package]]
+name = "owning_ref"
+version = "0.4.0"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+dependencies = [
+ "stable_deref_trait 1.1.1 (registry+https://github.com/rust-lang/crates.io-index)",
+]
+
+[[package]]
+name = "parking_lot"
+version = "0.7.1"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+dependencies = [
+ "lock_api 0.1.5 (registry+https://github.com/rust-lang/crates.io-index)",
+ "parking_lot_core 0.4.0 (registry+https://github.com/rust-lang/crates.io-index)",
+]
+
+[[package]]
+name = "parking_lot_core"
+version = "0.4.0"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+dependencies = [
+ "libc 0.2.58 (registry+https://github.com/rust-lang/crates.io-index)",
+ "rand 0.6.5 (registry+https://github.com/rust-lang/crates.io-index)",
+ "rustc_version 0.2.3 (registry+https://github.com/rust-lang/crates.io-index)",
+ "smallvec 0.6.10 (registry+https://github.com/rust-lang/crates.io-index)",
+ "winapi 0.3.7 (registry+https://github.com/rust-lang/crates.io-index)",
+]
+
+[[package]]
+name = "pretty_env_logger"
+version = "0.3.0"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+dependencies = [
+ "chrono 0.4.6 (registry+https://github.com/rust-lang/crates.io-index)",
+ "env_logger 0.6.1 (registry+https://github.com/rust-lang/crates.io-index)",
+ "log 0.4.6 (registry+https://github.com/rust-lang/crates.io-index)",
+]
+
+[[package]]
+name = "proc-macro2"
+version = "0.4.30"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+dependencies = [
+ "unicode-xid 0.1.0 (registry+https://github.com/rust-lang/crates.io-index)",
+]
+
+[[package]]
+name = "quick-error"
+version = "1.2.2"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+
+[[package]]
+name = "quote"
+version = "0.6.12"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+dependencies = [
+ "proc-macro2 0.4.30 (registry+https://github.com/rust-lang/crates.io-index)",
+]
+
+[[package]]
+name = "rand"
+version = "0.6.5"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+dependencies = [
+ "autocfg 0.1.4 (registry+https://github.com/rust-lang/crates.io-index)",
+ "libc 0.2.58 (registry+https://github.com/rust-lang/crates.io-index)",
+ "rand_chacha 0.1.1 (registry+https://github.com/rust-lang/crates.io-index)",
+ "rand_core 0.4.0 (registry+https://github.com/rust-lang/crates.io-index)",
+ "rand_hc 0.1.0 (registry+https://github.com/rust-lang/crates.io-index)",
+ "rand_isaac 0.1.1 (registry+https://github.com/rust-lang/crates.io-index)",
+ "rand_jitter 0.1.4 (registry+https://github.com/rust-lang/crates.io-index)",
+ "rand_os 0.1.3 (registry+https://github.com/rust-lang/crates.io-index)",
+ "rand_pcg 0.1.2 (registry+https://github.com/rust-lang/crates.io-index)",
+ "rand_xorshift 0.1.1 (registry+https://github.com/rust-lang/crates.io-index)",
+ "winapi 0.3.7 (registry+https://github.com/rust-lang/crates.io-index)",
+]
+
+[[package]]
+name = "rand_chacha"
+version = "0.1.1"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+dependencies = [
+ "autocfg 0.1.4 (registry+https://github.com/rust-lang/crates.io-index)",
+ "rand_core 0.3.1 (registry+https://github.com/rust-lang/crates.io-index)",
+]
+
+[[package]]
+name = "rand_core"
+version = "0.3.1"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+dependencies = [
+ "rand_core 0.4.0 (registry+https://github.com/rust-lang/crates.io-index)",
+]
+
+[[package]]
+name = "rand_core"
+version = "0.4.0"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+
+[[package]]
+name = "rand_hc"
+version = "0.1.0"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+dependencies = [
+ "rand_core 0.3.1 (registry+https://github.com/rust-lang/crates.io-index)",
+]
+
+[[package]]
+name = "rand_isaac"
+version = "0.1.1"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+dependencies = [
+ "rand_core 0.3.1 (registry+https://github.com/rust-lang/crates.io-index)",
+]
+
+[[package]]
+name = "rand_jitter"
+version = "0.1.4"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+dependencies = [
+ "libc 0.2.58 (registry+https://github.com/rust-lang/crates.io-index)",
+ "rand_core 0.4.0 (registry+https://github.com/rust-lang/crates.io-index)",
+ "winapi 0.3.7 (registry+https://github.com/rust-lang/crates.io-index)",
+]
+
+[[package]]
+name = "rand_os"
+version = "0.1.3"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+dependencies = [
+ "cloudabi 0.0.3 (registry+https://github.com/rust-lang/crates.io-index)",
+ "fuchsia-cprng 0.1.1 (registry+https://github.com/rust-lang/crates.io-index)",
+ "libc 0.2.58 (registry+https://github.com/rust-lang/crates.io-index)",
+ "rand_core 0.4.0 (registry+https://github.com/rust-lang/crates.io-index)",
+ "rdrand 0.4.0 (registry+https://github.com/rust-lang/crates.io-index)",
+ "winapi 0.3.7 (registry+https://github.com/rust-lang/crates.io-index)",
+]
+
+[[package]]
+name = "rand_pcg"
+version = "0.1.2"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+dependencies = [
+ "autocfg 0.1.4 (registry+https://github.com/rust-lang/crates.io-index)",
+ "rand_core 0.4.0 (registry+https://github.com/rust-lang/crates.io-index)",
+]
+
+[[package]]
+name = "rand_xorshift"
+version = "0.1.1"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+dependencies = [
+ "rand_core 0.3.1 (registry+https://github.com/rust-lang/crates.io-index)",
+]
+
+[[package]]
+name = "rdrand"
+version = "0.4.0"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+dependencies = [
+ "rand_core 0.3.1 (registry+https://github.com/rust-lang/crates.io-index)",
+]
+
+[[package]]
+name = "redox_syscall"
+version = "0.1.54"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+
+[[package]]
+name = "redox_termios"
+version = "0.1.1"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+dependencies = [
+ "redox_syscall 0.1.54 (registry+https://github.com/rust-lang/crates.io-index)",
+]
+
+[[package]]
+name = "regex"
+version = "1.1.7"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+dependencies = [
+ "aho-corasick 0.7.3 (registry+https://github.com/rust-lang/crates.io-index)",
+ "memchr 2.2.0 (registry+https://github.com/rust-lang/crates.io-index)",
+ "regex-syntax 0.6.7 (registry+https://github.com/rust-lang/crates.io-index)",
+ "thread_local 0.3.6 (registry+https://github.com/rust-lang/crates.io-index)",
+ "utf8-ranges 1.0.3 (registry+https://github.com/rust-lang/crates.io-index)",
+]
+
+[[package]]
+name = "regex-syntax"
+version = "0.6.7"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+dependencies = [
+ "ucd-util 0.1.3 (registry+https://github.com/rust-lang/crates.io-index)",
+]
+
+[[package]]
+name = "rustc-demangle"
+version = "0.1.15"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+
+[[package]]
+name = "rustc_version"
+version = "0.2.3"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+dependencies = [
+ "semver 0.9.0 (registry+https://github.com/rust-lang/crates.io-index)",
+]
+
+[[package]]
+name = "ryu"
+version = "0.2.8"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+
+[[package]]
+name = "scopeguard"
+version = "0.3.3"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+
+[[package]]
+name = "semver"
+version = "0.9.0"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+dependencies = [
+ "semver-parser 0.7.0 (registry+https://github.com/rust-lang/crates.io-index)",
+]
+
+[[package]]
+name = "semver-parser"
+version = "0.7.0"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+
+[[package]]
+name = "serde"
+version = "1.0.92"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+dependencies = [
+ "serde_derive 1.0.92 (registry+https://github.com/rust-lang/crates.io-index)",
+]
+
+[[package]]
+name = "serde_derive"
+version = "1.0.92"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+dependencies = [
+ "proc-macro2 0.4.30 (registry+https://github.com/rust-lang/crates.io-index)",
+ "quote 0.6.12 (registry+https://github.com/rust-lang/crates.io-index)",
+ "syn 0.15.36 (registry+https://github.com/rust-lang/crates.io-index)",
+]
+
+[[package]]
+name = "serde_json"
+version = "1.0.39"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+dependencies = [
+ "itoa 0.4.4 (registry+https://github.com/rust-lang/crates.io-index)",
+ "ryu 0.2.8 (registry+https://github.com/rust-lang/crates.io-index)",
+ "serde 1.0.92 (registry+https://github.com/rust-lang/crates.io-index)",
+]
+
+[[package]]
+name = "signal-hook"
+version = "0.1.9"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+dependencies = [
+ "libc 0.2.58 (registry+https://github.com/rust-lang/crates.io-index)",
+ "signal-hook-registry 1.0.1 (registry+https://github.com/rust-lang/crates.io-index)",
+]
+
+[[package]]
+name = "signal-hook-registry"
+version = "1.0.1"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+dependencies = [
+ "arc-swap 0.3.11 (registry+https://github.com/rust-lang/crates.io-index)",
+ "libc 0.2.58 (registry+https://github.com/rust-lang/crates.io-index)",
+]
+
+[[package]]
+name = "slab"
+version = "0.4.2"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+
+[[package]]
+name = "smallvec"
+version = "0.6.10"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+
+[[package]]
+name = "stable_deref_trait"
+version = "1.1.1"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+
+[[package]]
+name = "string"
+version = "0.2.0"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+dependencies = [
+ "bytes 0.4.12 (registry+https://github.com/rust-lang/crates.io-index)",
+]
+
+[[package]]
+name = "syn"
+version = "0.15.36"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+dependencies = [
+ "proc-macro2 0.4.30 (registry+https://github.com/rust-lang/crates.io-index)",
+ "quote 0.6.12 (registry+https://github.com/rust-lang/crates.io-index)",
+ "unicode-xid 0.1.0 (registry+https://github.com/rust-lang/crates.io-index)",
+]
+
+[[package]]
+name = "synstructure"
+version = "0.10.2"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+dependencies = [
+ "proc-macro2 0.4.30 (registry+https://github.com/rust-lang/crates.io-index)",
+ "quote 0.6.12 (registry+https://github.com/rust-lang/crates.io-index)",
+ "syn 0.15.36 (registry+https://github.com/rust-lang/crates.io-index)",
+ "unicode-xid 0.1.0 (registry+https://github.com/rust-lang/crates.io-index)",
+]
+
+[[package]]
+name = "termcolor"
+version = "1.0.5"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+dependencies = [
+ "wincolor 1.0.1 (registry+https://github.com/rust-lang/crates.io-index)",
+]
+
+[[package]]
+name = "termion"
+version = "1.5.3"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+dependencies = [
+ "libc 0.2.58 (registry+https://github.com/rust-lang/crates.io-index)",
+ "numtoa 0.1.0 (registry+https://github.com/rust-lang/crates.io-index)",
+ "redox_syscall 0.1.54 (registry+https://github.com/rust-lang/crates.io-index)",
+ "redox_termios 0.1.1 (registry+https://github.com/rust-lang/crates.io-index)",
+]
+
+[[package]]
+name = "testutils"
+version = "0.1.0"
+dependencies = [
+ "chrono 0.4.6 (registry+https://github.com/rust-lang/crates.io-index)",
+ "rand 0.6.5 (registry+https://github.com/rust-lang/crates.io-index)",
+ "uuid 0.7.4 (registry+https://github.com/rust-lang/crates.io-index)",
+ "vminfod 0.1.0",
+]
+
+[[package]]
+name = "thread_local"
+version = "0.3.6"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+dependencies = [
+ "lazy_static 1.3.0 (registry+https://github.com/rust-lang/crates.io-index)",
+]
+
+[[package]]
+name = "time"
+version = "0.1.42"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+dependencies = [
+ "libc 0.2.58 (registry+https://github.com/rust-lang/crates.io-index)",
+ "redox_syscall 0.1.54 (registry+https://github.com/rust-lang/crates.io-index)",
+ "winapi 0.3.7 (registry+https://github.com/rust-lang/crates.io-index)",
+]
+
+[[package]]
+name = "tokio"
+version = "0.1.21"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+dependencies = [
+ "bytes 0.4.12 (registry+https://github.com/rust-lang/crates.io-index)",
+ "futures 0.1.27 (registry+https://github.com/rust-lang/crates.io-index)",
+ "mio 0.6.19 (registry+https://github.com/rust-lang/crates.io-index)",
+ "num_cpus 1.10.1 (registry+https://github.com/rust-lang/crates.io-index)",
+ "tokio-codec 0.1.1 (registry+https://github.com/rust-lang/crates.io-index)",
+ "tokio-current-thread 0.1.6 (registry+https://github.com/rust-lang/crates.io-index)",
+ "tokio-executor 0.1.7 (registry+https://github.com/rust-lang/crates.io-index)",
+ "tokio-fs 0.1.6 (registry+https://github.com/rust-lang/crates.io-index)",
+ "tokio-io 0.1.12 (registry+https://github.com/rust-lang/crates.io-index)",
+ "tokio-reactor 0.1.9 (registry+https://github.com/rust-lang/crates.io-index)",
+ "tokio-sync 0.1.6 (registry+https://github.com/rust-lang/crates.io-index)",
+ "tokio-tcp 0.1.3 (registry+https://github.com/rust-lang/crates.io-index)",
+ "tokio-threadpool 0.1.14 (registry+https://github.com/rust-lang/crates.io-index)",
+ "tokio-timer 0.2.11 (registry+https://github.com/rust-lang/crates.io-index)",
+ "tokio-trace-core 0.2.0 (registry+https://github.com/rust-lang/crates.io-index)",
+ "tokio-udp 0.1.3 (registry+https://github.com/rust-lang/crates.io-index)",
+ "tokio-uds 0.2.5 (registry+https://github.com/rust-lang/crates.io-index)",
+]
+
+[[package]]
+name = "tokio-buf"
+version = "0.1.1"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+dependencies = [
+ "bytes 0.4.12 (registry+https://github.com/rust-lang/crates.io-index)",
+ "either 1.5.2 (registry+https://github.com/rust-lang/crates.io-index)",
+ "futures 0.1.27 (registry+https://github.com/rust-lang/crates.io-index)",
+]
+
+[[package]]
+name = "tokio-codec"
+version = "0.1.1"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+dependencies = [
+ "bytes 0.4.12 (registry+https://github.com/rust-lang/crates.io-index)",
+ "futures 0.1.27 (registry+https://github.com/rust-lang/crates.io-index)",
+ "tokio-io 0.1.12 (registry+https://github.com/rust-lang/crates.io-index)",
+]
+
+[[package]]
+name = "tokio-current-thread"
+version = "0.1.6"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+dependencies = [
+ "futures 0.1.27 (registry+https://github.com/rust-lang/crates.io-index)",
+ "tokio-executor 0.1.7 (registry+https://github.com/rust-lang/crates.io-index)",
+]
+
+[[package]]
+name = "tokio-executor"
+version = "0.1.7"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+dependencies = [
+ "crossbeam-utils 0.6.5 (registry+https://github.com/rust-lang/crates.io-index)",
+ "futures 0.1.27 (registry+https://github.com/rust-lang/crates.io-index)",
+]
+
+[[package]]
+name = "tokio-fs"
+version = "0.1.6"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+dependencies = [
+ "futures 0.1.27 (registry+https://github.com/rust-lang/crates.io-index)",
+ "tokio-io 0.1.12 (registry+https://github.com/rust-lang/crates.io-index)",
+ "tokio-threadpool 0.1.14 (registry+https://github.com/rust-lang/crates.io-index)",
+]
+
+[[package]]
+name = "tokio-io"
+version = "0.1.12"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+dependencies = [
+ "bytes 0.4.12 (registry+https://github.com/rust-lang/crates.io-index)",
+ "futures 0.1.27 (registry+https://github.com/rust-lang/crates.io-index)",
+ "log 0.4.6 (registry+https://github.com/rust-lang/crates.io-index)",
+]
+
+[[package]]
+name = "tokio-reactor"
+version = "0.1.9"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+dependencies = [
+ "crossbeam-utils 0.6.5 (registry+https://github.com/rust-lang/crates.io-index)",
+ "futures 0.1.27 (registry+https://github.com/rust-lang/crates.io-index)",
+ "lazy_static 1.3.0 (registry+https://github.com/rust-lang/crates.io-index)",
+ "log 0.4.6 (registry+https://github.com/rust-lang/crates.io-index)",
+ "mio 0.6.19 (registry+https://github.com/rust-lang/crates.io-index)",
+ "num_cpus 1.10.1 (registry+https://github.com/rust-lang/crates.io-index)",
+ "parking_lot 0.7.1 (registry+https://github.com/rust-lang/crates.io-index)",
+ "slab 0.4.2 (registry+https://github.com/rust-lang/crates.io-index)",
+ "tokio-executor 0.1.7 (registry+https://github.com/rust-lang/crates.io-index)",
+ "tokio-io 0.1.12 (registry+https://github.com/rust-lang/crates.io-index)",
+ "tokio-sync 0.1.6 (registry+https://github.com/rust-lang/crates.io-index)",
+]
+
+[[package]]
+name = "tokio-sync"
+version = "0.1.6"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+dependencies = [
+ "fnv 1.0.6 (registry+https://github.com/rust-lang/crates.io-index)",
+ "futures 0.1.27 (registry+https://github.com/rust-lang/crates.io-index)",
+]
+
+[[package]]
+name = "tokio-tcp"
+version = "0.1.3"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+dependencies = [
+ "bytes 0.4.12 (registry+https://github.com/rust-lang/crates.io-index)",
+ "futures 0.1.27 (registry+https://github.com/rust-lang/crates.io-index)",
+ "iovec 0.1.2 (registry+https://github.com/rust-lang/crates.io-index)",
+ "mio 0.6.19 (registry+https://github.com/rust-lang/crates.io-index)",
+ "tokio-io 0.1.12 (registry+https://github.com/rust-lang/crates.io-index)",
+ "tokio-reactor 0.1.9 (registry+https://github.com/rust-lang/crates.io-index)",
+]
+
+[[package]]
+name = "tokio-threadpool"
+version = "0.1.14"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+dependencies = [
+ "crossbeam-deque 0.7.1 (registry+https://github.com/rust-lang/crates.io-index)",
+ "crossbeam-queue 0.1.2 (registry+https://github.com/rust-lang/crates.io-index)",
+ "crossbeam-utils 0.6.5 (registry+https://github.com/rust-lang/crates.io-index)",
+ "futures 0.1.27 (registry+https://github.com/rust-lang/crates.io-index)",
+ "log 0.4.6 (registry+https://github.com/rust-lang/crates.io-index)",
+ "num_cpus 1.10.1 (registry+https://github.com/rust-lang/crates.io-index)",
+ "rand 0.6.5 (registry+https://github.com/rust-lang/crates.io-index)",
+ "slab 0.4.2 (registry+https://github.com/rust-lang/crates.io-index)",
+ "tokio-executor 0.1.7 (registry+https://github.com/rust-lang/crates.io-index)",
+]
+
+[[package]]
+name = "tokio-timer"
+version = "0.2.11"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+dependencies = [
+ "crossbeam-utils 0.6.5 (registry+https://github.com/rust-lang/crates.io-index)",
+ "futures 0.1.27 (registry+https://github.com/rust-lang/crates.io-index)",
+ "slab 0.4.2 (registry+https://github.com/rust-lang/crates.io-index)",
+ "tokio-executor 0.1.7 (registry+https://github.com/rust-lang/crates.io-index)",
+]
+
+[[package]]
+name = "tokio-trace-core"
+version = "0.2.0"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+dependencies = [
+ "lazy_static 1.3.0 (registry+https://github.com/rust-lang/crates.io-index)",
+]
+
+[[package]]
+name = "tokio-udp"
+version = "0.1.3"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+dependencies = [
+ "bytes 0.4.12 (registry+https://github.com/rust-lang/crates.io-index)",
+ "futures 0.1.27 (registry+https://github.com/rust-lang/crates.io-index)",
+ "log 0.4.6 (registry+https://github.com/rust-lang/crates.io-index)",
+ "mio 0.6.19 (registry+https://github.com/rust-lang/crates.io-index)",
+ "tokio-codec 0.1.1 (registry+https://github.com/rust-lang/crates.io-index)",
+ "tokio-io 0.1.12 (registry+https://github.com/rust-lang/crates.io-index)",
+ "tokio-reactor 0.1.9 (registry+https://github.com/rust-lang/crates.io-index)",
+]
+
+[[package]]
+name = "tokio-uds"
+version = "0.2.5"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+dependencies = [
+ "bytes 0.4.12 (registry+https://github.com/rust-lang/crates.io-index)",
+ "futures 0.1.27 (registry+https://github.com/rust-lang/crates.io-index)",
+ "iovec 0.1.2 (registry+https://github.com/rust-lang/crates.io-index)",
+ "libc 0.2.58 (registry+https://github.com/rust-lang/crates.io-index)",
+ "log 0.4.6 (registry+https://github.com/rust-lang/crates.io-index)",
+ "mio 0.6.19 (registry+https://github.com/rust-lang/crates.io-index)",
+ "mio-uds 0.6.7 (registry+https://github.com/rust-lang/crates.io-index)",
+ "tokio-codec 0.1.1 (registry+https://github.com/rust-lang/crates.io-index)",
+ "tokio-io 0.1.12 (registry+https://github.com/rust-lang/crates.io-index)",
+ "tokio-reactor 0.1.9 (registry+https://github.com/rust-lang/crates.io-index)",
+]
+
+[[package]]
+name = "try-lock"
+version = "0.2.2"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+
+[[package]]
+name = "ucd-util"
+version = "0.1.3"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+
+[[package]]
+name = "unicode-xid"
+version = "0.1.0"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+
+[[package]]
+name = "utf8-ranges"
+version = "1.0.3"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+
+[[package]]
+name = "uuid"
+version = "0.7.4"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+dependencies = [
+ "rand 0.6.5 (registry+https://github.com/rust-lang/crates.io-index)",
+ "serde 1.0.92 (registry+https://github.com/rust-lang/crates.io-index)",
+]
+
+[[package]]
+name = "version_check"
+version = "0.1.5"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+
+[[package]]
+name = "vminfod"
+version = "0.1.0"
+dependencies = [
+ "crossbeam-channel 0.3.8 (registry+https://github.com/rust-lang/crates.io-index)",
+ "futures 0.1.27 (registry+https://github.com/rust-lang/crates.io-index)",
+ "hyper 0.12.30 (registry+https://github.com/rust-lang/crates.io-index)",
+ "log 0.4.6 (registry+https://github.com/rust-lang/crates.io-index)",
+ "serde 1.0.92 (registry+https://github.com/rust-lang/crates.io-index)",
+ "serde_json 1.0.39 (registry+https://github.com/rust-lang/crates.io-index)",
+ "tokio 0.1.21 (registry+https://github.com/rust-lang/crates.io-index)",
+]
+
+[[package]]
+name = "want"
+version = "0.0.6"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+dependencies = [
+ "futures 0.1.27 (registry+https://github.com/rust-lang/crates.io-index)",
+ "log 0.4.6 (registry+https://github.com/rust-lang/crates.io-index)",
+ "try-lock 0.2.2 (registry+https://github.com/rust-lang/crates.io-index)",
+]
+
+[[package]]
+name = "winapi"
+version = "0.2.8"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+
+[[package]]
+name = "winapi"
+version = "0.3.7"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+dependencies = [
+ "winapi-i686-pc-windows-gnu 0.4.0 (registry+https://github.com/rust-lang/crates.io-index)",
+ "winapi-x86_64-pc-windows-gnu 0.4.0 (registry+https://github.com/rust-lang/crates.io-index)",
+]
+
+[[package]]
+name = "winapi-build"
+version = "0.1.1"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+
+[[package]]
+name = "winapi-i686-pc-windows-gnu"
+version = "0.4.0"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+
+[[package]]
+name = "winapi-util"
+version = "0.1.2"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+dependencies = [
+ "winapi 0.3.7 (registry+https://github.com/rust-lang/crates.io-index)",
+]
+
+[[package]]
+name = "winapi-x86_64-pc-windows-gnu"
+version = "0.4.0"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+
+[[package]]
+name = "wincolor"
+version = "1.0.1"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+dependencies = [
+ "winapi 0.3.7 (registry+https://github.com/rust-lang/crates.io-index)",
+ "winapi-util 0.1.2 (registry+https://github.com/rust-lang/crates.io-index)",
+]
+
+[[package]]
+name = "ws2_32-sys"
+version = "0.2.1"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+dependencies = [
+ "winapi 0.2.8 (registry+https://github.com/rust-lang/crates.io-index)",
+ "winapi-build 0.1.1 (registry+https://github.com/rust-lang/crates.io-index)",
+]
+
+[metadata]
+"checksum aho-corasick 0.7.3 (registry+https://github.com/rust-lang/crates.io-index)" = "e6f484ae0c99fec2e858eb6134949117399f222608d84cadb3f58c1f97c2364c"
+"checksum arc-swap 0.3.11 (registry+https://github.com/rust-lang/crates.io-index)" = "bc4662175ead9cd84451d5c35070517777949a2ed84551764129cedb88384841"
+"checksum arrayvec 0.4.10 (registry+https://github.com/rust-lang/crates.io-index)" = "92c7fb76bc8826a8b33b4ee5bb07a247a81e76764ab4d55e8f73e3a4d8808c71"
+"checksum atty 0.2.11 (registry+https://github.com/rust-lang/crates.io-index)" = "9a7d5b8723950951411ee34d271d99dddcc2035a16ab25310ea2c8cfd4369652"
+"checksum autocfg 0.1.4 (registry+https://github.com/rust-lang/crates.io-index)" = "0e49efa51329a5fd37e7c79db4621af617cd4e3e5bc224939808d076077077bf"
+"checksum backtrace 0.3.30 (registry+https://github.com/rust-lang/crates.io-index)" = "ada4c783bb7e7443c14e0480f429ae2cc99da95065aeab7ee1b81ada0419404f"
+"checksum backtrace-sys 0.1.28 (registry+https://github.com/rust-lang/crates.io-index)" = "797c830ac25ccc92a7f8a7b9862bde440715531514594a6154e3d4a54dd769b6"
+"checksum bitflags 1.1.0 (registry+https://github.com/rust-lang/crates.io-index)" = "3d155346769a6855b86399e9bc3814ab343cd3d62c7e985113d46a0ec3c281fd"
+"checksum byteorder 1.3.2 (registry+https://github.com/rust-lang/crates.io-index)" = "a7c3dd8985a7111efc5c80b44e23ecdd8c007de8ade3b96595387e812b957cf5"
+"checksum bytes 0.4.12 (registry+https://github.com/rust-lang/crates.io-index)" = "206fdffcfa2df7cbe15601ef46c813fce0965eb3286db6b56c583b814b51c81c"
+"checksum cc 1.0.37 (registry+https://github.com/rust-lang/crates.io-index)" = "39f75544d7bbaf57560d2168f28fd649ff9c76153874db88bdbdfd839b1a7e7d"
+"checksum cfg-if 0.1.9 (registry+https://github.com/rust-lang/crates.io-index)" = "b486ce3ccf7ffd79fdeb678eac06a9e6c09fc88d33836340becb8fffe87c5e33"
+"checksum chrono 0.4.6 (registry+https://github.com/rust-lang/crates.io-index)" = "45912881121cb26fad7c38c17ba7daa18764771836b34fab7d3fbd93ed633878"
+"checksum cloudabi 0.0.3 (registry+https://github.com/rust-lang/crates.io-index)" = "ddfc5b9aa5d4507acaf872de71051dfd0e309860e88966e1051e462a077aac4f"
+"checksum crossbeam 0.7.1 (registry+https://github.com/rust-lang/crates.io-index)" = "b14492071ca110999a20bf90e3833406d5d66bfd93b4e52ec9539025ff43fe0d"
+"checksum crossbeam-channel 0.3.8 (registry+https://github.com/rust-lang/crates.io-index)" = "0f0ed1a4de2235cabda8558ff5840bffb97fcb64c97827f354a451307df5f72b"
+"checksum crossbeam-deque 0.7.1 (registry+https://github.com/rust-lang/crates.io-index)" = "b18cd2e169ad86297e6bc0ad9aa679aee9daa4f19e8163860faf7c164e4f5a71"
+"checksum crossbeam-epoch 0.7.1 (registry+https://github.com/rust-lang/crates.io-index)" = "04c9e3102cc2d69cd681412141b390abd55a362afc1540965dad0ad4d34280b4"
+"checksum crossbeam-queue 0.1.2 (registry+https://github.com/rust-lang/crates.io-index)" = "7c979cd6cfe72335896575c6b5688da489e420d36a27a0b9eb0c73db574b4a4b"
+"checksum crossbeam-utils 0.6.5 (registry+https://github.com/rust-lang/crates.io-index)" = "f8306fcef4a7b563b76b7dd949ca48f52bc1141aa067d2ea09565f3e2652aa5c"
+"checksum either 1.5.2 (registry+https://github.com/rust-lang/crates.io-index)" = "5527cfe0d098f36e3f8839852688e63c8fff1c90b2b405aef730615f9a7bcf7b"
+"checksum env_logger 0.6.1 (registry+https://github.com/rust-lang/crates.io-index)" = "b61fa891024a945da30a9581546e8cfaf5602c7b3f4c137a2805cf388f92075a"
+"checksum failure 0.1.5 (registry+https://github.com/rust-lang/crates.io-index)" = "795bd83d3abeb9220f257e597aa0080a508b27533824adf336529648f6abf7e2"
+"checksum failure_derive 0.1.5 (registry+https://github.com/rust-lang/crates.io-index)" = "ea1063915fd7ef4309e222a5a07cf9c319fb9c7836b1f89b85458672dbb127e1"
+"checksum fnv 1.0.6 (registry+https://github.com/rust-lang/crates.io-index)" = "2fad85553e09a6f881f739c29f0b00b0f01357c743266d478b68951ce23285f3"
+"checksum fuchsia-cprng 0.1.1 (registry+https://github.com/rust-lang/crates.io-index)" = "a06f77d526c1a601b7c4cdd98f54b5eaabffc14d5f2f0296febdc7f357c6d3ba"
+"checksum fuchsia-zircon 0.3.3 (registry+https://github.com/rust-lang/crates.io-index)" = "2e9763c69ebaae630ba35f74888db465e49e259ba1bc0eda7d06f4a067615d82"
+"checksum fuchsia-zircon-sys 0.3.3 (registry+https://github.com/rust-lang/crates.io-index)" = "3dcaa9ae7725d12cdb85b3ad99a434db70b468c09ded17e012d86b5c1010f7a7"
+"checksum futures 0.1.27 (registry+https://github.com/rust-lang/crates.io-index)" = "a2037ec1c6c1c4f79557762eab1f7eae1f64f6cb418ace90fae88f0942b60139"
+"checksum futures-cpupool 0.1.8 (registry+https://github.com/rust-lang/crates.io-index)" = "ab90cde24b3319636588d0c35fe03b1333857621051837ed769faefb4c2162e4"
+"checksum h2 0.1.24 (registry+https://github.com/rust-lang/crates.io-index)" = "69b2a5a3092cbebbc951fe55408402e696ee2ed09019137d1800fc2c411265d2"
+"checksum http 0.1.17 (registry+https://github.com/rust-lang/crates.io-index)" = "eed324f0f0daf6ec10c474f150505af2c143f251722bf9dbd1261bd1f2ee2c1a"
+"checksum http-body 0.1.0 (registry+https://github.com/rust-lang/crates.io-index)" = "6741c859c1b2463a423a1dbce98d418e6c3c3fc720fb0d45528657320920292d"
+"checksum httparse 1.3.3 (registry+https://github.com/rust-lang/crates.io-index)" = "e8734b0cfd3bc3e101ec59100e101c2eecd19282202e87808b3037b442777a83"
+"checksum humantime 1.2.0 (registry+https://github.com/rust-lang/crates.io-index)" = "3ca7e5f2e110db35f93b837c81797f3714500b81d517bf20c431b16d3ca4f114"
+"checksum hyper 0.12.30 (registry+https://github.com/rust-lang/crates.io-index)" = "40e7692b2009a70b1e9b362284add4d8b75880fefddb4acaa5e67194e843f219"
+"checksum indexmap 1.0.2 (registry+https://github.com/rust-lang/crates.io-index)" = "7e81a7c05f79578dbc15793d8b619db9ba32b4577003ef3af1a91c416798c58d"
+"checksum iovec 0.1.2 (registry+https://github.com/rust-lang/crates.io-index)" = "dbe6e417e7d0975db6512b90796e8ce223145ac4e33c377e4a42882a0e88bb08"
+"checksum itoa 0.4.4 (registry+https://github.com/rust-lang/crates.io-index)" = "501266b7edd0174f8530248f87f99c88fbe60ca4ef3dd486835b8d8d53136f7f"
+"checksum kernel32-sys 0.2.2 (registry+https://github.com/rust-lang/crates.io-index)" = "7507624b29483431c0ba2d82aece8ca6cdba9382bff4ddd0f7490560c056098d"
+"checksum lazy_static 1.3.0 (registry+https://github.com/rust-lang/crates.io-index)" = "bc5729f27f159ddd61f4df6228e827e86643d4d3e7c32183cb30a1c08f604a14"
+"checksum libc 0.2.58 (registry+https://github.com/rust-lang/crates.io-index)" = "6281b86796ba5e4366000be6e9e18bf35580adf9e63fbe2294aadb587613a319"
+"checksum lock_api 0.1.5 (registry+https://github.com/rust-lang/crates.io-index)" = "62ebf1391f6acad60e5c8b43706dde4582df75c06698ab44511d15016bc2442c"
+"checksum log 0.4.6 (registry+https://github.com/rust-lang/crates.io-index)" = "c84ec4b527950aa83a329754b01dbe3f58361d1c5efacd1f6d68c494d08a17c6"
+"checksum memchr 2.2.0 (registry+https://github.com/rust-lang/crates.io-index)" = "2efc7bc57c883d4a4d6e3246905283d8dae951bb3bd32f49d6ef297f546e1c39"
+"checksum memoffset 0.2.1 (registry+https://github.com/rust-lang/crates.io-index)" = "0f9dc261e2b62d7a622bf416ea3c5245cdd5d9a7fcc428c0d06804dfce1775b3"
+"checksum mio 0.6.19 (registry+https://github.com/rust-lang/crates.io-index)" = "83f51996a3ed004ef184e16818edc51fadffe8e7ca68be67f9dee67d84d0ff23"
+"checksum mio-uds 0.6.7 (registry+https://github.com/rust-lang/crates.io-index)" = "966257a94e196b11bb43aca423754d87429960a768de9414f3691d6957abf125"
+"checksum miow 0.2.1 (registry+https://github.com/rust-lang/crates.io-index)" = "8c1f2f3b1cf331de6896aabf6e9d55dca90356cc9960cca7eaaf408a355ae919"
+"checksum net2 0.2.33 (registry+https://github.com/rust-lang/crates.io-index)" = "42550d9fb7b6684a6d404d9fa7250c2eb2646df731d1c06afc06dcee9e1bcf88"
+"checksum nodrop 0.1.13 (registry+https://github.com/rust-lang/crates.io-index)" = "2f9667ddcc6cc8a43afc9b7917599d7216aa09c463919ea32c59ed6cac8bc945"
+"checksum nom 4.2.3 (registry+https://github.com/rust-lang/crates.io-index)" = "2ad2a91a8e869eeb30b9cb3119ae87773a8f4ae617f41b1eb9c154b2905f7bd6"
+"checksum num-integer 0.1.41 (registry+https://github.com/rust-lang/crates.io-index)" = "b85e541ef8255f6cf42bbfe4ef361305c6c135d10919ecc26126c4e5ae94bc09"
+"checksum num-traits 0.2.8 (registry+https://github.com/rust-lang/crates.io-index)" = "6ba9a427cfca2be13aa6f6403b0b7e7368fe982bfa16fccc450ce74c46cd9b32"
+"checksum num_cpus 1.10.1 (registry+https://github.com/rust-lang/crates.io-index)" = "bcef43580c035376c0705c42792c294b66974abbfd2789b511784023f71f3273"
+"checksum numtoa 0.1.0 (registry+https://github.com/rust-lang/crates.io-index)" = "b8f8bdf33df195859076e54ab11ee78a1b208382d3a26ec40d142ffc1ecc49ef"
+"checksum owning_ref 0.4.0 (registry+https://github.com/rust-lang/crates.io-index)" = "49a4b8ea2179e6a2e27411d3bca09ca6dd630821cf6894c6c7c8467a8ee7ef13"
+"checksum parking_lot 0.7.1 (registry+https://github.com/rust-lang/crates.io-index)" = "ab41b4aed082705d1056416ae4468b6ea99d52599ecf3169b00088d43113e337"
+"checksum parking_lot_core 0.4.0 (registry+https://github.com/rust-lang/crates.io-index)" = "94c8c7923936b28d546dfd14d4472eaf34c99b14e1c973a32b3e6d4eb04298c9"
+"checksum pretty_env_logger 0.3.0 (registry+https://github.com/rust-lang/crates.io-index)" = "df8b3f4e0475def7d9c2e5de8e5a1306949849761e107b360d03e98eafaffd61"
+"checksum proc-macro2 0.4.30 (registry+https://github.com/rust-lang/crates.io-index)" = "cf3d2011ab5c909338f7887f4fc896d35932e29146c12c8d01da6b22a80ba759"
+"checksum quick-error 1.2.2 (registry+https://github.com/rust-lang/crates.io-index)" = "9274b940887ce9addde99c4eee6b5c44cc494b182b97e73dc8ffdcb3397fd3f0"
+"checksum quote 0.6.12 (registry+https://github.com/rust-lang/crates.io-index)" = "faf4799c5d274f3868a4aae320a0a182cbd2baee377b378f080e16a23e9d80db"
+"checksum rand 0.6.5 (registry+https://github.com/rust-lang/crates.io-index)" = "6d71dacdc3c88c1fde3885a3be3fbab9f35724e6ce99467f7d9c5026132184ca"
+"checksum rand_chacha 0.1.1 (registry+https://github.com/rust-lang/crates.io-index)" = "556d3a1ca6600bfcbab7c7c91ccb085ac7fbbcd70e008a98742e7847f4f7bcef"
+"checksum rand_core 0.3.1 (registry+https://github.com/rust-lang/crates.io-index)" = "7a6fdeb83b075e8266dcc8762c22776f6877a63111121f5f8c7411e5be7eed4b"
+"checksum rand_core 0.4.0 (registry+https://github.com/rust-lang/crates.io-index)" = "d0e7a549d590831370895ab7ba4ea0c1b6b011d106b5ff2da6eee112615e6dc0"
+"checksum rand_hc 0.1.0 (registry+https://github.com/rust-lang/crates.io-index)" = "7b40677c7be09ae76218dc623efbf7b18e34bced3f38883af07bb75630a21bc4"
+"checksum rand_isaac 0.1.1 (registry+https://github.com/rust-lang/crates.io-index)" = "ded997c9d5f13925be2a6fd7e66bf1872597f759fd9dd93513dd7e92e5a5ee08"
+"checksum rand_jitter 0.1.4 (registry+https://github.com/rust-lang/crates.io-index)" = "1166d5c91dc97b88d1decc3285bb0a99ed84b05cfd0bc2341bdf2d43fc41e39b"
+"checksum rand_os 0.1.3 (registry+https://github.com/rust-lang/crates.io-index)" = "7b75f676a1e053fc562eafbb47838d67c84801e38fc1ba459e8f180deabd5071"
+"checksum rand_pcg 0.1.2 (registry+https://github.com/rust-lang/crates.io-index)" = "abf9b09b01790cfe0364f52bf32995ea3c39f4d2dd011eac241d2914146d0b44"
+"checksum rand_xorshift 0.1.1 (registry+https://github.com/rust-lang/crates.io-index)" = "cbf7e9e623549b0e21f6e97cf8ecf247c1a8fd2e8a992ae265314300b2455d5c"
+"checksum rdrand 0.4.0 (registry+https://github.com/rust-lang/crates.io-index)" = "678054eb77286b51581ba43620cc911abf02758c91f93f479767aed0f90458b2"
+"checksum redox_syscall 0.1.54 (registry+https://github.com/rust-lang/crates.io-index)" = "12229c14a0f65c4f1cb046a3b52047cdd9da1f4b30f8a39c5063c8bae515e252"
+"checksum redox_termios 0.1.1 (registry+https://github.com/rust-lang/crates.io-index)" = "7e891cfe48e9100a70a3b6eb652fef28920c117d366339687bd5576160db0f76"
+"checksum regex 1.1.7 (registry+https://github.com/rust-lang/crates.io-index)" = "0b2f0808e7d7e4fb1cb07feb6ff2f4bc827938f24f8c2e6a3beb7370af544bdd"
+"checksum regex-syntax 0.6.7 (registry+https://github.com/rust-lang/crates.io-index)" = "9d76410686f9e3a17f06128962e0ecc5755870bb890c34820c7af7f1db2e1d48"
+"checksum rustc-demangle 0.1.15 (registry+https://github.com/rust-lang/crates.io-index)" = "a7f4dccf6f4891ebcc0c39f9b6eb1a83b9bf5d747cb439ec6fba4f3b977038af"
+"checksum rustc_version 0.2.3 (registry+https://github.com/rust-lang/crates.io-index)" = "138e3e0acb6c9fb258b19b67cb8abd63c00679d2851805ea151465464fe9030a"
+"checksum ryu 0.2.8 (registry+https://github.com/rust-lang/crates.io-index)" = "b96a9549dc8d48f2c283938303c4b5a77aa29bfbc5b54b084fb1630408899a8f"
+"checksum scopeguard 0.3.3 (registry+https://github.com/rust-lang/crates.io-index)" = "94258f53601af11e6a49f722422f6e3425c52b06245a5cf9bc09908b174f5e27"
+"checksum semver 0.9.0 (registry+https://github.com/rust-lang/crates.io-index)" = "1d7eb9ef2c18661902cc47e535f9bc51b78acd254da71d375c2f6720d9a40403"
+"checksum semver-parser 0.7.0 (registry+https://github.com/rust-lang/crates.io-index)" = "388a1df253eca08550bef6c72392cfe7c30914bf41df5269b68cbd6ff8f570a3"
+"checksum serde 1.0.92 (registry+https://github.com/rust-lang/crates.io-index)" = "32746bf0f26eab52f06af0d0aa1984f641341d06d8d673c693871da2d188c9be"
+"checksum serde_derive 1.0.92 (registry+https://github.com/rust-lang/crates.io-index)" = "46a3223d0c9ba936b61c0d2e3e559e3217dbfb8d65d06d26e8b3c25de38bae3e"
+"checksum serde_json 1.0.39 (registry+https://github.com/rust-lang/crates.io-index)" = "5a23aa71d4a4d43fdbfaac00eff68ba8a06a51759a89ac3304323e800c4dd40d"
+"checksum signal-hook 0.1.9 (registry+https://github.com/rust-lang/crates.io-index)" = "72ab58f1fda436857e6337dcb6a5aaa34f16c5ddc87b3a8b6ef7a212f90b9c5a"
+"checksum signal-hook-registry 1.0.1 (registry+https://github.com/rust-lang/crates.io-index)" = "cded4ffa32146722ec54ab1f16320568465aa922aa9ab4708129599740da85d7"
+"checksum slab 0.4.2 (registry+https://github.com/rust-lang/crates.io-index)" = "c111b5bd5695e56cffe5129854aa230b39c93a305372fdbb2668ca2394eea9f8"
+"checksum smallvec 0.6.10 (registry+https://github.com/rust-lang/crates.io-index)" = "ab606a9c5e214920bb66c458cd7be8ef094f813f20fe77a54cc7dbfff220d4b7"
+"checksum stable_deref_trait 1.1.1 (registry+https://github.com/rust-lang/crates.io-index)" = "dba1a27d3efae4351c8051072d619e3ade2820635c3958d826bfea39d59b54c8"
+"checksum string 0.2.0 (registry+https://github.com/rust-lang/crates.io-index)" = "d0bbfb8937e38e34c3444ff00afb28b0811d9554f15c5ad64d12b0308d1d1995"
+"checksum syn 0.15.36 (registry+https://github.com/rust-lang/crates.io-index)" = "8b4f551a91e2e3848aeef8751d0d4eec9489b6474c720fd4c55958d8d31a430c"
+"checksum synstructure 0.10.2 (registry+https://github.com/rust-lang/crates.io-index)" = "02353edf96d6e4dc81aea2d8490a7e9db177bf8acb0e951c24940bf866cb313f"
+"checksum termcolor 1.0.5 (registry+https://github.com/rust-lang/crates.io-index)" = "96d6098003bde162e4277c70665bd87c326f5a0c3f3fbfb285787fa482d54e6e"
+"checksum termion 1.5.3 (registry+https://github.com/rust-lang/crates.io-index)" = "6a8fb22f7cde82c8220e5aeacb3258ed7ce996142c77cba193f203515e26c330"
+"checksum thread_local 0.3.6 (registry+https://github.com/rust-lang/crates.io-index)" = "c6b53e329000edc2b34dbe8545fd20e55a333362d0a321909685a19bd28c3f1b"
+"checksum time 0.1.42 (registry+https://github.com/rust-lang/crates.io-index)" = "db8dcfca086c1143c9270ac42a2bbd8a7ee477b78ac8e45b19abfb0cbede4b6f"
+"checksum tokio 0.1.21 (registry+https://github.com/rust-lang/crates.io-index)" = "ec2ffcf4bcfc641413fa0f1427bf8f91dfc78f56a6559cbf50e04837ae442a87"
+"checksum tokio-buf 0.1.1 (registry+https://github.com/rust-lang/crates.io-index)" = "8fb220f46c53859a4b7ec083e41dec9778ff0b1851c0942b211edb89e0ccdc46"
+"checksum tokio-codec 0.1.1 (registry+https://github.com/rust-lang/crates.io-index)" = "5c501eceaf96f0e1793cf26beb63da3d11c738c4a943fdf3746d81d64684c39f"
+"checksum tokio-current-thread 0.1.6 (registry+https://github.com/rust-lang/crates.io-index)" = "d16217cad7f1b840c5a97dfb3c43b0c871fef423a6e8d2118c604e843662a443"
+"checksum tokio-executor 0.1.7 (registry+https://github.com/rust-lang/crates.io-index)" = "83ea44c6c0773cc034771693711c35c677b4b5a4b21b9e7071704c54de7d555e"
+"checksum tokio-fs 0.1.6 (registry+https://github.com/rust-lang/crates.io-index)" = "3fe6dc22b08d6993916647d108a1a7d15b9cd29c4f4496c62b92c45b5041b7af"
+"checksum tokio-io 0.1.12 (registry+https://github.com/rust-lang/crates.io-index)" = "5090db468dad16e1a7a54c8c67280c5e4b544f3d3e018f0b913b400261f85926"
+"checksum tokio-reactor 0.1.9 (registry+https://github.com/rust-lang/crates.io-index)" = "6af16bfac7e112bea8b0442542161bfc41cbfa4466b580bdda7d18cb88b911ce"
+"checksum tokio-sync 0.1.6 (registry+https://github.com/rust-lang/crates.io-index)" = "2162248ff317e2bc713b261f242b69dbb838b85248ed20bb21df56d60ea4cae7"
+"checksum tokio-tcp 0.1.3 (registry+https://github.com/rust-lang/crates.io-index)" = "1d14b10654be682ac43efee27401d792507e30fd8d26389e1da3b185de2e4119"
+"checksum tokio-threadpool 0.1.14 (registry+https://github.com/rust-lang/crates.io-index)" = "72558af20be886ea124595ea0f806dd5703b8958e4705429dd58b3d8231f72f2"
+"checksum tokio-timer 0.2.11 (registry+https://github.com/rust-lang/crates.io-index)" = "f2106812d500ed25a4f38235b9cae8f78a09edf43203e16e59c3b769a342a60e"
+"checksum tokio-trace-core 0.2.0 (registry+https://github.com/rust-lang/crates.io-index)" = "a9c8a256d6956f7cb5e2bdfe8b1e8022f1a09206c6c2b1ba00f3b746b260c613"
+"checksum tokio-udp 0.1.3 (registry+https://github.com/rust-lang/crates.io-index)" = "66268575b80f4a4a710ef83d087fdfeeabdce9b74c797535fbac18a2cb906e92"
+"checksum tokio-uds 0.2.5 (registry+https://github.com/rust-lang/crates.io-index)" = "037ffc3ba0e12a0ab4aca92e5234e0dedeb48fddf6ccd260f1f150a36a9f2445"
+"checksum try-lock 0.2.2 (registry+https://github.com/rust-lang/crates.io-index)" = "e604eb7b43c06650e854be16a2a03155743d3752dd1c943f6829e26b7a36e382"
+"checksum ucd-util 0.1.3 (registry+https://github.com/rust-lang/crates.io-index)" = "535c204ee4d8434478593480b8f86ab45ec9aae0e83c568ca81abf0fd0e88f86"
+"checksum unicode-xid 0.1.0 (registry+https://github.com/rust-lang/crates.io-index)" = "fc72304796d0818e357ead4e000d19c9c174ab23dc11093ac919054d20a6a7fc"
+"checksum utf8-ranges 1.0.3 (registry+https://github.com/rust-lang/crates.io-index)" = "9d50aa7650df78abf942826607c62468ce18d9019673d4a2ebe1865dbb96ffde"
+"checksum uuid 0.7.4 (registry+https://github.com/rust-lang/crates.io-index)" = "90dbc611eb48397705a6b0f6e917da23ae517e4d127123d2cf7674206627d32a"
+"checksum version_check 0.1.5 (registry+https://github.com/rust-lang/crates.io-index)" = "914b1a6776c4c929a602fafd8bc742e06365d4bcbe48c30f9cca5824f70dc9dd"
+"checksum want 0.0.6 (registry+https://github.com/rust-lang/crates.io-index)" = "797464475f30ddb8830cc529aaaae648d581f99e2036a928877dfde027ddf6b3"
+"checksum winapi 0.2.8 (registry+https://github.com/rust-lang/crates.io-index)" = "167dc9d6949a9b857f3451275e911c3f44255842c1f7a76f33c55103a909087a"
+"checksum winapi 0.3.7 (registry+https://github.com/rust-lang/crates.io-index)" = "f10e386af2b13e47c89e7236a7a14a086791a2b88ebad6df9bf42040195cf770"
+"checksum winapi-build 0.1.1 (registry+https://github.com/rust-lang/crates.io-index)" = "2d315eee3b34aca4797b2da6b13ed88266e6d612562a0c46390af8299fc699bc"
+"checksum winapi-i686-pc-windows-gnu 0.4.0 (registry+https://github.com/rust-lang/crates.io-index)" = "ac3b87c63620426dd9b991e5ce0329eff545bccbbb34f3be09ff6fb6ab51b7b6"
+"checksum winapi-util 0.1.2 (registry+https://github.com/rust-lang/crates.io-index)" = "7168bab6e1daee33b4557efd0e95d5ca70a03706d39fa5f3fe7a236f584b03c9"
+"checksum winapi-x86_64-pc-windows-gnu 0.4.0 (registry+https://github.com/rust-lang/crates.io-index)" = "712e227841d057c1ee1cd2fb22fa7e5a5461ae8e48fa2ca79ec42cfc1931183f"
+"checksum wincolor 1.0.1 (registry+https://github.com/rust-lang/crates.io-index)" = "561ed901ae465d6185fa7864d63fbd5720d0ef718366c9a4dc83cf6170d7e9ba"
+"checksum ws2_32-sys 0.2.1 (registry+https://github.com/rust-lang/crates.io-index)" = "d59cefebd0c892fa2dd6de581e937301d8552cb44489cdff035c6187cb63fa5e"
diff --git a/Cargo.toml b/Cargo.toml
new file mode 100644
index 0000000..56e6040
--- /dev/null
+++ b/Cargo.toml
@@ -0,0 +1,19 @@
+[workspace]
+
+members = [
+	"cfwlogd",
+	"testutils",
+	"vminfod",
+]
+
+# Temporary patch until the udpated rand crate is pushed to crates.io
+#[patch.crates-io]
+#rand = { git = "https://github.com/rust-random/rand", branch = "master" }
+
+[profile.dev]
+panic = 'abort'
+
+[profile.release]
+panic = 'abort'
+lto = true
+debug = true
diff --git a/LICENSE b/LICENSE
new file mode 100644
index 0000000..a612ad9
--- /dev/null
+++ b/LICENSE
@@ -0,0 +1,373 @@
+Mozilla Public License Version 2.0
+==================================
+
+1. Definitions
+--------------
+
+1.1. "Contributor"
+    means each individual or legal entity that creates, contributes to
+    the creation of, or owns Covered Software.
+
+1.2. "Contributor Version"
+    means the combination of the Contributions of others (if any) used
+    by a Contributor and that particular Contributor's Contribution.
+
+1.3. "Contribution"
+    means Covered Software of a particular Contributor.
+
+1.4. "Covered Software"
+    means Source Code Form to which the initial Contributor has attached
+    the notice in Exhibit A, the Executable Form of such Source Code
+    Form, and Modifications of such Source Code Form, in each case
+    including portions thereof.
+
+1.5. "Incompatible With Secondary Licenses"
+    means
+
+    (a) that the initial Contributor has attached the notice described
+        in Exhibit B to the Covered Software; or
+
+    (b) that the Covered Software was made available under the terms of
+        version 1.1 or earlier of the License, but not also under the
+        terms of a Secondary License.
+
+1.6. "Executable Form"
+    means any form of the work other than Source Code Form.
+
+1.7. "Larger Work"
+    means a work that combines Covered Software with other material, in
+    a separate file or files, that is not Covered Software.
+
+1.8. "License"
+    means this document.
+
+1.9. "Licensable"
+    means having the right to grant, to the maximum extent possible,
+    whether at the time of the initial grant or subsequently, any and
+    all of the rights conveyed by this License.
+
+1.10. "Modifications"
+    means any of the following:
+
+    (a) any file in Source Code Form that results from an addition to,
+        deletion from, or modification of the contents of Covered
+        Software; or
+
+    (b) any new file in Source Code Form that contains any Covered
+        Software.
+
+1.11. "Patent Claims" of a Contributor
+    means any patent claim(s), including without limitation, method,
+    process, and apparatus claims, in any patent Licensable by such
+    Contributor that would be infringed, but for the grant of the
+    License, by the making, using, selling, offering for sale, having
+    made, import, or transfer of either its Contributions or its
+    Contributor Version.
+
+1.12. "Secondary License"
+    means either the GNU General Public License, Version 2.0, the GNU
+    Lesser General Public License, Version 2.1, the GNU Affero General
+    Public License, Version 3.0, or any later versions of those
+    licenses.
+
+1.13. "Source Code Form"
+    means the form of the work preferred for making modifications.
+
+1.14. "You" (or "Your")
+    means an individual or a legal entity exercising rights under this
+    License. For legal entities, "You" includes any entity that
+    controls, is controlled by, or is under common control with You. For
+    purposes of this definition, "control" means (a) the power, direct
+    or indirect, to cause the direction or management of such entity,
+    whether by contract or otherwise, or (b) ownership of more than
+    fifty percent (50%) of the outstanding shares or beneficial
+    ownership of such entity.
+
+2. License Grants and Conditions
+--------------------------------
+
+2.1. Grants
+
+Each Contributor hereby grants You a world-wide, royalty-free,
+non-exclusive license:
+
+(a) under intellectual property rights (other than patent or trademark)
+    Licensable by such Contributor to use, reproduce, make available,
+    modify, display, perform, distribute, and otherwise exploit its
+    Contributions, either on an unmodified basis, with Modifications, or
+    as part of a Larger Work; and
+
+(b) under Patent Claims of such Contributor to make, use, sell, offer
+    for sale, have made, import, and otherwise transfer either its
+    Contributions or its Contributor Version.
+
+2.2. Effective Date
+
+The licenses granted in Section 2.1 with respect to any Contribution
+become effective for each Contribution on the date the Contributor first
+distributes such Contribution.
+
+2.3. Limitations on Grant Scope
+
+The licenses granted in this Section 2 are the only rights granted under
+this License. No additional rights or licenses will be implied from the
+distribution or licensing of Covered Software under this License.
+Notwithstanding Section 2.1(b) above, no patent license is granted by a
+Contributor:
+
+(a) for any code that a Contributor has removed from Covered Software;
+    or
+
+(b) for infringements caused by: (i) Your and any other third party's
+    modifications of Covered Software, or (ii) the combination of its
+    Contributions with other software (except as part of its Contributor
+    Version); or
+
+(c) under Patent Claims infringed by Covered Software in the absence of
+    its Contributions.
+
+This License does not grant any rights in the trademarks, service marks,
+or logos of any Contributor (except as may be necessary to comply with
+the notice requirements in Section 3.4).
+
+2.4. Subsequent Licenses
+
+No Contributor makes additional grants as a result of Your choice to
+distribute the Covered Software under a subsequent version of this
+License (see Section 10.2) or under the terms of a Secondary License (if
+permitted under the terms of Section 3.3).
+
+2.5. Representation
+
+Each Contributor represents that the Contributor believes its
+Contributions are its original creation(s) or it has sufficient rights
+to grant the rights to its Contributions conveyed by this License.
+
+2.6. Fair Use
+
+This License is not intended to limit any rights You have under
+applicable copyright doctrines of fair use, fair dealing, or other
+equivalents.
+
+2.7. Conditions
+
+Sections 3.1, 3.2, 3.3, and 3.4 are conditions of the licenses granted
+in Section 2.1.
+
+3. Responsibilities
+-------------------
+
+3.1. Distribution of Source Form
+
+All distribution of Covered Software in Source Code Form, including any
+Modifications that You create or to which You contribute, must be under
+the terms of this License. You must inform recipients that the Source
+Code Form of the Covered Software is governed by the terms of this
+License, and how they can obtain a copy of this License. You may not
+attempt to alter or restrict the recipients' rights in the Source Code
+Form.
+
+3.2. Distribution of Executable Form
+
+If You distribute Covered Software in Executable Form then:
+
+(a) such Covered Software must also be made available in Source Code
+    Form, as described in Section 3.1, and You must inform recipients of
+    the Executable Form how they can obtain a copy of such Source Code
+    Form by reasonable means in a timely manner, at a charge no more
+    than the cost of distribution to the recipient; and
+
+(b) You may distribute such Executable Form under the terms of this
+    License, or sublicense it under different terms, provided that the
+    license for the Executable Form does not attempt to limit or alter
+    the recipients' rights in the Source Code Form under this License.
+
+3.3. Distribution of a Larger Work
+
+You may create and distribute a Larger Work under terms of Your choice,
+provided that You also comply with the requirements of this License for
+the Covered Software. If the Larger Work is a combination of Covered
+Software with a work governed by one or more Secondary Licenses, and the
+Covered Software is not Incompatible With Secondary Licenses, this
+License permits You to additionally distribute such Covered Software
+under the terms of such Secondary License(s), so that the recipient of
+the Larger Work may, at their option, further distribute the Covered
+Software under the terms of either this License or such Secondary
+License(s).
+
+3.4. Notices
+
+You may not remove or alter the substance of any license notices
+(including copyright notices, patent notices, disclaimers of warranty,
+or limitations of liability) contained within the Source Code Form of
+the Covered Software, except that You may alter any license notices to
+the extent required to remedy known factual inaccuracies.
+
+3.5. Application of Additional Terms
+
+You may choose to offer, and to charge a fee for, warranty, support,
+indemnity or liability obligations to one or more recipients of Covered
+Software. However, You may do so only on Your own behalf, and not on
+behalf of any Contributor. You must make it absolutely clear that any
+such warranty, support, indemnity, or liability obligation is offered by
+You alone, and You hereby agree to indemnify every Contributor for any
+liability incurred by such Contributor as a result of warranty, support,
+indemnity or liability terms You offer. You may include additional
+disclaimers of warranty and limitations of liability specific to any
+jurisdiction.
+
+4. Inability to Comply Due to Statute or Regulation
+---------------------------------------------------
+
+If it is impossible for You to comply with any of the terms of this
+License with respect to some or all of the Covered Software due to
+statute, judicial order, or regulation then You must: (a) comply with
+the terms of this License to the maximum extent possible; and (b)
+describe the limitations and the code they affect. Such description must
+be placed in a text file included with all distributions of the Covered
+Software under this License. Except to the extent prohibited by statute
+or regulation, such description must be sufficiently detailed for a
+recipient of ordinary skill to be able to understand it.
+
+5. Termination
+--------------
+
+5.1. The rights granted under this License will terminate automatically
+if You fail to comply with any of its terms. However, if You become
+compliant, then the rights granted under this License from a particular
+Contributor are reinstated (a) provisionally, unless and until such
+Contributor explicitly and finally terminates Your grants, and (b) on an
+ongoing basis, if such Contributor fails to notify You of the
+non-compliance by some reasonable means prior to 60 days after You have
+come back into compliance. Moreover, Your grants from a particular
+Contributor are reinstated on an ongoing basis if such Contributor
+notifies You of the non-compliance by some reasonable means, this is the
+first time You have received notice of non-compliance with this License
+from such Contributor, and You become compliant prior to 30 days after
+Your receipt of the notice.
+
+5.2. If You initiate litigation against any entity by asserting a patent
+infringement claim (excluding declaratory judgment actions,
+counter-claims, and cross-claims) alleging that a Contributor Version
+directly or indirectly infringes any patent, then the rights granted to
+You by any and all Contributors for the Covered Software under Section
+2.1 of this License shall terminate.
+
+5.3. In the event of termination under Sections 5.1 or 5.2 above, all
+end user license agreements (excluding distributors and resellers) which
+have been validly granted by You or Your distributors under this License
+prior to termination shall survive termination.
+
+************************************************************************
+*                                                                      *
+*  6. Disclaimer of Warranty                                           *
+*  -------------------------                                           *
+*                                                                      *
+*  Covered Software is provided under this License on an "as is"       *
+*  basis, without warranty of any kind, either expressed, implied, or  *
+*  statutory, including, without limitation, warranties that the       *
+*  Covered Software is free of defects, merchantable, fit for a        *
+*  particular purpose or non-infringing. The entire risk as to the     *
+*  quality and performance of the Covered Software is with You.        *
+*  Should any Covered Software prove defective in any respect, You     *
+*  (not any Contributor) assume the cost of any necessary servicing,   *
+*  repair, or correction. This disclaimer of warranty constitutes an   *
+*  essential part of this License. No use of any Covered Software is   *
+*  authorized under this License except under this disclaimer.         *
+*                                                                      *
+************************************************************************
+
+************************************************************************
+*                                                                      *
+*  7. Limitation of Liability                                          *
+*  --------------------------                                          *
+*                                                                      *
+*  Under no circumstances and under no legal theory, whether tort      *
+*  (including negligence), contract, or otherwise, shall any           *
+*  Contributor, or anyone who distributes Covered Software as          *
+*  permitted above, be liable to You for any direct, indirect,         *
+*  special, incidental, or consequential damages of any character      *
+*  including, without limitation, damages for lost profits, loss of    *
+*  goodwill, work stoppage, computer failure or malfunction, or any    *
+*  and all other commercial damages or losses, even if such party      *
+*  shall have been informed of the possibility of such damages. This   *
+*  limitation of liability shall not apply to liability for death or   *
+*  personal injury resulting from such party's negligence to the       *
+*  extent applicable law prohibits such limitation. Some               *
+*  jurisdictions do not allow the exclusion or limitation of           *
+*  incidental or consequential damages, so this exclusion and          *
+*  limitation may not apply to You.                                    *
+*                                                                      *
+************************************************************************
+
+8. Litigation
+-------------
+
+Any litigation relating to this License may be brought only in the
+courts of a jurisdiction where the defendant maintains its principal
+place of business and such litigation shall be governed by laws of that
+jurisdiction, without reference to its conflict-of-law provisions.
+Nothing in this Section shall prevent a party's ability to bring
+cross-claims or counter-claims.
+
+9. Miscellaneous
+----------------
+
+This License represents the complete agreement concerning the subject
+matter hereof. If any provision of this License is held to be
+unenforceable, such provision shall be reformed only to the extent
+necessary to make it enforceable. Any law or regulation which provides
+that the language of a contract shall be construed against the drafter
+shall not be used to construe this License against a Contributor.
+
+10. Versions of the License
+---------------------------
+
+10.1. New Versions
+
+Mozilla Foundation is the license steward. Except as provided in Section
+10.3, no one other than the license steward has the right to modify or
+publish new versions of this License. Each version will be given a
+distinguishing version number.
+
+10.2. Effect of New Versions
+
+You may distribute the Covered Software under the terms of the version
+of the License under which You originally received the Covered Software,
+or under the terms of any subsequent version published by the license
+steward.
+
+10.3. Modified Versions
+
+If you create software not governed by this License, and you want to
+create a new license for such software, you may create and use a
+modified version of this License if you rename the license and remove
+any references to the name of the license steward (except to note that
+such modified license differs from this License).
+
+10.4. Distributing Source Code Form that is Incompatible With Secondary
+Licenses
+
+If You choose to distribute Source Code Form that is Incompatible With
+Secondary Licenses under the terms of this version of the License, the
+notice described in Exhibit B of this License must be attached.
+
+Exhibit A - Source Code Form License Notice
+-------------------------------------------
+
+  This Source Code Form is subject to the terms of the Mozilla Public
+  License, v. 2.0. If a copy of the MPL was not distributed with this
+  file, You can obtain one at http://mozilla.org/MPL/2.0/.
+
+If it is not possible or desirable to put the notice in a particular
+file, then You may include the notice in a location (such as a LICENSE
+file in a relevant directory) where a recipient would be likely to look
+for such a notice.
+
+You may add additional accurate notices of copyright ownership.
+
+Exhibit B - "Incompatible With Secondary Licenses" Notice
+---------------------------------------------------------
+
+  This Source Code Form is "Incompatible With Secondary Licenses", as
+  defined by the Mozilla Public License, v. 2.0.
diff --git a/Makefile b/Makefile
new file mode 100644
index 0000000..f90c279
--- /dev/null
+++ b/Makefile
@@ -0,0 +1,117 @@
+#
+# This Source Code Form is subject to the terms of the Mozilla Public
+# License, v. 2.0. If a copy of the MPL was not distributed with this
+# file, You can obtain one at http://mozilla.org/MPL/2.0/.
+#
+
+#
+# Copyright 2019 Joyent, Inc.
+#
+
+RUST_CODE = 1
+
+#
+# Files
+#
+DOC_FILES =		index.md
+JSON_FILES :=		package.json
+
+
+# While this component doesn't require a base image, we set this so
+# that validate-buildenv can determine whether we're building on
+# a recent enough image (including a reasonably recent version of rust)
+# triton-origin-x86_64-19.1.0
+BASE_IMAGE_UUID = cbf116a0-43a5-447c-ad8c-8fa57787351c
+
+#
+# Makefile.defs defines variables used as part of the build process.
+# Ensure we have the eng submodule before attempting to include it.
+#
+ENGBLD_REQUIRE := $(shell git submodule update --init deps/eng)
+include ./deps/eng/tools/mk/Makefile.defs
+TOP ?= $(error Unable to access eng.git submodule Makefiles.)
+
+include ./deps/eng/tools/mk/Makefile.smf.defs
+
+NAME :=			firewall-logger-agent
+RELEASE_TARBALL :=	$(NAME)-$(STAMP).tgz
+RELEASE_MANIFEST :=	$(NAME)-$(STAMP).manifest
+RELSTAGEDIR :=		/tmp/$(NAME)-$(STAMP)
+
+DISTCLEAN_FILES += $(NAME)-*.manifest $(NAME)-*.tgz
+
+#
+# Repo-specific targets
+#
+.PHONY: all
+all: $(SMF_MANIFESTS) | $(REPO_DEPS)
+
+.PHONY: rust
+rust:
+	@pkgin -y in rust
+
+.PHONY: cargo
+cargo: rust
+	@cargo build --release
+
+# Clean the target directory:
+TARGET_DIR ?= target
+DISTCLEAN_FILES += $(TARGET_DIR)
+
+.PHONY: release
+release: all cargo
+	@echo "Building $(RELEASE_TARBALL)"
+	@mkdir -p $(TOP)/bin
+	@cp $(TOP)/target/release/cfwlogd \
+		$(TOP)/bin/cfwlogd
+	@mkdir -p $(RELSTAGEDIR)/$(NAME)
+	cp -r \
+	    $(TOP)/bin \
+	    $(TOP)/npm \
+	    $(TOP)/package.json \
+	    $(TOP)/smf \
+	    $(TOP)/tools \
+	    $(RELSTAGEDIR)/$(NAME)
+	uuid -v4 >$(RELSTAGEDIR)/$(NAME)/image_uuid
+	cd $(RELSTAGEDIR) && $(TAR) -I pigz -cf $(TOP)/$(RELEASE_TARBALL) *
+	cat $(TOP)/manifest.tmpl | sed \
+	    -e "s/UUID/$$(cat $(RELSTAGEDIR)/$(NAME)/image_uuid)/" \
+	    -e "s/NAME/$$(json name < $(TOP)/package.json)/" \
+	    -e "s/VERSION/$$(json version < $(TOP)/package.json)/" \
+	    -e "s/DESCRIPTION/$$(json description < $(TOP)/package.json)/" \
+	    -e "s/BUILDSTAMP/$(STAMP)/" \
+	    -e "s/SIZE/$$(stat --printf="%s" $(TOP)/$(RELEASE_TARBALL))/" \
+	    -e "s/SHA/$$(openssl sha1 $(TOP)/$(RELEASE_TARBALL) \
+	    | cut -d ' ' -f2)/" \
+	    > $(TOP)/$(RELEASE_MANIFEST)
+	@rm -rf $(RELSTAGEDIR)
+
+.PHONY: publish
+publish: release
+	mkdir -p $(ENGBLD_BITS_DIR)/$(NAME)
+	cp $(TOP)/$(RELEASE_TARBALL) $(ENGBLD_BITS_DIR)/$(NAME)/$(RELEASE_TARBALL)
+	cp $(TOP)/$(RELEASE_MANIFEST) $(ENGBLD_BITS_DIR)/$(NAME)/$(RELEASE_MANIFEST)
+
+.PHONY: dumpvar
+dumpvar:
+	@if [[ -z "$(VAR)" ]]; then \
+	    echo "error: set 'VAR' to dump a var"; \
+	    exit 1; \
+	fi
+	@echo "$(VAR) is '$($(VAR))'"
+
+# Here "cutting a release" is just tagging the current commit with
+# "v(package.json version)". We don't publish this to npm.
+.PHONY: cutarelease
+cutarelease:
+	@echo "# Ensure working copy is clean."
+	[[ -z `git status --short` ]]  # If this fails, the working dir is dirty.
+	@echo "# Ensure have 'json' tool."
+	which json 2>/dev/null 1>/dev/null
+	ver=$(shell cat package.json | json version) && \
+	    git tag "v$$ver" && \
+	    git push origin "v$$ver"
+
+include ./deps/eng/tools/mk/Makefile.deps
+include ./deps/eng/tools/mk/Makefile.smf.targ
+include ./deps/eng/tools/mk/Makefile.targ
diff --git a/README.md b/README.md
new file mode 100644
index 0000000..6fde4f3
--- /dev/null
+++ b/README.md
@@ -0,0 +1,50 @@
+<!--y
+    This Source Code Form is subject to the terms of the Mozilla Public
+    License, v. 2.0. If a copy of the MPL was not distributed with this
+    file, You can obtain one at http://mozilla.org/MPL/2.0/.
+-->
+
+<!--
+    Copyright 2019 Joyent, Inc.
+-->
+
+This repository is part of the Joyent Triton project. See the [contribution
+guidelines](https://github.com/joyent/triton/blob/master/CONTRIBUTING.md) --
+*Triton does not use GitHub PRs* -- and general documentation at the main
+[Triton project](https://github.com/joyent/triton) page.
+
+# Triton Firewall Logger Agent
+
+For more information see [rfd-163](https://github.com/joyent/rfd/tree/master/rfd/0163)
+
+## Development
+
+Describe steps necessary for development here.
+
+    make all
+
+
+## Test
+
+Describe steps necessary for testing here.
+
+    make test
+
+## Documentation
+
+[Main docs file is at docs/index.md](docs/index.md).
+
+To update, edit "docs/index.md" and run `make docs` to generate/update
+"docs/index.html". Works on either SmartOS or Mac OS X.
+
+## Your Other Sections Here
+
+Add other sections to your README as necessary. E.g. Running a demo, adding
+development data.
+
+
+## License
+
+"Triton Firewall Logger" is licensed under the
+[Mozilla Public License version 2.0](http://mozilla.org/MPL/2.0/).
+See the file LICENSE.
diff --git a/cfwlogd/Cargo.toml b/cfwlogd/Cargo.toml
new file mode 100644
index 0000000..5f710d9
--- /dev/null
+++ b/cfwlogd/Cargo.toml
@@ -0,0 +1,23 @@
+[package]
+name = "cfwlogd"
+version = "0.1.0"
+authors = ["Mike Zeller <mike@mikezeller.net>"]
+edition = "2018"
+
+[dependencies]
+crossbeam = "0.7"
+vminfod = { path = "../vminfod" }
+serde = { version = "1.0", features = ["derive"] }
+serde_json = "1.0"
+nom = "4.2"
+uuid = { version = "0.7", features = ["serde", "v4"] }
+chrono = { version = "0.4", features = ["serde"] }
+pretty_env_logger = "0.3"
+log = "0.4"
+failure = "0.1"
+bytes = "0.4"
+signal-hook = "0.1"
+libc = "0.2"
+
+[dev-dependencies]
+testutils = { path = "../testutils" }
diff --git a/cfwlogd/src/events.rs b/cfwlogd/src/events.rs
new file mode 100644
index 0000000..cadbcaf
--- /dev/null
+++ b/cfwlogd/src/events.rs
@@ -0,0 +1,380 @@
+// This Source Code Form is subject to the terms of the Mozilla Public
+// License, v. 2.0. If a copy of the MPL was not distributed with this
+// file, You can obtain one at http://mozilla.org/MPL/2.0/.
+
+// Copyright 2019 Joyent, Inc.
+
+//! Event processing pipeline implementation that is responsible for reading from an `EventSource`
+//! and parsing the returned events and passing them off to a `Logger`
+
+use crate::logger::{self, Logger};
+use crate::parser::{self, CfwEvent};
+use crate::zones::{Vmobjs, Zonedid};
+use crossbeam::channel::{self, Receiver, Select, Sender, TrySendError};
+use std::collections::hash_map::Entry;
+use std::collections::HashMap;
+use std::sync::{Arc, Mutex};
+use std::thread;
+
+const RING_CAPACITY_MULTIPLIER: usize = 512;
+
+/// Holds a Mutex protected mapping of zonedid to Logging thread
+pub type Loggers = Arc<Mutex<HashMap<Zonedid, Logger>>>;
+
+/// Trait that represents a Firewall Event Source
+pub trait EventSource: Send {
+    /// Reads n events into the given buffer, returning how many bytes were read.
+    fn read_events(&mut self, buf: &mut [u8]) -> std::io::Result<usize>;
+    /// Returns a tuple that tells you the largest event size, and the max number of events that
+    /// may be returned in a single read.
+    fn event_sizing(&mut self) -> std::io::Result<(usize, usize)>;
+}
+
+/// Start a thread that consumes events from an `EventSource`.
+/// The consumed events will be sent to the returned `Receiver`.
+pub fn start_event_reader<T: EventSource + 'static>(
+    mut device: T,
+) -> (Receiver<CfwEvent>, thread::JoinHandle<()>) {
+    let (max, ringsize) = device.event_sizing().unwrap_or_else(|e| {
+        error!("failed to get ring size from device: {}", e);
+        std::process::exit(e.raw_os_error().unwrap_or(-1));
+    });
+    debug!(
+        "device responded with max event size: {}, ring size: {}",
+        &max, &ringsize
+    );
+    // This value is kind of picked out of thin air based on inital testing during development.
+    // It's quite possible this will need to be tuned at some point in the future. We could make
+    // the channel unbounded at the cost of some performance, but its probably a good idea to have
+    // some sort of backpressure control here so we don't endless grow in memory. Testing also
+    // showed that selecting a small ringsize provides the potential for cfwlogd to drop events
+    // so we set the ringsize lower bound to 1024.
+    let mut rs = std::cmp::max(1024, ringsize);
+    // We also set an upper bound to 2048 so that we don't needesly allocate a large chunk of
+    // memory at startup.
+    rs = std::cmp::min(2048, rs);
+    debug!(
+        "sizing the channel capacity to {} * {}",
+        rs, RING_CAPACITY_MULTIPLIER
+    );
+    let capacity = RING_CAPACITY_MULTIPLIER * rs;
+    let (tx, rx) = channel::bounded(capacity);
+    (
+        rx,
+        thread::Builder::new()
+            .name("EventReader".to_owned())
+            .spawn(move || {
+                // a buffer that can hold a full read of the ringbuffer
+                let mut buf = vec![0; max * ringsize];
+
+                loop {
+                    let size = match device.read_events(&mut buf) {
+                        Ok(size) => size,
+                        Err(e) => {
+                            // We failed to read from the EventSource so lets log the error and
+                            // drop our `Sender` so that we can shutdown gracefully"
+                            error!("failed to read from the EventSource: {:?}", e);
+                            break;
+                        }
+                    };
+
+                    if parse_events(&buf[..size], &tx) {
+                        // The recv channel is closed so we can stop reading events
+                        break;
+                    }
+                }
+            })
+            .expect("failed to start ipf reader thread"),
+    )
+}
+
+/// Takes a buffer of bytes and slices them up into `CfwEvent`s that are then sent to the provided
+/// `Sender`.
+fn parse_events(bytes: &[u8], sender: &Sender<CfwEvent>) -> bool {
+    let mut bytes = bytes;
+    loop {
+        // TRITON-XXX remove the unwrap once we handle unknown events
+        let (leftover, event) = parser::traffic_event(&bytes).unwrap();
+        if let Err(e) = sender.try_send(event) {
+            match e {
+                // Unfortunately we have to drop an event
+                TrySendError::Full(_dropped_event) => warn!(
+                    "processing channel is full ({} queued) so we are dropping an event",
+                    sender.len()
+                ),
+                // We are in the process of shutting down
+                TrySendError::Disconnected(_) => {
+                    info!("the event processing channel has disconnected");
+                    return true;
+                }
+            }
+        }
+        bytes = leftover;
+        if bytes.is_empty() {
+            break;
+        };
+    }
+    false
+}
+
+// Starts a thread that will receive `CfwEvent`s and fan them out to per zone logging threads.
+pub fn start_event_fanout(
+    events: Receiver<CfwEvent>,
+    shutdown: Receiver<()>,
+    vmobjs: Vmobjs,
+) -> (Loggers, thread::JoinHandle<()>) {
+    let loggers: Loggers = Arc::new(Mutex::new(HashMap::new()));
+    let loggers2 = Arc::clone(&loggers);
+    (
+        loggers,
+        thread::Builder::new()
+            .name("EventFanout".to_owned())
+            .spawn(move || fanout_events(events, shutdown, vmobjs, loggers2))
+            .expect("failed to start event fanout thread"),
+    )
+}
+
+fn fanout_events(
+    events: Receiver<CfwEvent>,
+    shutdown: Receiver<()>,
+    vmobjs: Vmobjs,
+    mut loggers: Loggers,
+) {
+    let mut sel = Select::new();
+    let events_ready = sel.recv(&events);
+    let shutdown_ready = sel.recv(&shutdown);
+
+    loop {
+        match sel.ready() {
+            // XXX handle Sender shutdown properly
+            // - Loop over the Loggers and tell them to shutdown
+            // - Wait for loggers shutdown
+            // - log the error and process::exit(1)
+            i if i == events_ready => {
+                // XXX coalesce events possibly?
+                thread::sleep(std::time::Duration::from_nanos(500_000));
+                queue_zone_events(
+                    events.try_iter().take(1024).collect(),
+                    &vmobjs,
+                    &mut loggers,
+                )
+            }
+            i if i == shutdown_ready => {
+                shutdown
+                    .recv()
+                    .expect("the signal rx should never outlive the tx");
+                debug!("event fanout thread received shutdown signal");
+                break;
+            }
+            _ => unreachable!(),
+        }
+    }
+
+    // Attempt to grab all of the events that are currently in the queue so we can
+    // shutdown the receiver as quickly as possible. We use "take()" here so that we
+    // don't end up in a situation where this processing thread is being blasted with
+    // incoming events and we can't disconnect in a timely manor.
+    drop(sel);
+    let drain: Vec<_> = events.try_iter().take(events.len()).collect();
+    drop(events);
+    debug!(
+        "event processing thread drained {} remaining events before shutdown",
+        drain.len()
+    );
+    queue_zone_events(drain, &vmobjs, &mut loggers);
+
+    info!("event processing thread exiting");
+}
+
+/// For a given cfw event, find or create a `Logger` thats responsible for serializing the event to
+/// disk.
+fn queue_zone_events(events: Vec<CfwEvent>, vmobjs: &Vmobjs, loggers: &mut Loggers) {
+    let mut loggers = loggers.lock().unwrap();
+    for event in events {
+        let zonedid = event.zone();
+        let logger = match loggers.entry(zonedid) {
+            Entry::Occupied(entry) => entry.into_mut(),
+            Entry::Vacant(entry) => match logger::start_logger(zonedid, Arc::clone(&vmobjs)) {
+                Some(logger) => {
+                    info!("new logging thread started for zonedid {}", zonedid);
+                    entry.insert(logger)
+                }
+                None => {
+                    // XXX CMON
+                    error!(
+                        "unable to match zonedid {} to vm object; dropping event",
+                        zonedid
+                    );
+                    return;
+                }
+            },
+        };
+        if logger.send(event).is_err() {
+            // Receive side of the log was disconnected somehow, so we drop the entry allowing it
+            // to be recreated on the next event.
+            // XXX CMON
+            error!(
+                "failed to log event for zone {} (logger channel disconnected)",
+                zonedid
+            );
+            loggers.remove(&zonedid);
+        }
+    }
+}
+
+#[cfg(test)]
+mod tests {
+    use super::*;
+    use crossbeam::sync::ShardedLock;
+    use std::time::Duration;
+
+    struct MockEventSource {}
+
+    impl EventSource for MockEventSource {
+        fn read_events(&mut self, buf: &mut [u8]) -> std::io::Result<usize> {
+            let event = testutils::generate_event();
+            let bytes = event.as_bytes();
+            assert!(
+                buf.len() >= bytes.len(),
+                "the provided buffer must be able to hold at least one event"
+            );
+            buf[..bytes.len()].copy_from_slice(bytes);
+            Ok(bytes.len())
+        }
+        fn event_sizing(&mut self) -> std::io::Result<(usize, usize)> {
+            Ok((testutils::generate_event().as_bytes().len(), 1))
+        }
+    }
+
+    #[test]
+    fn parse_events_test() {
+        let num_events = 10;
+        let (tx, rx) = crossbeam::channel::unbounded();
+        let event = testutils::generate_event();
+
+        let mut bytes = vec![];
+        std::iter::repeat(event.as_bytes())
+            .take(num_events)
+            .for_each(|b| bytes.extend_from_slice(b));
+
+        let done = parse_events(&bytes, &tx);
+
+        // Parse_events returns false because the channel is still open
+        assert!(!done);
+        // There should be num_events in the rx half of the channel
+        assert_eq!(
+            rx.len(),
+            num_events,
+            "the rx channel contains the same number of events passed in"
+        );
+
+        // drop the sender so we can easily iterate over all the events in the channel
+        drop(tx);
+        let cfwevent = parser::traffic_event(event.as_bytes()).unwrap().1;
+        for e in rx.iter() {
+            assert_eq!(cfwevent, e, "all events match the passed in events");
+        }
+    }
+
+    #[test]
+    fn start_event_reader_test() {
+        let device = MockEventSource {};
+        let (events, _handle) = start_event_reader(device);
+        let event = events.recv_timeout(Duration::from_secs(1));
+        assert!(
+            event.is_ok(),
+            "at least one event has made it through the returned rx channel"
+        );
+    }
+
+    #[test]
+    fn queue_zone_events_test() {
+        let mut loggers: Loggers = Arc::new(Mutex::new(HashMap::new()));
+        let vmobjs: Vmobjs = Arc::new(ShardedLock::new(HashMap::new()));
+
+        // Test that we don't create a logger for a zone we don't know about
+        let event = testutils::generate_event();
+        let cfwevent = parser::traffic_event(event.as_bytes()).unwrap().1;
+        queue_zone_events(vec![cfwevent], &vmobjs, &mut loggers);
+
+        let logs = loggers.lock().unwrap();
+        assert!(logs.is_empty(), "there were no loggers created");
+        drop(logs);
+
+        // Make a fake zone for our test
+        let zone1 = testutils::create_zone();
+        let event = testutils::generate_event_for_zone(&zone1);
+        let customer_uuid = zone1.owner_uuid.clone();
+
+        let mut vms = vmobjs.write().unwrap();
+        vms.insert(zone1.zonedid, zone1);
+        drop(vms);
+
+        // Test that we create a logger for a zone found in vmobjs
+        let cfwevent = parser::traffic_event(event.as_bytes()).unwrap().1;
+        queue_zone_events(vec![cfwevent], &vmobjs, &mut loggers);
+        let mut logs = loggers.lock().unwrap();
+        assert_eq!(logs.len(), 1, "there is exactly one logger created");
+        for (zonedid, logger) in logs.drain() {
+            assert_eq!(zonedid, event.zonedid, "zonedid's match");
+            // Shutdown the logger so the file flushes
+            logger.shutdown().expect("failed to shutdown logger");
+        }
+        drop(logs);
+
+        // make sure that the directory for the logs appeared and clean it up
+        assert_eq!(
+            true,
+            std::fs::remove_dir_all(format!("/var/tmp/cfwlogd-tests/{}", customer_uuid)).is_ok(),
+            "successfully cleaned up test files"
+        );
+    }
+
+    #[test]
+    fn start_event_fanout_test() {
+        let vmobjs: Vmobjs = Arc::new(ShardedLock::new(HashMap::new()));
+
+        let zone1 = testutils::create_zone();
+        let event = testutils::generate_event_for_zone(&zone1);
+        let customer_uuid = zone1.owner_uuid.clone();
+
+        let mut vms = vmobjs.write().unwrap();
+        vms.insert(zone1.zonedid, zone1);
+        drop(vms);
+
+        let cfwevent = parser::traffic_event(event.as_bytes()).unwrap().1;
+
+        let (tx, rx) = crossbeam::channel::unbounded();
+        let (stx, srx) = crossbeam::channel::unbounded();
+        let (loggers, handle) = start_event_fanout(rx, srx, Arc::clone(&vmobjs));
+
+        let logs = loggers.lock().unwrap();
+        assert!(logs.is_empty(), "no loggers exist yet");
+        drop(logs);
+
+        assert!(
+            tx.send(cfwevent).is_ok(),
+            "successfully sent CfwEvent to fanout thread"
+        );
+
+        assert!(stx.send(()).is_ok(), "shutdown signal sent");
+        // should be plenty of time for the thread to shutdown
+        thread::sleep(Duration::from_millis(500));
+        let cfwevent = parser::traffic_event(event.as_bytes()).unwrap().1;
+        assert!(
+            tx.send(cfwevent).is_err(),
+            "thread should no longer be accepting CfwEvents"
+        );
+        // XXX not sure how to deal with this potentially hanging forever other than setting a
+        // timeout in a test runner like jenkins. Although if we made it this far in the test then
+        // its looking like the thread has already actually shutdown.
+        assert!(handle.join().is_ok(), "thread shutdown");
+
+        // make sure that the directory for the logs appeared and clean it up
+        assert_eq!(
+            true,
+            std::fs::remove_dir_all(format!("/var/tmp/cfwlogd-tests/{}", customer_uuid)).is_ok(),
+            "successfully cleaned up test files"
+        );
+    }
+}
diff --git a/cfwlogd/src/fileutils.rs b/cfwlogd/src/fileutils.rs
new file mode 100644
index 0000000..9764c4a
--- /dev/null
+++ b/cfwlogd/src/fileutils.rs
@@ -0,0 +1,134 @@
+use std::io;
+use std::io::prelude::*;
+use std::io::{Seek, SeekFrom};
+
+/// Iterates through the given bytes backwards looking for the provided char
+fn rev_locate_char(bytes: &[u8], c: u8) -> Option<usize> {
+    bytes.iter().rposition(|p| *p == c)
+}
+
+/// Seeks to the end of a given `Reader` and processes a chunk of bytes looking for a specific
+/// char. This processes is repeated until the given char is found, otherwise returns None if its
+/// not found. This function panics if the chunk size is 0.
+pub fn rseek_and_scan<R: Read + Seek>(r: &mut R, chunk: u64, c: u8) -> io::Result<Option<u64>> {
+    assert!(chunk > 0, "chunk size must be greater than 0");
+    let mut buf = String::with_capacity(chunk as usize);
+    // Track the overall seek offset
+    let mut ptr = r.seek(SeekFrom::End(0))?;
+    // Used to calculate the next seek offset when reading a chunk
+    let mut pos = ptr;
+
+    loop {
+        // Set the buffer len back to 0 leaving capacity alone.
+        buf.truncate(0);
+        pos = pos.saturating_sub(chunk);
+        // Seek to the next chunk of bytes.
+        r.seek(SeekFrom::Start(pos))?;
+        // Avoid overlapping chunks when there is less than a chunks worth of data left
+        let max = if ptr < chunk { ptr } else { chunk };
+        // Read the max number of bytes into the buffer.
+        r.take(max).read_to_string(&mut buf)?;
+
+        // Attempt to locate the char.
+        if let Some(found) = rev_locate_char(buf.as_bytes(), c) {
+            return Ok(Some(found as u64 + pos));
+        }
+
+        // If our pos is 0 there's nothing left to read. Using `saturating_sub` guarantees we
+        // will never dip below 0.
+        if pos == 0 {
+            break;
+        };
+
+        // Move our offset pointer to reflect how much was read and where we are in the Reader
+        ptr -= max;
+    }
+    Ok(None)
+}
+
+#[cfg(test)]
+mod test {
+    use super::*;
+    use std::io::Cursor;
+
+    #[test]
+    fn test_beginning() {
+        let mut data = Cursor::new(vec![b'\n', 2, 3, 4, 5]);
+        let pos = rseek_and_scan(&mut data, 100, b'\n').unwrap();
+        assert_eq!(Some(0), pos, "newline is in the first position");
+        assert_eq!(
+            b'\n',
+            data.get_ref()[pos.unwrap() as usize],
+            r#"byte should be \n"#
+        );
+    }
+
+    #[test]
+    fn test_middle() {
+        let mut data = Cursor::new(vec![1, 2, 3, b'\n', 5, 6, 7]);
+        let pos = rseek_and_scan(&mut data, 100, b'\n').unwrap();
+        assert_eq!(Some(3), pos, "newline is in the middle position");
+        assert_eq!(
+            b'\n',
+            data.get_ref()[pos.unwrap() as usize],
+            r#"byte should be \n"#
+        );
+    }
+
+    #[test]
+    fn test_end() {
+        let mut data = Cursor::new(vec![1, 2, 3, 4, 5, b'\n']);
+        let pos = rseek_and_scan(&mut data, 100, b'\n').unwrap();
+        assert_eq!(Some(5), pos, "newline is in the last position");
+        assert_eq!(
+            b'\n',
+            data.get_ref()[pos.unwrap() as usize],
+            r#"byte should be \n"#
+        );
+    }
+
+    #[test]
+    fn test_reading_single_byte() {
+        let mut data = Cursor::new(vec![b'\n', 2, 3, 4, 5]);
+        let pos = rseek_and_scan(&mut data, 1, b'\n').unwrap();
+        assert_eq!(Some(0), pos, "newline is in the first position");
+        assert_eq!(
+            b'\n',
+            data.get_ref()[pos.unwrap() as usize],
+            r#"byte should be \n"#
+        );
+    }
+
+    #[test]
+    fn test_overlapping_chunks() {
+        let mut data = Cursor::new(vec![b'\n', 2, 3, 4, 5]);
+        let pos = rseek_and_scan(&mut data, 2, b'\n').unwrap();
+        assert_eq!(Some(0), pos, "newline is in the first position");
+        assert_eq!(
+            b'\n',
+            data.get_ref()[pos.unwrap() as usize],
+            r#"byte should be \n"#
+        );
+    }
+
+    #[test]
+    fn test_multiple_newlines() {
+        let mut data = Cursor::new(vec![1, 2, b'\n', 4, b'\n', b'\n']);
+        let pos = rseek_and_scan(&mut data, 100, b'\n').unwrap();
+        assert_eq!(Some(5), pos, "newline is in the last position");
+        assert_eq!(
+            b'\n',
+            data.get_ref()[pos.unwrap() as usize],
+            r#"byte should be \n"#
+        );
+
+        data.get_mut().remove(pos.unwrap() as usize);
+        let pos = rseek_and_scan(&mut data, 50, b'\n').unwrap();
+        assert_eq!(Some(4), pos, "newline is in the last position");
+        assert_eq!(
+            b'\n',
+            data.get_ref()[pos.unwrap() as usize],
+            r#"byte should be \n"#
+        );
+    }
+}
diff --git a/cfwlogd/src/ipf.rs b/cfwlogd/src/ipf.rs
new file mode 100644
index 0000000..ef17cf2
--- /dev/null
+++ b/cfwlogd/src/ipf.rs
@@ -0,0 +1,66 @@
+// This Source Code Form is subject to the terms of the Mozilla Public
+// License, v. 2.0. If a copy of the MPL was not distributed with this
+// file, You can obtain one at http://mozilla.org/MPL/2.0/.
+
+// Copyright 2019 Joyent, Inc.
+
+//! This is the implementation of an EventSource for ipfilter.
+
+use crate::events::EventSource;
+use libc::c_int;
+use std::fs::File;
+use std::io::Read;
+use std::os::unix::io::AsRawFd;
+use std::path::Path;
+
+/// SIOCIPFCFWCFG generated from:
+/// _IOR('r', 98, struct ipfcfwcfg)
+/// This _MUST_ be recomputed if any changes to `Ipfcfwcfg` are made.
+const SIOCIPFCFWCFG: c_int = 1_075_343_970;
+
+// C representation of a cfw event
+#[repr(C)]
+#[derive(Default)]
+pub struct Ipfcfwcfg {
+    pub max_event_size: u32,
+    pub ring_size: u32,
+    pub num_reports: u64,
+    pub num_drops: u64,
+}
+
+/// Wrapper around the ipfev device
+pub struct IpfevDevice {
+    fd: File,
+}
+
+impl IpfevDevice {
+    /// Create a new `IpfevDevice`
+    pub fn new<P: AsRef<Path>>(device: P) -> std::io::Result<Self> {
+        let device = device.as_ref();
+        let fd = File::open(device)?;
+        info!("connected to {}", device.display());
+        Ok(IpfevDevice { fd })
+    }
+}
+
+impl EventSource for IpfevDevice {
+    /// Pull at least one event from the ipfev device into the provided buffer
+    fn read_events(&mut self, buf: &mut [u8]) -> std::io::Result<usize> {
+        let size = self.fd.read(buf)?;
+        // The device should never return from a read with 0 bytes
+        assert!(size > 0);
+        Ok(size)
+    }
+
+    /// Dynamically read the largest known event size and the current sizing of the ring buffer
+    fn event_sizing(&mut self) -> std::io::Result<(usize, usize)> {
+        let mut cfg = Ipfcfwcfg::default();
+        unsafe {
+            let cfg_ptr = &mut cfg as *mut Ipfcfwcfg;
+            match libc::ioctl(self.fd.as_raw_fd(), SIOCIPFCFWCFG, cfg_ptr) {
+                -1 => Err(std::io::Error::last_os_error()),
+                _ => Ok((cfg.max_event_size as usize, cfg.ring_size as usize)),
+            }
+        }
+    }
+}
diff --git a/cfwlogd/src/logger.rs b/cfwlogd/src/logger.rs
new file mode 100644
index 0000000..9ad767d
--- /dev/null
+++ b/cfwlogd/src/logger.rs
@@ -0,0 +1,300 @@
+// This Source Code Form is subject to the terms of the Mozilla Public
+// License, v. 2.0. If a copy of the MPL was not distributed with this
+// file, You can obtain one at http://mozilla.org/MPL/2.0/.
+
+// Copyright 2019 Joyent, Inc.
+
+use crate::parser::CfwEvent;
+use crate::zones::{Vmobjs, Zonedid};
+use crossbeam::channel::{self, Select, SendError};
+use failure::Error;
+use serde::Serialize;
+use std::fs::File;
+use std::fs::OpenOptions;
+use std::io::{BufWriter, Write};
+use std::path::PathBuf;
+use std::sync::Arc;
+use std::thread;
+
+/// Configure where log files will be created
+#[cfg(not(test))]
+const LOG_DIR: &str = "/var/log/firewall";
+#[cfg(test)]
+const LOG_DIR: &str = "/var/tmp/cfwlogd-tests";
+
+/// Capacity used for Logger's BufWriter.  This may need to be tuned later.
+const BUF_SIZE: usize = 1024 * 1024;
+
+#[derive(Debug, Serialize)]
+struct LogEvent<'a> {
+    #[serde(flatten)]
+    event: CfwEvent,
+    vm: &'a str,
+    alias: &'a str,
+}
+
+/// A signal that can be sent to the logger
+#[derive(PartialEq)]
+pub enum LoggerSignal {
+    /// Tell the thread to flush and continue
+    Flush,
+    /// Tell the thread to flush and shutdown
+    Shutdown,
+    /// Tell the thread to flush and rotate the log file
+    Rotate,
+}
+
+/// A Logger represents a thread tied to a specific zone that is responsible for logging out
+/// CfwEvents to disk.
+pub struct Logger {
+    /// Zone UUID
+    pub uuid: String,
+    /// Threads handle
+    handle: thread::JoinHandle<()>,
+    /// Send half of a channel that's used to get CfwEvents into the Logger
+    sender: channel::Sender<CfwEvent>,
+    /// Send half of a channel that's used to signal the Logger to preform specific actions
+    signal: channel::Sender<LoggerSignal>,
+}
+
+impl Logger {
+    /// Send an event to the logger to be logged out to disk
+    pub fn send(&self, e: CfwEvent) -> Result<(), SendError<CfwEvent>> {
+        self.sender.send(e)
+    }
+
+    /// Flushes the loggers internal `BufWriter` to disk
+    pub fn flush(&self) -> Result<(), SendError<LoggerSignal>> {
+        self.signal.send(LoggerSignal::Flush)
+    }
+
+    /// Flushes the loggers internal `BufWriter` to disk, and reopens "current.log"
+    pub fn rotate(&self) -> Result<(), SendError<LoggerSignal>> {
+        self.signal.send(LoggerSignal::Rotate)
+    }
+
+    /// Flushes the loggers internal `BufWriter` to disk, and shutdowns the `Logger`, therefore
+    /// requiring ownership of self to be consumed.
+    pub fn shutdown(self) -> Result<(), SendError<LoggerSignal>> {
+        self.signal.send(LoggerSignal::Shutdown).and_then(|_| {
+            self.handle.join().unwrap();
+            Ok(())
+        })
+    }
+}
+
+/// Open "current.log" in "RW" for the given customer and zone.
+fn open_file(vm: &str, customer: &str) -> std::io::Result<File> {
+    let path: PathBuf = [LOG_DIR, customer, vm, "current.log"].iter().collect();
+    // we know the unwrap is safe because we just created the path above
+    std::fs::create_dir_all(path.parent().unwrap())?;
+    Ok(OpenOptions::new().append(true).create(true).open(path)?)
+}
+
+/// Given a collection of `CfwEvent`, serialize them out to disk as JSON formatted logs.
+fn log_events<W: Write>(
+    events: Vec<CfwEvent>,
+    mut writer: W,
+    vmobjs: &Vmobjs,
+) -> Result<(), Error> {
+    // force the event type for now
+    let vmobjs = vmobjs.read().unwrap();
+    for event in events {
+        let vmobj = vmobjs
+            .get(&event.zone())
+            .expect("we should have the zonedid:uuid mapping already");
+        // Check if the zone has an alias set, if not we provide a default one
+        // Note instead of String::as_ref we could also use "|s| &**s"
+        let alias = vmobj.alias.as_ref().map_or("", String::as_ref);
+        let event = LogEvent {
+            event,
+            vm: &vmobj.uuid,
+            alias: &alias,
+        };
+        // TODO these errors no longer make sense when its multiple events being processed
+        serde_json::to_writer(&mut writer, &event)?;
+        writer.write_all(b"\n")?;
+    }
+    Ok(())
+}
+
+fn _start_logger(
+    vm: String,
+    customer: String,
+    vmobjs: Vmobjs,
+    events: channel::Receiver<CfwEvent>,
+    signal: channel::Receiver<LoggerSignal>,
+) -> thread::JoinHandle<()> {
+    thread::Builder::new()
+        .name("logger".to_owned())
+        .spawn(move || {
+            let file = match open_file(&vm, &customer) {
+                Ok(file) => file,
+                Err(e) => {
+                    // XXX CMON?
+                    error!("failed to open log file: {}", e);
+                    return;
+                }
+            };
+
+            let mut writer = BufWriter::with_capacity(BUF_SIZE, file);
+            let mut sel = Select::new();
+            let events_ready = sel.recv(&events);
+            let signal_ready = sel.recv(&signal);
+            loop {
+                match sel.ready() {
+                    i if i == events_ready => {
+                        thread::sleep(std::time::Duration::from_nanos(500_000));
+                        if let Err(e) =
+                            log_events(events.try_iter().take(1024).collect(), &mut writer, &vmobjs)
+                        {
+                            error!("failed to log event(s): {}", e);
+                        }
+                    }
+                    i if i == signal_ready => {
+                        // TODO handle disconnected channel
+                        if let Ok(signal) = signal.recv() {
+                            match signal {
+                                LoggerSignal::Rotate => {
+                                    let _ = writer.flush();
+                                    let file = match open_file(&vm, &customer) {
+                                        Ok(file) => file,
+                                        Err(e) => {
+                                            // XXX CMON?
+                                            error!("failed to open log file after rotation: {}", e);
+                                            return;
+                                        }
+                                    };
+                                    // drop the old writer and create a new one
+                                    writer = BufWriter::with_capacity(1024 * 1024, file);
+                                }
+                                LoggerSignal::Shutdown => break,
+                                LoggerSignal::Flush => {
+                                    // XXX handle error
+                                    info!("flushing log for {}", &vm);
+                                    let _ = writer.flush();
+                                }
+                            }
+                        }
+                    }
+                    _ => unreachable!(),
+                }
+            }
+
+            // We are shutting down now so we drain the channel and then drop it
+            if let Err(e) = log_events(events.try_iter().collect(), &mut writer, &vmobjs) {
+                error!("failed to log event: {}", e);
+            }
+            drop(sel);
+            drop(events);
+            let _res = writer.flush();
+        })
+        .expect("failed to spawn IpfReader thread")
+}
+
+/// Return a Logger if we have information for the zone already otherwise return None
+pub fn start_logger(zonedid: Zonedid, vmobjs: Vmobjs) -> Option<Logger> {
+    let (event_tx, event_rx) = channel::unbounded();
+    let (signal_tx, signal_rx) = channel::bounded(1);
+    let vms = vmobjs.read().unwrap();
+    if let Some(vm) = vms.get(&zonedid) {
+        let handle = _start_logger(
+            vm.uuid.clone(),
+            vm.owner_uuid.clone(),
+            Arc::clone(&vmobjs),
+            event_rx,
+            signal_rx,
+        );
+        return Some(Logger {
+            uuid: vm.uuid.clone(),
+            handle,
+            sender: event_tx,
+            signal: signal_tx,
+        });
+    }
+    None
+}
+
+#[cfg(test)]
+mod tests {
+    use super::*;
+    use crate::parser;
+    use crate::zones::Vmobjs;
+    use crossbeam::sync::ShardedLock;
+    use std::collections::HashMap;
+    use std::io::Read;
+    use std::path::PathBuf;
+
+    #[test]
+    fn open_file_test() {
+        let vm = "zone1";
+        let customer = "customer1";
+        let _f = open_file(vm, customer).expect("failed to open file");
+        let mut path: PathBuf = [LOG_DIR, customer, vm, "current.log"].iter().collect();
+        assert!(path.as_path().is_file(), "current.log file path is correct");
+        path.pop(); //current.log
+        path.pop(); // zone name
+        std::fs::remove_dir_all(path).expect("failed to cleanup log dir");
+    }
+
+    #[test]
+    fn log_events_test() {
+        let num_events = 4;
+        let vmobjs: Vmobjs = Arc::new(ShardedLock::new(HashMap::new()));
+
+        let zone1 = testutils::create_zone();
+
+        let events = std::iter::repeat_with(|| {
+            let event = testutils::generate_event_for_zone(&zone1);
+            parser::traffic_event(event.as_bytes()).unwrap().1
+        })
+        .take(num_events)
+        .collect();
+
+        let mut vms = vmobjs.write().unwrap();
+        vms.insert(zone1.zonedid, zone1);
+        drop(vms);
+
+        let mut writer = vec![];
+        assert!(
+            log_events(events, &mut writer, &vmobjs).is_ok(),
+            "logs written to writer"
+        );
+
+        let mut buf = String::new();
+        writer
+            .as_slice()
+            .read_to_string(&mut buf)
+            .expect("failed to read all of the bytes from the writer");
+
+        let lines: Vec<&str> = buf.lines().collect();
+
+        assert_eq!(
+            num_events,
+            lines.len(),
+            "all events were written to the writer"
+        );
+    }
+
+    #[test]
+    fn start_logger_test() {
+        let vmobjs: Vmobjs = Arc::new(ShardedLock::new(HashMap::new()));
+        let logger = start_logger(10, Arc::clone(&vmobjs));
+        assert!(
+            logger.is_none(),
+            "no logger is created for an unknown zonedid",
+        );
+
+        let zone1 = testutils::create_zone();
+        let zonedid = zone1.zonedid;
+        let mut vms = vmobjs.write().unwrap();
+        vms.insert(zone1.zonedid, zone1);
+        drop(vms);
+
+        let logger = start_logger(zonedid, Arc::clone(&vmobjs));
+        assert!(
+            logger.is_some(),
+            "logger is created when we have the correct zone info",
+        );
+    }
+}
diff --git a/cfwlogd/src/main.rs b/cfwlogd/src/main.rs
new file mode 100644
index 0000000..29b405a
--- /dev/null
+++ b/cfwlogd/src/main.rs
@@ -0,0 +1,246 @@
+// This Source Code Form is subject to the terms of the Mozilla Public
+// License, v. 2.0. If a copy of the MPL was not distributed with this
+// file, You can obtain one at http://mozilla.org/MPL/2.0/.
+
+// Copyright 2019 Joyent, Inc.
+
+//! cfwlogd is a userland daemon that is responsible for translating firewall events into json
+//! logs.  It does so by attaching to a kernel device such as `/dev/ipfev` and reading in a
+//! buffer of bytes.  cfwlogd is then responsible for parsing the buffer into cloud
+//! firewall logs that will be serialized out to disk as json in
+//! `/var/log/firewall/<customer>/<vm>/<timestamp>.log`
+//!
+//!
+//!                      cfwlogd's current design
+//!
+//!
+//! gz process 127.0.0.1:9090
+//! +-------------------+
+//! |  vminfod process  |
+//! +-------------------+
+//!         |
+//!         |
+//!         v                Kernel Device
+//!   Vminfod Thread         +------------+
+//! +-----------------+      | /dev/ipfev |
+//! | vminfod watcher |      +------------+
+//! +-----------------+            |
+//!         |                      v
+//!         |                Userland Daemon
+//!         |                  +---------+
+//!         -----------------> | cfwlogd |
+//!                            +---------+
+//!                                |
+//!                                v
+//!                       Processing Threads
+//!                  -----------------------------
+//!                  |             |             |
+//!                +-----+      +-----+      +-----+
+//!                | vm1 |      | vm2 |      | vm3 |
+//!                +-----+      +-----+      +-----+
+//!                  |             |             |
+//!                  v             v             v
+//!              +--------+    +--------+    +--------+
+//!              |vm1.log |    |vm2.log |    |vm3.log |
+//!              +--------+    +--------+    +--------+
+//!
+//!
+//! In summary, cfwlogd must correlate the firewall log events it gets from the kernel with the
+//! apprpriate zone information that contains the customer uuid, the vm uuid, and the vm  alias.
+//! It does this by spawning a `vminfod` watcher thread that is responsible for tracking vminfod
+//! events as they arrive. Each binary payload from the kernel contains a `zonedid` which is a
+//! unique id per vm until the box is rebooted, this allows cfwlogd to keep track of what event is
+//! for what vm. Also, becauase cfwlogd gets vminfod events in real time it is able to track things
+//! like vm alias changes. As events are read from the kernel cfwlogd looks for an existing logging
+//! thread or creates one if its the first event it has seen for a specific zone. Each logging
+//! thread serialzes the internal data structure to json and writes the log to a `BufWriter` which
+//! has its own internall buffer that will flush to disk once full, this is to cut down on the
+//! number of write syscalls cfwlogd has to make.
+
+use crossbeam::channel;
+use crossbeam::sync::ShardedLock;
+use failure::Error;
+use libc::c_int;
+use std::collections::HashMap;
+use std::fs::OpenOptions;
+use std::path::PathBuf;
+use std::sync::Arc;
+
+#[macro_use]
+extern crate nom;
+#[macro_use]
+extern crate log;
+
+mod events;
+mod fileutils;
+mod ipf;
+mod logger;
+mod parser;
+mod signal;
+mod zones;
+use events::Loggers;
+use ipf::IpfevDevice;
+
+// Method successfully but purposefully leaves no processes remaining in the
+// contract; it should be treated as if it had a transient service model.
+const SMF_EXIT_NODAEMON: i32 = 94;
+
+/// Process incoming unix signals. Returns `true` if the signal indicates that
+/// we should shutdown the process.
+fn handle_signal(s: c_int, loggers: &Loggers) -> bool {
+    let mut shutdown = false;
+    match s {
+        // XXX might be interesting to have USR1 export some statistics
+        libc::SIGUSR1 => info!("SIGUSR1: caught sigusr1"),
+        libc::SIGUSR2 => {
+            info!("SIGUSR2: flushing logs");
+            let loggers = loggers.lock().unwrap();
+            for logger in loggers.values() {
+                if logger.flush().is_err() {
+                    error!(
+                        "Failed to flush logs for {} because the logger is no \
+                         longer listening on its signal handler",
+                        &logger.uuid
+                    );
+                }
+            }
+        }
+        libc::SIGHUP => {
+            info!(
+                "SIGHUP: log rotation -- flushing currently opened files and \
+                 reopening current.log"
+            );
+            let loggers = loggers.lock().unwrap();
+            for logger in loggers.values() {
+                if logger.rotate().is_err() {
+                    error!(
+                        "Failed to rotate logs for {} because the logger is no \
+                         longer listening on its signal handler",
+                        &logger.uuid
+                    );
+                }
+            }
+        }
+        libc::SIGINT => {
+            info!("SIGINT: shutting down");
+            shutdown = true;
+        }
+        libc::SIGTERM => {
+            info!("SIGTERM: shutting down");
+            shutdown = true;
+        }
+        val => debug!("ignoring signal: {}", val),
+    }
+    shutdown
+}
+
+fn main() -> Result<(), Error> {
+    // We may want bunyan here or some other log crate that implements "log"
+    pretty_env_logger::init();
+
+    let vmobjs = Arc::new(ShardedLock::new(HashMap::new()));
+    let _vminfod_handle = zones::start_vminfod(Arc::clone(&vmobjs));
+
+    // Now that we have a list of zones, we iterate through them looking for any existing log files
+    // that may have incomplete data from a previous run of cfwlogd, such as a crash or a SIGKILL.
+    // If we find an existing log file we attempt to scan the file backwards looking for a "\n" to
+    // ensure that the log file does not end with incomplete json logs. If we find a "\n" we
+    // truncate the file to that position, and if we dont find one then we truncate the log all
+    // together. Note that this currently doesn't account for a zone that was deleted between now
+    // and the time where cfwlogd was torn down without a chance to clean up.
+    let zones = vmobjs.read().unwrap();
+    for zone in zones.values() {
+        let path: PathBuf = [
+            "/var/log/firewall",
+            &zone.owner_uuid,
+            &zone.uuid,
+            "current.log",
+        ]
+        .iter()
+        .collect();
+
+        // If the file doesn't exist on disk we can skip over it
+        if !path.is_file() {
+            continue;
+        };
+
+        match OpenOptions::new().read(true).write(true).open(&path) {
+            Ok(mut file) => match fileutils::rseek_and_scan(&mut file, 512, b'\n') {
+                Ok(idx) => {
+                    // If we find the "\n" we add 1 to the index so that when we truncate the file
+                    // we keep the newline.
+                    let idx = idx.and_then(|i| Some(i + 1)).unwrap_or(0);
+                    if let Err(e) = file.set_len(idx) {
+                        error!("failed to truncate file ({}): {}", &path.display(), e);
+                    } else {
+                        debug!("tuncated {} to {}", &path.display(), idx);
+                    }
+                }
+                Err(e) => {
+                    error!(
+                        "failed to scan file ({}) for a newline: {}",
+                        &path.display(),
+                        e
+                    );
+                }
+            },
+            Err(e) => error!(
+                "failed to open file ({}) for cleanup: {}",
+                path.display(),
+                e
+            ),
+        }
+    }
+    drop(zones);
+
+    // This is unbounded so that we don't block in the signal handler
+    let (sig_tx, sig_rx) = channel::unbounded();
+    let (shutdown_tx, shutdown_rx) = channel::bounded(1);
+
+    let device = IpfevDevice::new("/dev/ipfev").unwrap_or_else(|e| match e.kind() {
+        // The device was not found but ipfilter is online because the smf dependency requires it
+        // to be up before starting, therefore we are on a platform that doesn't support ipfev. So
+        // we exit with SMF_EXIT_NODAEMON to indicate success leaving no process running.
+        std::io::ErrorKind::NotFound => {
+            info!(
+                "/dev/ipfev not present on this system -- \
+                 treating the daemon as a transient service "
+            );
+            std::process::exit(SMF_EXIT_NODAEMON);
+        }
+        // Anything other than NotFound should be treated as a hard error.
+        _ => {
+            error!("failed to open /dev/ipfev: {}", e);
+            std::process::exit(e.raw_os_error().unwrap_or(-1));
+        }
+    });
+
+    // Setup our processing pipeline
+    let (ipf_events, _ipf_handle) = events::start_event_reader(device);
+    let (loggers, fanout_handle) =
+        events::start_event_fanout(ipf_events, shutdown_rx, Arc::clone(&vmobjs));
+    let _signal_handle = signal::start_signalhandler(sig_tx);
+
+    // Handle signals until we are told to exit
+    for sig in sig_rx.iter() {
+        if handle_signal(sig, &loggers) {
+            break;
+        };
+    }
+
+    // Wait for the event processor to drain all queued events into its loggers
+    shutdown_tx.send(()).unwrap();
+    fanout_handle.join().unwrap();
+
+    // Wait for loggers to finish flushing to disk
+    let mut loggers = loggers.lock().unwrap();
+    for (zonedid, logger) in loggers.drain() {
+        info!(
+            "shutting down logging thread for {} ({})",
+            logger.uuid, zonedid
+        );
+        logger.shutdown().unwrap();
+    }
+
+    Ok(())
+}
diff --git a/cfwlogd/src/parser.rs b/cfwlogd/src/parser.rs
new file mode 100644
index 0000000..72ffc27
--- /dev/null
+++ b/cfwlogd/src/parser.rs
@@ -0,0 +1,203 @@
+// This Source Code Form is subject to the terms of the Mozilla Public
+// License, v. 2.0. If a copy of the MPL was not distributed with this
+// file, You can obtain one at http://mozilla.org/MPL/2.0/.
+
+// Copyright 2019 Joyent, Inc.
+
+use std::net::Ipv6Addr;
+
+use chrono::{DateTime, TimeZone, Utc};
+use nom::{be_u128, be_u16};
+use serde::Serialize;
+use uuid::Uuid;
+
+#[cfg(target_endian = "little")]
+const NATIVE_ENDIAN: nom::Endianness = nom::Endianness::Little;
+
+#[cfg(target_endian = "big")]
+const NATIVE_ENDIAN: nom::Endianness = nom::Endianness::Big;
+
+#[derive(Debug, PartialEq, Serialize)]
+#[serde(rename_all = "lowercase")]
+pub enum CfwEvType {
+    Block,
+    Begin,
+    End,
+    Unknown, // Catch all (perhaps the kernel version is ahead of userland?)
+}
+
+impl From<u16> for CfwEvType {
+    fn from(val: u16) -> CfwEvType {
+        match val {
+            1 => CfwEvType::Block,
+            2 => CfwEvType::Begin,
+            3 => CfwEvType::End,
+            _ => CfwEvType::Unknown,
+        }
+    }
+}
+
+#[derive(Debug, PartialEq, Serialize)]
+#[serde(rename_all = "lowercase")]
+pub enum Direction {
+    In,
+    Out,
+}
+
+impl From<u8> for Direction {
+    fn from(val: u8) -> Direction {
+        match val {
+            1 => Direction::In,
+            2 => Direction::Out,
+            // This should never happen
+            _ => panic!("unknown direction: {}", val),
+        }
+    }
+}
+
+#[derive(Debug, PartialEq, Serialize)]
+pub enum Protocol {
+    AH,
+    ESP,
+    ICMP,
+    ICMPV6,
+    TCP,
+    UDP,
+    UNKNOWN,
+}
+
+impl From<u8> for Protocol {
+    fn from(val: u8) -> Protocol {
+        match val {
+            1 => Protocol::ICMP,
+            6 => Protocol::TCP,
+            17 => Protocol::UDP,
+            50 => Protocol::ESP,
+            51 => Protocol::AH,
+            58 => Protocol::ICMPV6,
+            // catch all in case fwadm ever supports additional protocols
+            _ => Protocol::UNKNOWN,
+        }
+    }
+}
+
+#[derive(Debug, Serialize, PartialEq)]
+#[serde(untagged)]
+pub enum CfwEvent {
+    Traffic(TrafficEvent),
+}
+
+impl CfwEvent {
+    pub fn zone(&self) -> u32 {
+        match &*self {
+            CfwEvent::Traffic(event) => event.zonedid,
+        }
+    }
+}
+
+#[derive(Debug, PartialEq, Serialize)]
+pub struct TrafficEvent {
+    pub event: CfwEvType,
+    #[serde(skip)]
+    pub length: u16,
+    #[serde(skip)]
+    pub zonedid: u32,
+    #[serde(skip)]
+    pub rule_id: u32,
+    pub source_port: u16,
+    pub destination_port: u16,
+    pub protocol: Protocol,
+    pub direction: Direction,
+    pub source_ip: Ipv6Addr,
+    pub destination_ip: Ipv6Addr,
+    pub timestamp: DateTime<Utc>,
+    #[serde(rename = "rule")]
+    pub rule_uuid: Uuid,
+}
+
+// Parse an entire event
+named!(pub traffic_event( &[u8] ) -> CfwEvent,
+    do_parse!(
+        event: u16!(NATIVE_ENDIAN) >>
+        length: u16!(NATIVE_ENDIAN) >>
+        zonedid: u32!(NATIVE_ENDIAN) >>
+        rule_id: u32!(NATIVE_ENDIAN) >>
+        source_port: be_u16 >>
+        destination_port: be_u16 >>
+        protocol: take!(1) >>
+        direction: take!(1) >>
+        // padding
+        _reserved: take!(6) >>
+        source_ip: be_u128 >>
+        destination_ip: be_u128 >>
+        time_sec: i64!(NATIVE_ENDIAN) >>
+        time_usec: i64!(NATIVE_ENDIAN) >>
+        rule_uuid: take!(16) >>
+        (CfwEvent::Traffic(TrafficEvent{
+                event: CfwEvType::from(event),
+                length,
+                zonedid,
+                rule_id,
+                protocol: Protocol::from(protocol[0]),
+                direction: Direction::from(direction[0]),
+                source_port,
+                destination_port,
+                source_ip: Ipv6Addr::from(source_ip),
+                destination_ip: Ipv6Addr::from(destination_ip),
+                timestamp: Utc.timestamp(time_sec, (time_usec * 1000) as u32),
+                rule_uuid: Uuid::from_slice(rule_uuid)
+                    .expect("we should have 16 bytes exactly"),
+            })
+        )
+    )
+);
+
+#[cfg(test)]
+mod tests {
+    use super::*;
+
+    #[test]
+    fn parse_event() {
+        let mut event = testutils::generate_event();
+        std::dbg!(&event);
+        let now = chrono::offset::Utc::now();
+        let ts = Utc.timestamp(now.timestamp(), now.timestamp_subsec_micros() * 1000);
+        let uuid = Uuid::parse_str("a7963143-14da-48d6-bef0-422f305d1556").unwrap();
+
+        // customize some values in the generated event
+        event.time_sec = ts.timestamp();
+        event.time_usec = ts.timestamp_subsec_micros() as i64;
+        event.rule_uuid = uuid.as_bytes().clone();
+
+        let bytes = event.as_bytes();
+        let cfw_event = traffic_event(bytes);
+        std::dbg!(&cfw_event);
+
+        // parsed successfully
+        assert!(cfw_event.is_ok());
+        let cfw_event = cfw_event.unwrap();
+
+        // no leftover bytes after parsing
+        let leftover: Vec<u8> = vec![];
+        assert_eq!(cfw_event.0, leftover.as_slice(), "no bytes left over");
+        match cfw_event.1 {
+            CfwEvent::Traffic(e) => {
+                assert_eq!(e.event, CfwEvType::from(event.event));
+                assert_eq!(e.length, event.length);
+                assert_eq!(e.zonedid, event.zonedid);
+                assert_eq!(e.rule_id, event.rule_id);
+                assert_eq!(e.protocol, Protocol::from(event.protocol));
+                assert_eq!(e.direction, Direction::from(event.direction));
+                assert_eq!(e.source_port, u16::from_be(event.source_port));
+                assert_eq!(e.destination_port, u16::from_be(event.destination_port));
+                assert_eq!(e.source_ip, Ipv6Addr::from(u128::from_be(event.source_ip)));
+                assert_eq!(
+                    e.destination_ip,
+                    Ipv6Addr::from(u128::from_be(event.destination_ip))
+                );
+                assert_eq!(e.timestamp, ts);
+                assert_eq!(e.rule_uuid, uuid);
+            }
+        }
+    }
+}
diff --git a/cfwlogd/src/signal.rs b/cfwlogd/src/signal.rs
new file mode 100644
index 0000000..e674acb
--- /dev/null
+++ b/cfwlogd/src/signal.rs
@@ -0,0 +1,34 @@
+// This Source Code Form is subject to the terms of the Mozilla Public
+// License, v. 2.0. If a copy of the MPL was not distributed with this
+// file, You can obtain one at http://mozilla.org/MPL/2.0/.
+
+// Copyright 2019 Joyent, Inc.
+
+use crossbeam::channel::Sender;
+use libc::c_int;
+use std::thread;
+
+fn signal_handler(tx: Sender<c_int>) {
+    let signals = signal_hook::iterator::Signals::new(&[
+        libc::SIGHUP,
+        libc::SIGINT,
+        libc::SIGTERM,
+        libc::SIGUSR1,
+        libc::SIGUSR2,
+    ])
+    .expect("unable to create signal handler");
+
+    for signal in signals.forever() {
+        if tx.send(signal).is_err() {
+            trace!("receive half of signal handler channel is disconnected");
+            break;
+        }
+    }
+}
+
+pub fn start_signalhandler(tx: Sender<c_int>) -> thread::JoinHandle<()> {
+    thread::Builder::new()
+        .name("signal_handler".to_owned())
+        .spawn(move || signal_handler(tx))
+        .expect("failed to spawn signal watcher thread")
+}
diff --git a/cfwlogd/src/zones.rs b/cfwlogd/src/zones.rs
new file mode 100644
index 0000000..137e98f
--- /dev/null
+++ b/cfwlogd/src/zones.rs
@@ -0,0 +1,95 @@
+// This Source Code Form is subject to the terms of the Mozilla Public
+// License, v. 2.0. If a copy of the MPL was not distributed with this
+// file, You can obtain one at http://mozilla.org/MPL/2.0/.
+
+// Copyright 2019 Joyent, Inc.
+
+use std::collections::HashMap;
+use std::sync::{Arc, Condvar, Mutex};
+use std::thread;
+
+use crossbeam::sync::ShardedLock;
+use vminfod::{Changes, VminfodEvent, Zone};
+
+pub type Vmobjs = Arc<ShardedLock<HashMap<Zonedid, Zone>>>;
+pub type Zonedid = u32;
+
+/// Inserts or updates an existing vmobj into a given `Vmobjs`
+fn insert_vmobj(zone: Zone, vmobjs: Vmobjs) {
+    let mut w = vmobjs.write().unwrap();
+    w.insert(zone.zonedid, zone);
+}
+
+/// Search through a vminfod changes payload and see if the alias was apart of the update
+fn alias_changed(changes: &[Changes]) -> bool {
+    for change in changes {
+        if change
+            .path
+            .first()
+            // double map_or because path is a `Vec<Option<String>>`
+            .map_or(false, |v| v.as_ref().map_or(false, |a| a == "alias"))
+        {
+            return true;
+        }
+    }
+    false
+}
+
+/// Start a vminfod watcher thread that will keep a `Vmobjs` object up-to-date.
+/// This function will block until the spawned thread has processed the `Ready` event from vminfod
+pub fn start_vminfod(vmobjs: Vmobjs) -> thread::JoinHandle<()> {
+    #[allow(clippy::mutex_atomic)] // this lint doesn't realize we are using it with a CondVar
+    let waiter = Arc::new((Mutex::new(false), Condvar::new()));
+    let waiter2 = Arc::clone(&waiter);
+    let handle = thread::Builder::new()
+        .name("vminfod_event_processor".to_owned())
+        .spawn(move || {
+            info!("starting vminfod thread");
+            let &(ref lock, ref cvar) = &*waiter2;
+            let (r, _) = vminfod::start_vminfod_stream();
+            for event in r.iter() {
+                match event {
+                    VminfodEvent::Ready(event) => {
+                        let raw_vms = event.vms;
+                        let mut ready = lock.lock().unwrap();
+                        // Make sure we don't see another ready event in the future
+                        assert_eq!(*ready, false);
+                        let vms: Vec<Zone> = serde_json::from_str(&raw_vms)
+                            .expect("failed to parse vms payload from vminfod");
+                        let mut w = vmobjs.write().unwrap();
+                        for vm in vms {
+                            w.insert(vm.zonedid, vm);
+                        }
+                        *ready = true;
+                        debug!("vminfod ready event processed");
+                        cvar.notify_one();
+                    }
+                    VminfodEvent::Create(event) => insert_vmobj(event.vm, Arc::clone(&vmobjs)),
+                    VminfodEvent::Modify(event) => {
+                        if alias_changed(&event.changes) {
+                            debug!(
+                                "alias changed for {} ({}), updating vmobj mapping",
+                                &event.vm.uuid, &event.vm.zonedid
+                            );
+                            insert_vmobj(event.vm, Arc::clone(&vmobjs));
+                        }
+                    }
+                    // Nothing to be done with deletes currently. We don't modify `Vmobjs` since
+                    // cfw event logs in various processing queues may not have made it to disk
+                    // yet. We may eventually want to signal a logger that it's okay to shutdown.
+                    VminfodEvent::Delete(_) => (),
+                }
+            }
+            // TODO TRITON-1754: implement retry logic here, until then just panic
+            panic!("vminfod event stream closed");
+        })
+        .expect("vminfod client thread spawn failed.");
+
+    let &(ref lock, ref cvar) = &*waiter;
+    let mut ready = lock.lock().unwrap();
+    while !*ready {
+        ready = cvar.wait(ready).unwrap();
+    }
+
+    handle
+}
diff --git a/deps/eng b/deps/eng
new file mode 160000
index 0000000..ed10150
--- /dev/null
+++ b/deps/eng
@@ -0,0 +1 @@
+Subproject commit ed10150657b1f3ae4f061c751daf365ed16ca8d5
diff --git a/docs/index.md b/docs/index.md
new file mode 100644
index 0000000..a11ec38
--- /dev/null
+++ b/docs/index.md
@@ -0,0 +1,1531 @@
+---
+title: Joyent Engineering Guide
+markdown2extras: tables, code-friendly
+apisections:
+---
+<!--
+    This Source Code Form is subject to the terms of the Mozilla Public
+    License, v. 2.0. If a copy of the MPL was not distributed with this
+    file, You can obtain one at http://mozilla.org/MPL/2.0/.
+-->
+
+<!--
+    Copyright (c) 2019, Joyent, Inc.
+-->
+
+# Joyent Engineering Guide
+
+This document describes standards and best practices for software development at
+Joyent. These standards are intended to maintain product quality and to provide
+consistency across codebases to make it easier for all engineers to learn new
+parts of the system. This latter goal is important to encourage everyone to feel
+comfortable diving into all parts of the system, as is often necessary when
+debugging.
+
+It's important to remember that all situations are unique, so rules should not
+be followed blindly. However, these guidelines represent the best practices
+agreed upon by the team. If you feel it necessary to diverge from them, that's
+okay, but be prepared to explain why.
+
+Note: In this document (and elsewhere at Joyent), a service implementing an API
+is referred to by the API name itself. For example, "SAPI" denotes both the
+"Services API" in the abstract as well as the software component that implements
+that API.
+
+
+# Repository Guidelines
+
+These guidelines cover naming, structure, and processes around repositories.
+A template repository is included in this repo so you can quickly get something
+working that follows these guidelines.
+
+
+## Rule #1: FCS Quality All the Time
+
+In general, use the "master" branch for development. Development should not be
+ongoing in the release branches. "master" must be
+**FCS quality all the times**. The deliverables should always be of
+high enough quality to ship to a first customer, FCS (first customer ship).
+
+
+When working on large features, it's tempting to use development branches that
+eventually get integrated into master. Indeed, this is sometimes necessary.
+However, it should be avoided when possible, as it means people are running dev
+branches rather than "master", which can lead to a [quality death spiral
+(QDS)](http://wiki.illumos.org/display/illumos/On+the+Quality+Death+Spiral)
+as fewer people actually run the mainline tree. Where possible, consider
+whether larger projects can be split into reasonably-sized chunks that can
+individually be integrated into "master" without breaking existing
+functionality. This allows you to continue developing on "master" while still
+being able to commit frequently.
+
+
+## Repositories and documentation
+
+Open-source projects and components live at github.com/joyent. These include
+Node.js, SmartOS, Triton, Manta, and a large number of smaller Node modules and
+other components. Some components still live under individuals' github
+accounts, but new components should generally be created under the "joyent"
+organization.
+
+Note that just because a repo is on github doesn't mean its issues are tracked
+there. That's decided on a per-project basis.
+
+Some older components (and a few proprietary ones that are still used) are
+managed by gitosis running on the internal Joyent git server. Files, commits,
+and documentation for these projects can be browsed at mo.joyent.com by Joyent
+employees.
+
+
+## Repository Naming
+
+For repositories representing an API, the repo name that matches how the API is
+discussed (spoken, chatted and emailed) means you'll get the repo name right on
+first guess. If you can get away with it, a repo named after the abbreviate API
+name is best. For example:
+
+    Network API -> NAPI -> napi.git          # Good.
+                        -> network-api.git   # Less good.
+                        -> network_api.git   # Even less good.
+                        -> NAPI.git          # Whoa! Capital letters are crazy here.
+
+
+## Language
+
+New server-side projects should almost certainly use Node.js with C/C++
+components as needed. Consolidating onto one language makes it easier for
+everyone to dig into other teams' projects as needed (for development as well
+as debugging) and allows us to share code and tools.
+
+
+## Code Layout
+
+Here is a suggested directory/file structure for your repository. All
+repos **must** have a `README.md` and `Makefile`. The others are suggested
+namings for particular usages, should your repo require them.
+
+    build/          Built bits.
+    deps/           Git submodules and/or committed 3rd-party deps should go
+                    here. See "node_modules/" for node.js deps.
+    docs/           Project docs. Uses markdown and man.
+    lib/            JavaScript source files.
+    node_modules/   Node.js deps, either populated at build time or committed.
+    pkg/            Package lifecycle scripts
+    smf/manifests   SMF manifests
+    smf/methods     SMF method scripts
+    src/            C/C++ source files.
+    test/           Test suite (able to generate TAP output).
+    tools/          Miscellaneous dev/upgrade/deployment tools and data.
+    Makefile        See below.
+    package.json    npm module info, if applicable (holds the project version)
+    README.md       See below.
+
+
+"docs" or "doc"? "test" or "tst"? We're not being religious about the
+directory names, however the Makefile target names should use the names
+specified below to allow automated build tools to rely on those names. The
+reason to suggest "docs" and "test" as the directory names is to have the
+same name as the Makefile targets.
+
+
+### README.md
+
+Every repository **must** have in its root a README.md (Markdown) file that
+describes the repo and covers:
+
+* the name of the API or other component(s) contained in the repo and a brief
+  description of what they do
+* the boilerplate text for referencing the contribution and issue tracking
+  guidelines of the master project (Triton or Manta)
+* the JIRA project for this repo (and any additional instructions, like how JIRA
+  components are used for this project)
+* owners of the project
+* the style and lint configurations used, any additional pre-commit checks, and
+  any non-standard useful Makefile targets
+* some overview of the structure of the project, potentially including
+  descriptions of the subcomponents, directory structure, and basic design
+  principles
+* basic development workflow: how to run the code and start playing with it
+
+It's strongly recommended to start with the template in this repo.
+
+
+### Makefile
+
+All repos **must** have a Makefile that defines at least the following targets:
+
+* `all`: builds all intermediate objects (e.g., binaries, executables, docs,
+  etc.). This should be the default target.
+* `check`: checks all files for adherence to lint, style, and other
+  repo-specific rules not described here.
+* `clean`: removes all built files
+* `prepush`: runs all checks/tests required before pushing changes to the repo
+* `docs`: builds documentation (restdown markdown, man pages)
+* `test`: Runs the test suite. Specifically, this runs the subset of the
+  tests that are runnable in a dev environment. See the "Testing" section
+  below.
+* `release`: build releasable artifacts, e.g. a tarball (for projects that
+  generate release packages)
+
+The `check` and `test` targets **must** fail if they find any 'check'
+violations or failed tests. The `prepush` target is intended to cover all
+pre-commit checks. It **must** run successfully before any push to the repo.
+It **must** also be part of the automated build. Any commit which introduces a
+prepush failure **must** be fixed immediately or backed out. A typical prepush
+target will look like the following, but some non-code repositories might
+differ (e.g. not have a test suite):
+
+    prepush: check test
+            @echo "Okay to push."
+
+There are several modular Makefiles you can use to implement most of this. See
+"Writing Makefiles" (below) for details.
+
+
+### package.json and git submodules
+
+Repositories containing node.js code should have a `package.json` file at the
+root of the repository [1]. Normally most dependencies should be taken care of
+through `npm` and this `package.json` file, by adding entries to the
+`dependencies` or `devDependencies` arrays (see examples in this repository, or
+documentation on the `npm` website).
+
+Dependencies via `npm` can either take the form of an `npm` package name with a
+version specifier (in which case it must be published to the public `npm`
+package servers), or a `git` URL.
+
+For externally developed packages not published by Joyent, version specifiers
+should always be used (and the package published to `npm`):
+
+    "dependencies": {
+      "external-module": "^1.0.0"
+    }
+
+The use of version ranges (such as the `"^"` in the example above) is not
+required and you should use your judgment about the quality of release
+management and adherence to semantic versioning in the module you depend on.
+
+For packages developed by us, we have a weak preference towards publishing `npm`
+packages, stronger for shared library code that could be used outside Triton
+proper. Most usage of `git` URLs is historic and due to the original closed-
+source nature of the Triton stack.
+
+If you are using a `git` URL in an `npm` dependency, you must use a
+`git+https://` URL to specify it (not `git://` or `git+ssh://`). Plain `git://`
+operations are not authenticated in any way and can be hijacked by malicious
+WiFi or other network man-in-the-middle attacks (e.g. at airports and coffee
+shops - yes, this actually happens). The use of `git+ssh://` URLs is discouraged
+because it prevents users from being able to clone and build the package on a
+machine that does not have their GitHub private key on it. In particular, some
+Joyent CI bots (like the Gerrit `make check` bot) will complain if you use these
+URLs.
+
+    "dependencies": {
+      "joyent-module": "git+https://github.com/joyent/node-joyent-module.git#016977"
+    }
+
+For certain dependencies, it is standard practice across the Joyent repositories
+to use `git` submodules and not `npm`. This applies in particular to
+`javascriptlint`, `jsstyle`, `restdown`, `sdc-scripts` and some other modules
+that are not node.js-based. Similar to `npm` `git` dependencies, these must use
+`https://` URLs only. Your `.gitmodules` file should look like:
+
+    [submodule "deps/javascriptlint"]
+            path = deps/javascriptlint
+            url = https://github.com/davepacheco/javascriptlint.git
+
+Lastly, though you will find discussion about it in places, we don't currently
+use the npm "shrinkwrap" feature in any repositories. This is for a variety of
+reasons, the discussion about which is far too involved to relate here (but feel
+free to ask a senior Joyeur about the sordid history of SDC release management).
+
+*[1]* There are a handful of exceptions here in cases where multiple logical
+node.js modules are combined in one repository (e.g. `ca-native` and `amon`
+modules).
+
+
+## Coding Style
+
+Every repository **must** have a consistent coding style that is enforced by
+some tool. It's not necessary that all projects use the same style, though it's
+strongly suggested to keep differences to a minimum (e.g., only hard vs. soft
+tabs and tabstops). All styles **must** limit line length to 80
+columns<sup>[1](#footnote1)</sup>.  Existing style-checking tools
+include:
+
+* C: [cstyle](https://github.com/joyent/illumos-joyent/blob/master/usr/src/tools/scripts/cstyle.pl)
+* JavaScript: [jsstyle](https://github.com/davepacheco/jsstyle),
+  [gjslint](https://code.google.com/closure/utilities/docs/linter_howto.html),
+  [eslint](http://eslint.org/)
+* Bash: bashstyle (contained in eng.git:tools/bashstyle)
+* Makefiles: use bashstyle for now
+
+Both cstyle and jsstyle (which are 90% the same code) support overriding style
+checks on a per-line and block basis. `jsstyle` also now supports
+configuration options for indent style and few other things. E.g., you
+might like this in your Makefile:
+
+    JSSTYLE_FLAGS = -o indent=4,doxygen,unparenthesized-return=0
+
+Options can also be put in a "tools/jsstyle.conf" and passed in with '-f
+tools/jsstyle.conf'. See the [jsstyle
+README](https://github.com/davepacheco/jsstyle)) for details on
+JSSTYLED-comments and configuration options.
+
+Note that gjslint can be used as a style checker, but it is **not** a
+substitute for javascriptlint. And as with all style checkers, it **must** be
+integrated into `make check`.
+
+Bash scripts and Makefiles must also be checked for style. The only style
+guideline for now is the 80-column limit.
+
+Make target: "check"
+
+
+## Lint
+
+Every C repository **must** run "lint" and every JavaScript repository **must**
+run [javascriptlint](http://github.com/davepacheco/javascriptlint) and/or
+[eslint](http://eslint.org) and all **must** be lint-clean. Note that lint is
+not the same as style: lint covers objectively dangerous patterns like
+undeclared variables, while style covers subjective conventions like spacing.
+
+All of `lint`, `javascriptlint`, and `eslint` are very configurable. See
+[RFD 100](https://github.com/joyent/rfd/tree/master/rfd/0100) for eslint usage
+in Joyent repositories. Projects may choose to enable and disable particular
+sets of checks as they deem appropriate. Most checks can be disabled on a
+per-line basis. As with style, it's recommended that we minimize divergence
+between repositories.
+
+Make target: "check"
+
+
+## Copyright
+
+All source files (including Makefiles) should have the MPL 2.0 header and a
+copyright statement. These statements should match the entries in the
+`prototypes` directory. For an easy way to ensure that new files have the right
+MPL 2.0 header and copyright statement, you can copy the corresponding file out
+of the `prototypes` directory in `eng.git`.
+
+The contents of the MPL 2.0 header must be:
+
+    This Source Code Form is subject to the terms of the Mozilla Public
+    License, v. 2.0. If a copy of the MPL was not distributed with this
+    file, You can obtain one at http://mozilla.org/MPL/2.0/.
+
+When modifying existing code, the year should be updated to be the
+current year that the file was modified. There should only be a single
+year, not a list. For example:
+
+    Copyright (c) 2014, Joyent, Inc.
+
+
+## Testing
+
+tl;dr: `make test` for dev environment tests. 'test/runtests' driver script
+for in-Triton systems tests (see boilerplate 'tools/runtests.in').
+
+All repos **must** be tested by a comprehensive automated test suite and must
+be able to generate TAP output. (No particular node.js test framework is
+required, but all things being equal, use "nodeunit" or "node-tap".)
+These tests may be repo-specific, or may be part of a broader system test
+suite (ideally both). In either case, bug fixes and new features should not
+be integrated without adding new tests, and the tests **must** be run
+automatically (as via jenkins) either with every commit or daily. Currently
+this is handled by the [nightly environment](https://github.com/joyent/globe-theatre/)
+and the "stage-test-\*" Jenkins jobs. In other words, your project should
+have some sort of "stage-test-\*" job. Understanding and fixing failures in
+the automated test run **must** be considered the top development priority for
+that repo's team.  Persistent failures are not acceptable. Currently, these
+nightly and CI environments can only be accessed by Joyent employees.
+
+All installed components **should** provide a "runtests" driver script
+(preferably in the "test" subdirectory) and the necessary test files
+for running system tests (and unit tests are fine too) against your
+running service -- as opposed to starting up parallel dev versions of your
+service. The goal here is to provide a common, simple, and "just works"
+entry point for test components as they are deployed in the product,
+for the benefit of QA, continuous-integration testing, and devs not
+familiar with a given component. Dev environment != production environment.
+All "runtests" scripts **must** exit non-zero if any tests failed.
+
+
+Q&A:
+
+- Why not just "make test"?
+
+  Not all components install (or should install) their Makefile. For example
+  `make` isn't available in the headnode GZ. So "runtests" is a lowest common
+  denominator in this regard. Also dev env != production env -- separate
+  "make test" and "runtests" entry points can facilitate different test
+  setup and test case selection, if necessary.
+
+- What about a customer running tests and blowing away production data?!
+
+  Each runtests will be prefixed with a kill switch so that 'runtests' will
+  not be accidentally run on production systems. The kill switch is the
+  presence of the '/lib/sdc/.sdc-test-no-production-data' file. See
+  handling in 'tools/runtests.in' boilerplate.
+
+- What's a "system" test? "unit" test?
+
+    - unit tests: Tesing local code, no parts of the "system" are required
+      to run, and no mocking.
+    - integration tests: Local code, no "system", parts of the system are
+      mocked out as required.
+    - system tests: Testing the service(s) in a deployed environment.
+
+
+## cscope
+
+cscope is a terminal-based tool for browsing source. For performance, it's best
+to use it with an index. For repos using this repo's Makefile, you can build a
+basic index in a source tree using:
+
+    # make xref
+
+which translates to a make recipe something like this:
+
+    .PHONY: xref
+    xref: cscope.files
+        $(CSCOPE) -bqR
+
+    .PHONY: cscope.files
+    cscope.files:
+        find . -name '*.c' -o -name '*.h' -o -name '*.cc' -o -name '*.js' \
+            -o -name '*.s' -o -name '*.cpp' > $@
+
+
+You may also want the "-k" flag to cscope, which tells it to ignore standard
+header files.
+
+Once the index is built, you can browse the source with:
+
+    # cscope -dq
+
+cscope is available for SmartOS in pkgsrc. It's also buildable on MacOS. For
+instructions, see [the
+wiki](https://hub.joyent.com/wiki/display/dev/Snow+Leopard+tips%2C+fixes+and+bugs).
+
+Make target: "xref"
+
+
+## Documentation
+
+### API Documentation
+
+You **must** use [restdown](https://github.com/trentm/restdown). Please discuss
+with Trent if this isn't workable for your project.
+
+Restdown is a tool for creating docs (and especially REST API docs) using a
+single Markdown file with a few added conventions. You can set it up as
+follows. Get the restdown tool:
+
+    git submodule add https://github.com/trentm/restdown.git deps/restdown
+    cd deps/restdown/
+    git checkout 1.2.15    # let's use a restdown release tag
+
+Get a starter restdown file:
+
+    mkdir -p docs/media/img
+    cp ../eng/docs/boilerplateapi.md docs/index.md
+    cp ../eng/docs/media/img/favicon.ico docs/media/img/
+    cp ../eng/docs/media/img/logo.png docs/media/img/
+
+Tell the Makefile about it (`make docs`):
+
+    DOC_FILES = index.md
+
+
+
+### Code Documentation
+
+Consider adding a block comment at the top of every file that describes at a
+high level the component that's implemented in the file. For example:
+
+    /*
+     * ca-profile.js: profile support
+     *
+     * Profiles are sets of metrics. They can be used to limit visibility of
+     * metrics based on module, stat, or field names, or to suggest a group of
+     * metrics to a user for a particular use case.
+     */
+
+For non-trivial subsystems, consider adding a Big Theory statement that
+describes what the component does, the external interface, and internal details.
+For a great example, check out
+[panic.c](https://github.com/joyent/illumos-joyent/blob/master/usr/src/uts/common/os/panic.c#L29)
+in the kernel.
+
+Consider keeping design documents in restdown inside the repo. It's okay to have
+one-off documents for specific projects, even if they become out of date as the
+code evolves, but make clear in the document that the content may be out of
+date. Keep such docs separate from general design documents that are kept up to
+date.
+
+
+## Node Build
+
+If your deployed service or tool uses node, then it **must** provide its own
+node build. The exception is services whose upgrade is tied to the Triton
+platform, and hence can be tested against a known node build (the platform's
+node build). There are two ways you can get a node build for your repo:
+
+1. Build your own from sources. Read and use "tools/mk/Makefile.node.defs" and
+   "tools/mk/Makefile.node.targ". You'll also need a git submodule of the node
+   sources:
+
+        $ git submodule add https://github.com/joyent/node.git deps/node
+        $ cd deps/node
+        $ git checkout v0.6.18   # select whichever version you want
+
+2. Use a prebuilt node. Read and use "tools/mk/Makefile.node_prebuilt.defs"
+   and "tools/mk/Makefile.node_prebuilt.targ".
+
+
+## Node add-ons (binary modules)
+
+Because C++ does not define a useful compiler- or platform-dependent
+[binary](http://stackoverflow.com/questions/7492180/c-abi-issues-list)
+[interface](http://developers.sun.com/solaris/articles/CC_abi/CC_abi_content.html),
+and we have seen breakage resulting from changing compiler versions, any repo
+that uses add-ons (binary modules) **must** bundle its own copy of "node" and
+use that copy at runtime. And almost every repo will fall into this bucket,
+since we use the native node-dtrace-provider heavily for observability.
+
+The recommended way to do this is to add the official node repo as a git
+submodule and build it during the build process. There are existing modular
+Makefiles in this repo (eng.git) to do all the work. All you need to do is
+include them and then add the appropriate dependencies on `$(NODE)`.
+
+
+## Commit Comments and JIRA Tickets
+
+In collaborating on a body of software as large as Triton, it's critical that
+the issues and thought processes behind non-trivial code changes be documented,
+whether that's through code comments, git commit comments, or JIRA tickets.
+There are many cases where people other than the original author need to
+examine the git log:
+
+* An engineer in another area tries to understand a bug they've run into (in
+  your repo or not), possibly as a result of a recent change. The easier it is
+  for people to move between repos and understand recent changes, the more
+  quickly bugs in master can be root-caused. This is particularly important to
+  avoid an issue bouncing around between teams where the problem is *not*.
+* An engineer in another area tries to understand when a feature or bugfix
+  was integrated into your repo so that they can pull it down to use it.
+* An engineer working on the same code base, possibly years later, needs to
+  modify (or even rewrite) the same code to fix another bug. They need to
+  understand why a particular change was made the way it was to avoid
+  reintroducing the original bug (or introducing a new bug).
+* A release engineer tries to better understand the risk and test impact of a
+  change to decide whether it's appropriate to backport.
+* A support engineer tries to better understand the risk and test impact of a
+  change to decide whether it's appropriate for binary relief or hot patching.
+* Product management wants to determine when a feature or bugfix was integrated.
+* Automated tools want to connect commits to JIRA tickets.
+
+To this end, we require that with every commit there **must** be a comment that
+includes the list of JIRA tickets addressed with this commit and a synopsis of
+the changes (*either* for the whole commit *or* for each change, one by one).
+**Between the JIRA ticket and the commit comment itself, there must be
+sufficient information for an engineer that's moderately familiar with the code
+base, possibly years later but with source in hand, to understand how and why
+the change was made.**
+
+The worst case is when the thought process and issue list are nowhere: not in
+the comments and not in the JIRA tickets.
+
+### Commit Comments
+
+Across Joyent we require that **each commit be associated with one or more JIRA
+tickets and that those tickets be listed in the commit comments**. This way,
+given either the commit or the JIRA ticket, one can find the other.
+
+Historically, some repos (notably illumos-joyent and cloud-analytics) have
+additionally required that tickets must not be reused for multiple commits in
+the same repo except for very minor changes like fixing lint or style warnings.
+This makes it easier to correlate tickets and commits, since there's usually
+exactly one commit for each resolved ticket. It also makes it easier to
+back out the changes for a particular project. For these repos, the git
+comments for the commit consist of a single line per JIRA ticket being resolved
+in the commit. Each line consists of the ticket identifier and the synopsis
+exactly as it appears in JIRA (optionally truncated to 80 characters with
+"..."):
+
+    OS-147 vfsstat command to show VFS activity by zone
+    OS-148 Update ziostat to coexist peacefully with vfsstat
+    OS-149 New kstats to support vfsstat
+
+This approach encourages short, descriptive ticket synopses. For repos that keep
+track of code reviews (e.g., illumos-joyent), that information is appended like
+this:
+
+    OS-850 Add support for Intel copper quad I350 to igb.
+    Reviewed by: Jerry Jelinek <jerry.jelinek@joyent.com>
+
+In the rare cases where the same ticket is used for multiple commits, a
+parenthetical is used to explain why:
+
+    INTRO-581 move mdb_v8 into illumos-joyent (missing file)
+
+This structure works well for established repos like illumos, but it's not
+always appropriate. For new work on greenfield projects, it may not even make
+sense to use more than one ticket until the project reaches a first milestone.
+
+### JIRA Tickets
+
+For bugs, especially those that a customer could hit, consider including
+additional information in the JIRA ticket:
+
+* An explanation of what happened and the root cause, referencing the source
+  where appropriate. This can be useful to engineers debugging similar issues
+  or working on the same area of code who want to understand exactly why a
+  change was made.
+* An explanation of how to tell if you've hit this issue. This can be pretty
+  technical (log entries, tools to run, etc.). This can be useful for engineers
+  to tell if they've hit this bug in development as well as whether a customer
+  has hit the bug.
+* A workaround, if any.
+
+Of course, much of this information won't make sense for many bugs, so use your
+judgment, but don't assume that you're the only person who will ever look at the
+ticket.
+
+# Logging
+
+There are at least three different consumers for a service's logs:
+
+- engineers debugging issues related to the service (which may not actually be
+  problems with the service)
+- monitoring tools that alert operators based on error events or levels of
+  service activity
+- non real-time analysis tools examining API activity to understand performance
+  and workload characteristics and how people use the service
+
+For the debugging use case, **the goal should be to have enough information
+available after a crash or an individual error to debug the problem from the
+very first occurrence in the field**. It should also be possible for engineers
+to manually dump the same information as needed to debug non-fatal failures.
+
+Triton service logs **must** be formatted in JSON. Node.js services **must**
+use [Bunyan](https://github.com/trentm/node-bunyan). Exceptions: (a) you are
+using syslog (see use case for syslog below); (b) your service is legacy; or,
+(c) you just haven't migrated to Bunyan yet (which is fine, JSON log output
+is not a top-priority make work project). If you have an example of a log for
+which JSON format gets in the way, please bring it up for discussion).
+
+Multiple use cases do not require multiple log files. Most services should log
+all activity (debugging, errors, and API activity) in JSON to either the SMF
+log or into a separate log file in
+"/var/smartdc/&lt;service&gt;/log/&lt;component&gt;.log". For services with
+extraordinarily high volume for which it makes sense to separate out API
+activity into a separate file, that should be directed to
+"/var/smartdc/&lt;service&gt;/log/requests.log". However, don't use separate
+log files unless you're sure you need it. All log files in
+"/var/smartdc/&lt;service&gt;/log" should be configured for appropriate log
+rotation.
+
+For any log entries generated while handling a particular request, the log
+entry **must** include the request id. See "Request Identifiers" under "REST
+API Guidelines" below.
+
+Log record fields **must** conform to the following (most of which comes
+for free with Bunyan usage):
+
+| JSON key | Description | Examples | Required |
+| -------- | ----------- | -------- | -------- |
+| **name** | Service name. | "ca" (for Cloud Analytics) | All entries |
+| **hostname** | Server hostname. | `uname -n`, `os.hostname()` | All entries |
+| **pid** | Process id. | 1234 | All entries |
+| **time** | `YYYY-MM-DDThh:mm:ss.sssZ` | "2012-01-26T19:20:30.450Z" | All entries |
+| **level** | Log level. | "fatal", "error", "warn", "info", or "debug" | All entries |
+| **msg** | The log message | "illegal argument: parameter 'foo' must be an integer" | All entries |
+| **component** | Service component. A sub-name on the Logger "name". | "aggregator-12" | Optional |
+| **req_id** | Request UUID | See "Request Identifiers" section below. Restify simplifies this. | All entries relating to a particular request |
+| **latency** | Time of request in milliseconds | 155 | Strongly suggested for entries describing the completion of a request or other backend operation |
+| **req** | HTTP request | -- | At least once as per Restify's or [Bunyan's serializer](https://github.com/trentm/node-bunyan/blob/master/lib/bunyan.js#L856-870) for each request. |
+| **res** | HTTP response | -- | At least once as per Restify's or [Bunyan's serializer](https://github.com/trentm/node-bunyan/blob/master/lib/bunyan.js#L872-878) for each response. |
+
+We use these definitions for log levels:
+
+- "fatal" (60): The service/app is going to stop or become unusable now.
+  An operator should definitely look into this soon.
+- "error" (50): Fatal for a particular request, but the service/app continues
+  servicing other requests. An operator should look at this soon(ish).
+- "warn" (40): A note on something that should probably be looked at by an
+  operator eventually.
+- "info" (30): Detail on regular operation.
+- "debug" (20): Anything else, i.e. too verbose to be included in "info" level.
+- "trace" (10): Logging from external libraries used by your app or *very*
+  detailed application logging.
+
+Suggestions: Use "debug" sparingly. Information that will be useful to debug
+errors *post mortem* should usually be included in "info" messages if it's
+generally relevant or else with the corresponding "error" event. Don't rely
+on spewing mostly irrelevant debug messages all the time and sifting through
+them when an error occurs.
+
+Most of the time, different services should log to different files. But in some
+cases it's desirable for multiple consumers to log to the same file, as for
+vmadm and vmadmd. For such cases, syslog is an appropriate choice for logging
+since it handles synchronization automatically. Care must be taken to support
+entries longer than 1024 characters.
+
+
+# SMF Integration
+
+All services **must** be delivered as SMF services. This means:
+
+- They deliver an SMF service manifest.
+- The install mechanism imports the manifest.
+- The uninstall mechanism deletes the service.
+- The service is started, stopped, restarted, etc. via SMF.
+
+While SMF itself is grimy and the documentation is far from perfect, the
+documentation *is* extensive and useful. Many common misunderstandings about
+how SMF works are addressed in the documentation. It's strongly recommended
+that you take a pass through the docs before starting the SMF integration for
+your service. In order of importance, check out:
+
+- SMF concepts: smf(5), smf_restarter(5), smf_method(5), svc.startd(1M)
+- Tools: svcs(1), svcadm(1M), svccfg(1M)
+
+Common mistakes include:
+
+- Setting the start method to run the program you care about (e.g., "node
+  foo.js") rather than backgrounding it (e.g., "node foo.js &"). SMF expects
+  the start method to start the service, not *be* the service. It times out
+  start methods that don't complete, so if you do this you'll find that your
+  service is killed after some default timeout interval. After this happens
+  three times, SMF moves the service into maintenance.
+- Using "child" or "wait model" services to avoid the above problem. Read the
+  documentation carefully; this probably doesn't do what you want. In
+  particular, if your "wait model" service fails repeatedly, SMF will never put
+  it into maintenance. It will just loop forever, forking and exiting.
+- Not using "-s" with svcadm enable/disable. Without "-s", these commands are
+  asynchronous, which means the service may not be running when "svcadm enable"
+  returns. If you really care about this, you should check the service itself
+  for liveness, not rely on SMF, since the start method may have completed
+  before the service has opened its TCP socket (for example).
+
+## Managing processes under SMF
+
+SMF manages processes using an OS mechanism called contracts. See contract(4)
+for details. The upshot is that it can reliably tell when a process is no
+longer running, and it can also track child processes.
+
+Quoting svc.startd(1M):
+
+     A contract model service fails if any of the following conditions
+     occur:
+
+         o    all processes in the service exit
+
+         o    any processes in the service produce a core dump
+
+         o    a process outside the service sends a service process a
+              fatal signal (for example, an administrator terminates a
+              service process with the pkill command)
+
+Notice that if your service forks a process and *that* process exits,
+successfully or otherwise, SMF will not consider that a service failure. One
+common mistake here is forking a process that will be part of your service, but
+not considering what happens when that process fails (exits). SMF will not
+restart your service, so you'll have to manage that somehow.
+
+## Service logs
+
+SMF maintains a log for each service in /var/svc/log. The system logs restarter
+events here and launches the start method with stderr redirected to the log,
+which often means the service itself will have stderr going to this log as
+well. It's recommended that services either use this log for free-form debug
+output or use the standard logging facility described under "Logging" above.
+
+# REST API Guidelines
+
+It's strongly recommended to use
+[restify](https://github.com/mcavage/node-restify) for all web services. Not
+only will you leverage common code and test coverage, but restify gives you
+features like DTrace observability, debuggability, throttling, and versioning
+out of the box. If it doesn't support something you need, consider adding it
+rather than rolling your own.
+
+## Request Identifiers
+
+A request identifier uniquely identifies an operation across multiple services
+(e.g., portal, cloudapi, ca, ufds). It's essential for debugging issues after
+they've happened. The goal is for issues to be debuggable from the information
+available after their first occurrence, without having to reproduce it to
+gather more information. To facilitate this:
+
+- When an external service receives a request from the outside, it **must**
+  generate a unique request identifier and include it in the "x-request-id"
+  header of any requests made as part of handling the initial request.
+- When any service receives a request with an "x-request-id" header, it
+  **must** include it in the "x-request-id" header of any request made as part
+  of handling that request.
+- When each service logs activity (API requests), alerts, or debug messages
+  related to a particular request, it **must** include the request id as
+  the "req_id" field (as described in the [Bunyan
+  docs](https://github.com/trentm/node-bunyan)).
+
+
+## Naming Endpoints
+
+Service API endpoints **should** be named. Endpoint names **must** be
+CamelCase, **should** include the name of resource being operated on,
+and **should** follow the lead of
+[CloudAPI](https://apidocs.joyent.com/cloudapi/) for verb usage, e.g.:
+
+    # CRUD examples:
+    ListMachines
+    GetMachine
+    CreateMachine
+    DeleteMachine
+
+    # Other actions, if applicable:
+    StopMachine
+    StartMachine
+    RebootMachine
+    ResizeMachine
+
+    # Example using "Put" verb from
+    # <https://apidocs.joyent.com/manta/api.html#PutObject> when the action
+    # is idempotent.
+    PutObject
+
+
+## Error Handling
+
+APIs must provide meaningful and actionable error responses, especially for
+requests that involve submitting data (i.e. non-GET requests). "Actionable"
+here means that enough information is provided for programmatic handling of
+errors.
+
+### Motivation
+
+[Node-restify error support](https://github.com/restify/errors)
+provides a set of `RestError` classes with a typical response like:
+
+    HTTP/1.1 409 Conflict
+    ...
+
+    {
+      "code": "InvalidArgument",
+      "message": "I just don't like you"
+    }
+
+However, API clients often need more information about the failure of a
+request. This scheme does not provide a way to programmatically match a
+failure to one of multiple input parameters. E.g., consider a client
+attempting to present errors in a form for a "CreateFoo" endpoint.
+
+E.g., API clients that implement user interfaces need to give users
+feedback about the errors produced after submitting a form, where a message
+such as "RAM must be greater than 1024" is more useful than "Arguments are
+invalid" messages produced by generic *node restify* server implementations.
+
+### Error Response Format
+
+    HTTP/1.1 <statusCode> ...
+    ...
+
+    {
+      "code": "<restCode>",
+      "message": "<message>",
+      "errors": [
+        {
+          "field": "<errorField>",
+          "code": "<errorCode>",
+          "message": "<errorMessage>"
+        },
+        ...
+      ]
+    }
+
+JEG-based API error response guidelines:
+
+- **must** use a meaningful HTTP `statusCode`. See
+  <http://en.wikipedia.org/wiki/List_of_HTTP_status_codes>.
+- **must** include a `code` CamelCase string code field
+- **must** include a `message` string description of the error. The
+  message must be a human readable string that allows users to understand
+  the nature of the error *code*, as the same error *code* can be produced
+  with two different *messages*. An example of this might be an
+  "InternalError" *code* that could return "Database is offline" or "Cache
+  not running" as its *message*.
+- **may** include an `errors` array. Each element of that array **must**
+  include a `field` name, **must** include a `code` CamelCase string code
+  field and **may** include a `message` string field.
+
+Suggested `errors.*.code` fields are:
+
+| Code | Description |
+| ---- | ----------- |
+| Missing | The resource does not exist. |
+| MissingParameter | A required parameter was not provided. |
+| Invalid | The formatting of the field is invalid. |
+
+
+### Example:
+
+    HTTP/1.1 422 Unprocessable Entity
+    ...
+
+    {
+      "code": "InvalidParameters",
+      "message": "Invalid paramaters to create a VM",
+      "errors": [
+        {
+          "field": "ram",
+          "code": "Invalid",
+          "message": "RAM is not a number"
+        },
+        {
+          "field": "brand",
+          "code": "MissingParameter"
+        },
+        {
+          "field": "image_uuid",
+          "code": "Missing",
+          "message": "Image '6b288017-2c2d-354b-83d4-69748d50284d' does not exist"
+        }
+      ]
+    }
+
+
+### Best Practice
+
+TODO: Trent is working on code to use with restify v2.0 to facilitate
+subclassing restify.RestError to make the above simpler.
+
+
+### Documenting Errors
+
+An API **must** document all of the `restCodes` it can produce. An "Errors"
+section near the top of the API's restdown docs is suggested. For example:
+
+    ||**HTTP Status Code**||**JSON Code**||**Description**||
+    ||400||OperationNotAllowedOnRootDirectory||Trying to call PUT on `/`||
+    ||404||ResourceNotFound||If `:account` does not exist||
+    ||409||EntityExists||If the specifed path already exists and is not a directory||
+    ||409||ParentNotDirectory||Trying to create a directory under an object||
+
+Additionally, each endpoint **must** document all custom `errors.*.code`
+values it can produce. If just a stock set of error codes is used, then
+it is sufficient to document those in the "Errors" section at the top
+of the API docs.
+
+
+
+# Bash programming guidelines
+
+## xtrace
+
+Bash has a very useful feature called "xtrace" that causes it to emit
+information about each expression that it evaluates. You can enable it for a
+script with:
+
+    set -o xtrace
+
+With newer versions, you can redirect this output somewhere other than stderr
+by setting
+[BASH_XTRACEFD](https://www.gnu.org/software/bash/manual/bashref.html#Bash-Variables).
+
+This is incredibly useful for several situations:
+
+- debugging non-interactive system scripts (e.g., SMF start methods) *post
+  mortem*. Such scripts should leave xtrace on all the time, since they're not
+  run frequently enough for the extra logging to become a problem and the
+  xtrace output makes it significantly easier to understand what happened when
+  these scripts go wrong.
+- debugging interactive scripts in development. You can run bash with "-x" to
+  enable xtrace for a single run. You usually don't want to leave xtrace on for
+  interactive scripts, unless you redirect the xtrace output:
+- debugging interactive scripts *post mortem* by enabling the xtrace output and
+  redirecting it to a temporary file. Be sure to remove the file when the
+  script exits successfully.
+
+## Error handling
+
+It's absolutely possible to write robust shell scripts, but the default shell
+behavior to ignore errors means you have to consider how to handle errors in
+order to avoid creating brittle scripts that are difficult to debug.
+
+The biggest hammer is the "errexit" option, which you can enable with:
+
+    set -o errexit
+
+This will cause the program to exit when simple commands, pipelines, and
+subshells return non-zero. Commands invoked in a conditional test, a loop test,
+or as part of an `&&` or `||` list do not get this special treatment. While this
+approach is nice because the default is that errors are fatal (so it's harder to
+forget to handle them), it's not a silver bullet and doesn't let you forget
+about error handling completely. For example, many commands *can* reasonably
+fail with no ill effects and so must be explicitly modified with the unfortunate
+` || true` to keep errexit happy.
+
+A more fine-grained approach is to explicitly check for failure of invocations
+that may reasonably fail. A concise pattern is to define a `fail` function
+which emits its arguments to stderr and exits with failure:
+
+    function fail()
+    {
+        echo "$*" >&2
+        exit 1
+    }
+
+and then use it like this:
+
+    echo "about to do something that might fail"
+    zfs create zones/myfilesystem || fail "failed to create zones"
+
+You can also use this with variable assignments
+
+    echo "about to list contents of a directory that may not exist"
+    foo=$(ls -1 $tmpdir) || fail "failed to list contents of '$tmpdir'"
+
+It's also important to remember how error handling works with pipelines. From
+the Bash manual:
+
+    The exit status of a pipeline is the exit status of the last command in the
+    pipeline, unless the pipefail option is enabled (see The Set Builtin). If
+    pipefail is enabled, the pipeline's return status is the value of the last
+    (rightmost) command to exit with a non-zero status, or zero if all commands
+    exit successfully.
+
+This means that if you run this to look for compressed datasets:
+
+    # zfs list -oname,compression | grep on
+
+If the "zfs" command bails out partway through, that pipeline will still
+succeed (unless pipefail is set) because "grep" will succeed. To set pipefail,
+use:
+
+    set -o pipefail
+
+## Running subcommands
+
+Prefer `$(subcommand)` to `` `subcommand` ``, since it can be nested:
+
+    type_of_grep=$(file $(which grep))
+
+## Automatic Checks
+
+See "Coding Style" above for style checks. Currently, the only enforced check
+is an 80-column limit on line length.
+
+It's also worth using "bash -n" to check the syntax of bash scripts as part of
+your Makefile's "check" target. The Makefiles in eng.git automatically check
+both syntax and style.
+
+## Temporary Files
+
+Put temporary files in /var/tmp/`$(dirname $0)`.`$$`. This will generally be
+unique but also allows people to figure out what script left this output
+around.
+
+On successful invocations, remove any such temporary directories or files,
+though consider supporting a `-k` flag (or similar) to keep the temporary files
+from a successful run.
+
+## Parsing command line options
+
+By convention, illumos scripts tend to use `opt_X` variables to store the value
+of the `-X` option (e.g., `opt_d` for `-d`). Options are best parsed with
+getopts(1) (not to be confused with getopt(1)) using a block like this:
+
+    function usage
+    {
+        [[ $# -gt 0 ]] && echo "$(dirname $0): $*" >&2
+
+        cat <<-USAGE >&2
+        Usage: $(dirname $0) [-fn] [-d argument] args ...
+
+        Frobs args (optionally with argument <argument>).
+
+        -f    force frobnification in the face of errors
+        -n    dry-run (don't actually do anything)
+        -d    specify temporary directory
+        USAGE
+
+        exit 2
+    }
+
+    opt_f=false
+    opt_n=false
+    opt_d=
+
+    while getopts ":fnd:" c; do
+            case "$c" in
+            f|n)    eval opt_$c=true                                ;;
+            d)      eval opt_$c=$OPTARG                             ;;
+            :)      usage "option requires an argument -- $OPTARG"  ;;
+            *)      usage "illegal option -- $OPTARG"               ;;
+            esac
+    done
+
+    # Set $1, $2, ... to the rest of the arguments.
+    shift $((OPTIND - 1))
+
+Below are common command line options. If you're implementing the functionality
+below, try to stick to the same option letters to maintain consistency. Of
+course, many of these options won't apply to most tools.
+
+    -?          Display usage message.
+    -d dir      Use directory "dir" for temporary files
+    -i          Interactive mode (force confirmation)
+    -f          Barrel on in the face of errors
+    -k          Keep temporary files (for debugging)
+    -n          Dry-run: print out what would be done, but don't do it
+    -o file     Specify output file
+    -p pid      Specify process identifiers
+    -r          Recursive mode
+    -y          Non-interactive mode (override confirmations with "yes")
+    -z          Generate (or extract) a compressed artifact
+
+## Command-line scripts that perform multiple complex tasks
+
+With xpg_echo, you can use "\c" with "echo" to avoid printing a newline.
+Combined with the above error handling pattern, you can write clean scripts
+that perform a bunch of tasks in series:
+
+    shopt -s xpg_echo
+
+    echo "Setting nodename to 'devel' ... \c"
+    hostname devel || fail "failed to set hostname"
+    echo "done."
+
+    echo "Testing DNS ... \c"
+    ping example.com || fail "failed"
+
+    echo "Restarting ssh ... \c"
+    svcadm disable -s ssh || fail "failed to disable service"
+    svcadm enable -s ssh || fail "failed to enable service"
+    echo "done."
+
+The output is clean both when it succeeds:
+
+    # ./setup.sh
+    Setting nodename to 'devel' ... done.
+    Testing DNS ... example.com is alive
+    Restarting ssh ... done.
+
+and when it fails:
+
+    # ./setup.sh
+    Setting nodename to 'devel' ... done.
+    Testing DNS ... ping: unknown host example.com
+    failed
+
+This is primarily useful for complex scripts that people run interactively
+rather than system scripts whose output goes to a log.
+
+# Java
+
+If you find yourself having to do anything related to Java or the Java Manta SDK
+[QUICKSTART.md in java-manta](https://github.com/joyent/java-manta/blob/master/QUICKSTART.md)
+has a condensed guide for getting started and covers many aspects of our usage
+of Java and Maven that might not be familiar to an engineer that hasn't worked
+with these tools yet.
+
+# JIRA best practices for Customer Issues
+
+We have all noticed that there are a lot more JIRA tickets originating from
+customers than there were 6 months ago.
+
+We need to refine the process of opening, updating, and resolving bugs so that
+we maximize productivity, and reduce the time that bugs sit idle with not
+enough information to act on.
+
+
+
+## JIRA Updates
+
+Providing regular updates to JIRA tickets is the best way to keep stakeholders
+informed on the progress of the issue. A little more effort spent updating
+JIRA issues will create a valuable knowledge base full of information for the
+Ops and Support teams to use. The result will be:
+
+1. Fewer issues coming through to the Dev team, as issues that have been worked
+   through before will be able to be triaged before they are passed to
+   engineering. We are are shooting for a 10:1 ratio of issues opened to
+   issues passed to engineering. Currently were well over 10:5.
+
+2. Issues that are passed to engineering have had more diagnostic information
+   in them, as the Ops and Support teams learn from example on how to
+   troubleshoot issues. This will reduce the time it takes for you to resolve
+   issues, leaving you more time to work on more interesting projects.
+
+3. Better customer relations, as the customer teams will be able to explain to
+   the customers what steps are being taken to find them a solution to their
+   problem.
+
+
+### Assigning Tickets
+
+![Assign To Me](media/img/assign_to_me.jpg)
+
+Its important to make sure that active tickets (tickets that someone is
+working on) have an Assignee. The way that the triage teams decide if something
+needs to be escalated to engineering triage is using this field. If there is no
+one assigned to an issue, Ben and Deniz will continue to ask for a triage in
+scrum. So if youre working on an issue, even if you dont intend to be the
+final owner, click the Assign to me button in the top nav. This will indicate
+that weve had eyes on this issue and its not sitting idle. You can always
+reassign the issue if something changes.
+
+
+### Needs More Info
+
+![Needs More Info](media/img/needs_more_info.jpg)
+
+Needs More Info is a custom status that we added last year to indicate that
+there is not enough information to proceed on an issue. This could be used both
+for development, when there is not enough info to troubleshoot, or by the
+customer teams, if there is not enough info to resolve the issue with the
+customer. Whenever you put a ticket into this state, make sure to assign the
+ticket to whoever needs to provide the info. In your case, this will often be
+the reporter. It is important to force a ticket into this status if you are
+blocked on proceeding with diagnostics due to lack of information.
+
+
+### Comments
+
+![Comments](media/img/comments.jpg)
+
+Commenting is important to keep our cross-functional teams aware of progress
+made (or not made) on the resolution of a customer issue. Commenting best
+practices:
+
+- Comment often, even if you dont have all of the details worked out yet. The
+  more information provided the more informed we can make the customers.
+- Comment instructionally, as if you were explaining to someone else how you
+  diagnosed the issue. This will help the Ops and Support teams to be able to
+  recognize, categorize and help diagnose issues on their own in the future,
+  saving everyone time and energy.
+- Comment consistently, at the end of the work day, for instance. Even if the
+  comment is that you made little progress, it keeps the customer facing teams
+  informed of whats going on, so that they can make an informed decision on
+  what to communicate back to the customer.
+
+
+### Moving Tickets
+
+Often a ticket will come in through the JPC project, but logically belongs in
+another, as the problem needs to be resolved in Triton.
+
+For issues like this, please use the Move feature to move the ticket into the
+appropriate project. The JPC ticket will then automatically redirect users to
+the new location.
+
+![Move Ticket](media/img/move_ticket.jpg)
+
+
+### Linking Tickets
+
+When an issue is either related to, or a duplicate of , or depends on a ticket
+that is already in the system, its valuable to link the 2 tickets in JIRA. You
+can do this using the Link feature:
+
+![Linking Tickets](media/img/linking_tickets.jpg)
+
+The linked issue(s) will then show up in the ticket as a separate section:
+
+![Issue Links](media/img/issue_links.jpg)
+
+
+### Field: `Target Fix Version`
+
+If you are working on an issue, it is important to know what release you are
+targeting for the fix. Typically, you will either be targeting a Simpsons
+version (meaning that the release to the customer would be the next major), or
+a dot release. Your project lead can help clarify which release your fix should
+go into if you are unsure.
+
+Note: The JPC project only contains dot release versions. If something is more
+suited for a Simpsons release version, you should move the ticket to another
+project.
+
+The customer teams will use this information to decide whether they can wait
+for the fix in the next release, or if theyll need to find a short term work
+around.
+
+![Target Fix Version](media/img/fix_version.jpg)
+
+
+## Resolving JIRA Tickets
+
+Issue resolutions should provide valuable information to the customer teams,
+allowing them to communicate solutions back to our customers, as well as make
+decisions on patches and workarounds.
+
+Please spend some extra time when resolving bugs that originated from a
+customer.
+
+
+### Field: `Issue Resolution (Public)`
+
+The Issue Resolution field is the field that the customer teams are going to
+use to communicate information back to their customers about the bug fix.
+
+This field must be filled out upon resolution if the ticket has originated from
+a customer (either coming from the JPC project or having a customer label
+attached to it).
+
+When you are filling in this field, try to word it as though you are talking to
+the customer. The customer doesnt need to know every detail of how the issue
+was resolved, but they do want to know how it will impact them.
+
+![Resolution](media/img/resolution.jpg)
+
+### Field: `Resolution`
+
+Resolving a ticket once youve committed code indicates to the customer teams
+that they can deliver a solution (or estimated release date) to the customer.
+It is important to resolve your issues so that we continue to rotate bugs out
+of the queue, and keep our customers happy.
+
+| Resolution | Description |
+| ---------- | ----------- |
+| Fixed/Implemented | Indicates that a development solution has been implemented and can be communicated to the customer. Release date can be deduced from the fix version. |
+| Duplicate | Indicates that this is already being worked on (or has been fixed) by another bug. Please Link the duplicate issue to the ticket. |
+| Won't Fix | There are certain circumstances under which we will decline to resolve a customer issue. A couple examples of this are if the issue being raised occurs by design, if we are refactoring a part of the code base that will eliminate the bug once released, or if there is a sufficient workaround. Bryan and Laurel should be consulted if you think a bug is a "won't fix", and always include an explanation for the customer teams if you use this resolution on a ticket that originated from a customer. |
+
+
+### Field: `Fix Version/s`
+
+Fix version is the version in which you actually committed the code that fixed
+the bug. It differs from Target Fix Version in that it should only be added to
+the ticket after resolution. The fix version is what the customer teams will
+use to decide whether or not a patch is required on the current operating
+version.
+
+![Fix Version](media/img/fix_version.jpg)
+
+
+
+# Writing Makefiles
+
+This repo (eng.git) provides a number of modular Makefiles which you can use
+(perhaps even by direct reference using submodules) to provide the required
+targets described above, as well as several other useful pieces (like building
+Node in your repo). These are designed to be dropped in without modification:
+they consume Make variables as input and either export other variables or define
+rules. You should use these existing Makefiles wherever possible.
+
+Importantly:
+
+* If you find yourself adding anything other than a repo-specific variable
+  definition or a repo-specific rule to your Makefile, consider creating a
+  new Makefile with a crisp interface and adding it to the existing ones in
+  eng.git. We want to avoid Makefile code duplication just as we would
+  JavaScript code duplication.
+* Do **not** modify a copy of any of the existing sub Makefiles from eng.git.
+  Feel free to generalize or improve the original, but don't fork it.
+* We do not use recursive Make. Avoid it within a project if at all possible.
+
+Top level Makefiles should generally have the following structure:
+
+1. Repo-specific definitions. These serve as inputs for included Makefiles. For
+   example, you might define the list of JavaScript files that should be
+   style-checked here.
+2. Includes for Makefiles that define variables based on the repo-specific
+   variables (e.g., repo specifies input files, and the included Makefile
+   defines a list of output files, or modifies the list of files that will be
+   removed with "make clean").
+3. Repo-specific rules. These must be the first rules that appear in the
+   Makefile so that the repo can control the default target and the order of
+   dependencies for common targets.
+4. Includes for Makefiles that define other rules.
+
+The goal is that most top-level Makefiles only specify their parameters, the
+repo-specific rules, and which other Makefiles get included to do the heavy
+lift. Here's an example from eng.git:
+
+    DOC_FILES        = index.md boilerplateapi.md
+    JS_FILES        := $(shell ls *.js) $(shell find lib test -name '*.js')
+    JSL_CONF_NODE    = tools/jsl.node.conf
+    JSL_FILES_NODE   = $(JS_FILES)
+    JSSTYLE_FILES    = $(JS_FILES)
+    JSSTYLE_FLAGS    = -o indent=4,doxygen,unparenthesized-return=0
+    SMF_MANIFESTS_IN = smf/manifests/bapi.xml.in
+
+    include ./Makefile.defs
+    include ./Makefile.node.defs
+    include ./Makefile.smf.defs
+
+    .PHONY: all
+    all: $(SMF_MANIFESTS) | $(NPM_EXEC)
+        $(NPM) install
+
+    include ./Makefile.deps
+    include ./Makefile.node.targ
+    include ./Makefile.smf.targ
+    include ./Makefile.targ
+
+All of the included Makefiles are modular and know nothing about this repo, but
+this Makefile provides all of the required targets: "docs" to build HTML from
+restdown, "check" to check SMF manifests as well as JavaScript style and lint,
+"all" to build node and npm and then use that npm to rebuild local dependencies,
+"clean" to remove built files, and so on.
+
+See the top-level Makefile in eng.git for the complete details.
+
+
+# Software development process
+
+Team synchronization begins daily with our morning scrum. We use
+continuous integration with Git/Gerrit. Bugs and feature requests are tracked in Jira.
+For more details on Joyent's morning scrum please read the: [Onboarding
+Guide](https://mo.joyent.com/docs/engdoc/master/engguide/onboard.html#scrum).
+
+In general, process is shrink-to-fit: we adopt process that help us work better,
+but process for process's sake is avoided. Any resemblance to formalized
+methodologies, living or dead, is purely coincidental.
+
+# Security Statement and Best Practices
+
+Joyent Engineering makes security a top priority for all of our projects. All engineering work is expected to follow industry best practices. New changes affecting security are reviewed by a developer other than the person who wrote the new code. Both developers test that these changes are not vulnerable to the OWASP top 10 security, pass PCI DSS, and are safe.
+
+Common vulnerabilities to watch out for:
+
+- Prevent code injection
+- Buffer overflow. Truncate strings at their maximum length.
+- Encrypt all sensitive data over HTTPS.
+- Do not leak sensitive data into error logs.
+- Block cross-site-scripting(XSS) by specifically validating input and auto-escaping HTML template output.
+- Wrap all routes in security checks to verify user passes ACLs.
+- Prevent cross-site-request-forgery(CSRF)
+
+## Production code deployment process
+
+For the Joyent Public Cloud, Jira change tickets should include the following before the code is promoted to production:
+
+- Description of the change's impact
+- Record of approval by authorized stake holders
+- Confirmation of the code's functionality and proof that vulnerablity testing was performed. A log or screenshot from a security scanner is sufficient
+- Steps to undo this change if necessary.
+
+For reference, read the [owasp top 10](https://www.owasp.org/index.php/Category:OWASP_Top_Ten_Project) vulnerabilities.
+
+# Community Interaction
+
+Due to the open source nature of Joyent software, community interaction is very
+important.
+
+There are mailing lists and IRC channels for top-level Joyent projects (Triton,
+Manta, SmartOS). In addition to using these channels for assisting community
+members with developing and using Joyent products, they are useful for notifying
+the community of major changes.
+
+## Flag Day and Heads Up Notifications
+
+A flag day change is a change that makes a service or tool incompatible with
+another service or tool. Flag day changes can result in complicated operational
+procedures to ensure dependent services and tools are updated in lock step.
+An example would be changing Manta's Muskie to communicate over the Gopher
+protocol instead of HTTP. All of the existing client software would have to be
+redeployed to a version supporting the Gopher protocol at the exact same time
+Muskie is updated.
+
+Flag days should be avoided, but sometimes they are necessary. It is important
+to notify the community as soon as flag day changes are integrated. For changes
+that require attention, but are not flag days, a heads up email should be sent.
+In either case, the message must be sent to the relevant public distribution
+lists (e.g. sdc-discuss).
+
+For heads up notifications, the subject line should include '[HEADS-UP]' and a
+brief summary of what has changed.
+
+For flag day notifications, the subject line should include '[FLAG-DAY]' and a
+list of components effected by the flag day.
+
+The body of each message should start with a statement describing who
+may safely ignore the message. This should be followed by a brief description
+of the change that was integrated and a ticket for the change.
+
+Heads up notifications may optionally include information describing what will
+happen if no action is taken.
+
+Flag day notifications should also include or link to instructions for how to
+successfully complete the flag day upgrade.
+
+Here is an example of a flag day email:
+
+```
+Subject: [FLAG-DAY] OS-4498 (illumos-joyent and smartos-live)
+Body:
+Hi folks,
+
+If you are not building your own SmartOS (or SDC) platform images,
+you can ignore this mail.
+
+I just put back OS-4498. This slightly changes the behaviour of
+"custr_cstr()" from "libcmdutils.so.1" in the OS, so if you are
+updating your copy of "smartos-live" you should first update your
+"illumos-joyent".
+```
+
+Here is an example of a heads up email:
+
+```
+Subject: [HEADS-UP] storage zone CPU shares
+Body:
+I've pushed the change for:
+MANTA-1573 mako should have larger shares than marlin compute zones
+
+This increases the cpu_shares allotted for storage zones, which can improve
+download/upload performance when the system is saturated with compute jobs
+(at the potential expense of the compute jobs).  This change only affects new
+deployments.  If you want to apply it to an existing Manta deployment, you'll
+want to do something like this for each "storage" zone in your fleet:
+
+    vmadm update $zone_uuid cpu_shares=2048
+
+You should probably also update the Manta SAPI application so that subsequent
+storage zone deployments get the updated value.  That will be something like
+this:
+
+    echo '{ "params": { "cpu_shares": 2048 } }' | \
+        sapiadm update $uuid
+
+where $uuid is the uuid of the "storage" SAPI service.
+```
+
+# Miscellaneous Best Practices
+
+- Use JSON for config data. Not ini files: iniparser module has bugs, there
+  are always questions about encoding non-string values.
+- For services and distributed systems, consider building rich tools to
+  understand the state of the service, like lists of the service's objects and
+  information about each one. Think of the SmartOS proc(1) tools (see man pages
+  for pgrep, pstack, pfiles, pargs).
+- Consider doing development inside a SmartOS zone rather than on your Macbook
+  or a CoaL global zone. That forces us to use our product the way customers
+  might, and it eliminates classes of problems where the dev environment doesn't
+  match production (e.g., because you've inadvertently picked up a
+  globally-installed library instead of checking it in, or resource limits
+  differ between MacOS and a SmartOS zone.
+- Whether you develop in CoaL or on your Macbook, document what's necessary to
+  get from scratch to a working development environment so that other people can
+  try it out. Ideally, automate it. Having a script is especially useful if you
+  do develop on CoaL, which also forces you to keep it up to date.
+- Similarly, build tools to automate deploying bits to a test system (usually a
+  SmartOS headnode zone). The easier it is to test the actual deployment, the
+  more likely people will actually test that, and you'll catch environment
+  issues in development instead of after pushing.
+
+
+# Examples
+
+- The [boilerplate API](./boilerplateapi.html) example in this repo gives you a
+  starter file and some suggestions on how to document a web service.
+
+---
+
+<a name="footnote1">[1]</a> : Why? I don't know, but this was enough to convince me to stop
+worrying about it [80-characters line length limit in 2017 (and
+later)](http://katafrakt.me/2017/09/16/80-characters-line-length-limit/)
diff --git a/manifest.tmpl b/manifest.tmpl
new file mode 100644
index 0000000..665d7b9
--- /dev/null
+++ b/manifest.tmpl
@@ -0,0 +1,27 @@
+{
+  "v": 2,
+  "uuid": "UUID",
+  "owner": "00000000-0000-0000-0000-000000000000",
+  "name": "NAME",
+  "version": "VERSION",
+  "state": "active",
+  "disabled": false,
+  "public": false,
+  "type": "other",
+  "os": "other",
+  "files": [
+    {
+      "sha1": "SHA",
+      "size": SIZE,
+      "compression": "gzip"
+    }
+  ],
+  "description": "DESCRIPTION",
+  "tags": {
+    "smartdc_service": "true",
+    "buildstamp": "BUILDSTAMP"
+  },
+  "channels": [
+    "dev"
+  ]
+}
diff --git a/npm/lib/error_handler.sh b/npm/lib/error_handler.sh
new file mode 100755
index 0000000..c7da4e8
--- /dev/null
+++ b/npm/lib/error_handler.sh
@@ -0,0 +1,73 @@
+#!/bin/bash
+#
+# This Source Code Form is subject to the terms of the Mozilla Public
+# License, v. 2.0. If a copy of the MPL was not distributed with this
+# file, You can obtain one at http://mozilla.org/MPL/2.0/.
+#
+
+#
+# Copyright 2019 Joyent, Inc.
+#
+
+function stack_trace
+{
+    set +o xtrace
+
+    (( cnt = ${#FUNCNAME[@]} ))
+    (( i = 0 ))
+    while (( i < cnt )); do
+        printf '  [%3d] %s\n' "${i}" "${FUNCNAME[i]}"
+        if (( i > 0 )); then
+            line="${BASH_LINENO[$((i - 1))]}"
+        else
+            line="${LINENO}"
+        fi
+        printf '        (file "%s" line %d)\n' "${BASH_SOURCE[i]}" "${line}"
+        (( i++ ))
+    done
+}
+
+function fatal
+{
+    # Disable error traps from here on:
+    set +o xtrace
+    set +o errexit
+    set +o errtrace
+    set +o nounset
+    trap '' ERR
+
+    local fmt="$1"
+    shift
+
+    printf "ERROR: $(basename $0): ${fmt}\n" "$@" >&2
+    stack_trace
+    exit 1
+}
+
+function trap_err
+{
+    st=$?
+    set +o nounset
+    fatal 'exit status %d at line %d' ${st} ${BASH_LINENO[0]}
+}
+
+#
+# We set errexit (a.k.a. "set -e") to force an exit on error conditions, but
+# there are many important error conditions that this does not capture --
+# first among them failures within a pipeline (only the exit status of the
+# final stage is propagated).  To exit on these failures, we also set
+# "pipefail" (a very useful option introduced to bash as of version 3 that
+# propagates any non-zero exit values in a pipeline).
+#
+set -o errexit
+set -o pipefail
+
+shopt -s extglob
+
+#
+# Install our error handling trap, so that we can have stack traces on
+# failures.  We set "errtrace" so that the ERR trap handler is inherited
+# by each function call.
+#
+trap trap_err ERR
+set -o errtrace
diff --git a/npm/lib/trace_logger.sh b/npm/lib/trace_logger.sh
new file mode 100755
index 0000000..8a8ad59
--- /dev/null
+++ b/npm/lib/trace_logger.sh
@@ -0,0 +1,36 @@
+#!/bin/bash
+#
+# This Source Code Form is subject to the terms of the Mozilla Public
+# License, v. 2.0. If a copy of the MPL was not distributed with this
+# file, You can obtain one at http://mozilla.org/MPL/2.0/.
+#
+
+#
+# Copyright 2019 Joyent, Inc.
+#
+
+#
+# If TRACE is set in the environment, enable xtrace.  Additionally,
+# assuming the current shell is bash version 4.1 or later, more advanced
+# tracing output will be emitted and some additional features may be used:
+#
+#   TRACE_LOG   Send xtrace output to this file instead of stderr.
+#   TRACE_FD    Send xtrace output to this fd instead of stderr.
+#               The file descriptor must be open before the shell
+#               script is started.
+#
+if [[ -n ${TRACE} ]]; then
+    if [[ ${BASH_VERSINFO[0]} -ge 4 && ${BASH_VERSINFO[1]} -ge 1 ]]; then
+        PS4=
+        PS4="${PS4}"'[\D{%FT%TZ}] ${BASH_SOURCE}:${LINENO}: '
+        PS4="${PS4}"'${FUNCNAME[0]:+${FUNCNAME[0]}(): }'
+        export PS4
+        if [[ -n ${TRACE_LOG} ]]; then
+            exec 4>>${TRACE_LOG}
+            export BASH_XTRACEFD=4
+        elif [[ -n ${TRACE_FD} ]]; then
+            export BASH_XTRACEFD=${TRACE_FD}
+        fi
+    fi
+    set -o xtrace
+fi
diff --git a/npm/postinstall.sh b/npm/postinstall.sh
new file mode 100755
index 0000000..f13a1c7
--- /dev/null
+++ b/npm/postinstall.sh
@@ -0,0 +1,273 @@
+#!/bin/bash
+#
+# This Source Code Form is subject to the terms of the Mozilla Public
+# License, v. 2.0. If a copy of the MPL was not distributed with this
+# file, You can obtain one at http://mozilla.org/MPL/2.0/.
+#
+
+#
+# Copyright 2019 Joyent, Inc.
+#
+
+if [[ "${SDC_AGENT_SKIP_LIFECYCLE:-no}" = "yes" ]]; then
+    printf 'Running during package build; skipping lifecycle script.\n' >&2
+    exit 0
+fi
+
+#
+# We must load the SDC configuration before setting any strict error handling
+# options.
+#
+. /lib/sdc/config.sh
+load_sdc_config
+
+ROOT="$(cd `dirname $0`/../ 2>/dev/null && pwd)"
+
+. "${ROOT}/npm/lib/error_handler.sh"
+. "${ROOT}/npm/lib/trace_logger.sh"
+
+set -o nounset
+
+export PREFIX="$npm_config_prefix"
+export ETC_DIR="$npm_config_etc"
+export SMF_DIR="$npm_config_smfdir"
+export VERSION="$npm_package_version"
+export ENABLED="true"
+
+AGENT="$npm_package_name"
+
+BOOTPARAMS=/usr/bin/bootparams
+AWK=/usr/bin/awk
+
+
+# ---- support functions
+
+#
+# Replace various substitution tokens in the input file, and write the result
+# into the output file.
+#
+function subfile
+{
+    local infile="$1"
+    local outfile="$2"
+
+    if [[ -z "${infile}" || -z "${outfile}" ]]; then
+        fatal 'subfile requires two arguments'
+    fi
+
+    if ! sed -e "s#@@PREFIX@@#$PREFIX#g" \
+      -e "s/@@VERSION@@/$VERSION/g" \
+      -e "s#@@ROOT@@#$ROOT#g" \
+      -e "s/@@ENABLED@@/$ENABLED/g" \
+      "${infile}" > "${outfile}"; then
+        fatal 'sed failure ("%s" -> "%s")' "${infile}" "${outfile}"
+    fi
+}
+
+#
+# Replace substitution tokens in the SMF manifest files, and then import the SMF
+# services.
+#
+function import_smf_manifest
+{
+    local agent_manifest_in="$ROOT/smf/manifests/$AGENT.xml.in"
+    local agent_manifest_out="$SMF_DIR/$AGENT.xml"
+    local agent_setup_manifest_in="$ROOT/smf/manifests/${AGENT}-setup.xml.in"
+    local agent_setup_manifest_out="$SMF_DIR/${AGENT}-setup.xml"
+
+    if [[ ! -f "${agent_manifest_in}" ]]; then
+        fatal 'could not find smf manifest input file: %s' \
+            "${agent_manifest_in}"
+    fi
+
+    if [[ ! -f "${agent_setup_manifest_in}" ]]; then
+        fatal 'could not find smf manifest input file: %s'
+        "${agent_setup_manifest_in}"
+    fi
+
+    if ! subfile "${agent_manifest_in}" "${agent_manifest_out}" ||
+      ! svccfg import "${agent_manifest_out}"; then
+        fatal 'could not process smf manifest (%s)' "${agent_manifest_in}"
+    fi
+
+    if ! subfile "${agent_setup_manifest_in}" "${agent_setup_manifest_out}" ||
+      ! svccfg import "${agent_setup_manifest_out}"; then
+        fatal 'could not process smf manifest (%s)' "${agent_setup_manifest_in}"
+    fi
+}
+
+#
+# Each installation of an agent is represented by a SAPI instance of the SAPI
+# service for that agent.  These UUIDs are persistent, so that upgrades do not
+# induce the generation of a new UUID.  If a UUID has not yet been written to
+# disk, we generate one now.  Otherwise, the existing UUID is read and
+# returned.
+#
+function get_or_create_instance_uuid
+{
+    local uuid_file="${ETC_DIR}/${AGENT}"
+    local uuid
+
+    if [[ -z "${ETC_DIR}" || -z "${AGENT}" ]]; then
+        fatal 'ETC_DIR and AGENT must be set'
+    fi
+
+    if [[ ! -f "${uuid_file}" ]]; then
+        #
+        # The instance UUID file does not exist.  Create one.
+        #
+        printf 'New agent instance.  Generating new UUID.\n' >&2
+        if ! /usr/bin/uuid -v4 >"${uuid_file}"; then
+            fatal 'could not write new UUID to "%s"' "${uuid_file}"
+        fi
+    fi
+
+    if ! uuid="$(<${uuid_file})" || [[ -z "${uuid}" ]]; then
+            fatal 'could not read UUID from "%s"' "${uuid_file}"
+    fi
+
+    printf 'Agent UUID: %s\n' "${uuid}" >&2
+    printf '%s' "${uuid}"
+    return 0
+}
+
+function adopt_instance
+{
+    local instance_uuid=$1
+    local service_uuid
+    local retry=10
+    local url
+    local data
+
+    if [[ -z "${instance_uuid}" ]]; then
+        fatal 'must pass instance_uuid'
+    fi
+
+    while (( retry-- > 0 )); do
+        #
+        # Fetch the UUID of the SAPI service for this agent.
+        #
+        url="${SAPI_URL}/services?type=agent&name=${AGENT}"
+        if ! service_uuid="$(curl -sSf -H 'Accept: application/json' "${url}" \
+          | json -Ha uuid)"; then
+            printf 'Could not retrieve SAPI service UUID for "%s"\n' \
+              "${AGENT}" >&2
+            sleep 5
+            continue
+        fi
+
+        #
+        # Attempt to register the SAPI instance for this agent installation.
+        # We need not be overly clever here; SAPI returns success for a
+        # duplicate agent adoption.
+        #
+        url="${SAPI_URL}/instances"
+        data="{
+            \"service_uuid\": \"${service_uuid}\",
+            \"uuid\": \"${instance_uuid}\"
+        }"
+        if ! curl -sSf -X POST -H 'Content-Type: application/json' \
+          -d "${data}" "${url}"; then
+            printf 'Could not register SAPI instance with UUID "%s"\n' \
+              "${instance_uuid}" >&2
+            sleep 5
+            continue
+        fi
+
+        printf 'Agent successfully adopted into SAPI.\n' >&2
+        return 0
+    done
+
+    fatal 'adopt_instance: failing after too many retries'
+}
+
+#
+# Check if we expect SAPI to be available.  Generally, registering with SAPI is
+# a hard requirement for the correct functioning of the system, but this
+# postinstall script can also be run during headnode setup; SAPI is not yet
+# available at that point.
+#
+function sapi_should_be_available
+{
+    local headnode
+    local script
+    local setup_complete
+
+    #
+    # In the event that SAPI is unavailable, we allow the operator to force us
+    # not to register with SAPI.  This behaviour should NOT be exercised
+    # programatically; it exists purely to allow operators to attempt
+    # (manually) to correct in the face of an abject failure of the system.
+    #
+    if [[ "${NO_SAPI:-false}" = true ]]; then
+        printf 'NO_SAPI=true in environment.\n' >&2
+        return 1
+    fi
+
+    script='
+        $1 == "headnode" {
+            print $2;
+            exit 0;
+        }
+    '
+    if ! headnode=$(${BOOTPARAMS} | ${AWK} -F= "${script}"); then
+        fatal 'could not read bootparams'
+    fi
+
+    if [[ "${headnode}" != 'true' ]]; then
+        #
+        # This is a compute node.  SAPI is expected to be available, and
+        # registration is expected to work.
+        #
+        printf 'This is not the headnode.\n' >&2
+        return 0
+    fi
+
+    #
+    # This is the headnode.  If setup has not yet been completed, then SAPI
+    # is not yet available.
+    #
+    if [[ ! -f '/var/lib/setup.json' ]]; then
+        fatal 'could not find setup state file: "/var/lib/setup.json"'
+    fi
+    if ! setup_complete=$(json -f '/var/lib/setup.json' 'complete'); then
+        fatal 'could not read "complete" from "/var/lib/setup.json"'
+    fi
+
+    if [[ "${setup_complete}" = true ]]; then
+        #
+        # Setup is complete.  SAPI is available.  Registration is expected
+        # to work.
+        #
+        printf 'This is the headnode, and setup is already complete.\n' >&2
+        return 0
+    fi
+
+    #
+    # Setup is not yet complete.  The headnode setup process will register
+    # this SAPI instance at the appropriate time.
+    #
+    printf 'This is the headnode, but setup is not yet complete.\n' >&2
+    return 1
+}
+
+
+# ---- mainline
+
+if [[ -z "${CONFIG_sapi_domain}" ]]; then
+    fatal '"sapi_domain" was not found in "node.config".'
+fi
+SAPI_URL="http://${CONFIG_sapi_domain}"
+
+import_smf_manifest
+
+INSTANCE_UUID="$(get_or_create_instance_uuid)"
+
+if sapi_should_be_available; then
+    printf 'SAPI expected to be available.  Adopting agent instance.\n' >&2
+    adopt_instance "${INSTANCE_UUID}"
+else
+    printf 'SAPI not yet available.  Skipping agent registration.\n' >&2
+fi
+
+exit 0
diff --git a/npm/postuninstall.sh b/npm/postuninstall.sh
new file mode 100755
index 0000000..dd8c3a3
--- /dev/null
+++ b/npm/postuninstall.sh
@@ -0,0 +1,33 @@
+#!/bin/bash
+#
+# This Source Code Form is subject to the terms of the Mozilla Public
+# License, v. 2.0. If a copy of the MPL was not distributed with this
+# file, You can obtain one at http://mozilla.org/MPL/2.0/.
+#
+
+#
+# Copyright 2019 Joyent, Inc.
+#
+
+if [[ "${SDC_AGENT_SKIP_LIFECYCLE:-no}" = "yes" ]]; then
+    printf 'Running during package build; skipping lifecycle script.\n' >&2
+    exit 0
+fi
+
+ROOT="$(cd `dirname $0`/../ 2>/dev/null && pwd)"
+
+. "${ROOT}/npm/lib/error_handler.sh"
+. "${ROOT}/npm/lib/trace_logger.sh"
+
+set -o nounset
+
+export SMF_DIR="${npm_config_smfdir}"
+
+AGENT="${npm_package_name}"
+
+if svcs "${AGENT}"; then
+    svcadm disable -s "${AGENT}"
+    svccfg delete "${AGENT}"
+fi
+
+rm -f "${SMF_DIR}/${AGENT}.xml"
diff --git a/package.json b/package.json
new file mode 100644
index 0000000..e88de48
--- /dev/null
+++ b/package.json
@@ -0,0 +1,16 @@
+{
+  "name": "firewall-logger-agent",
+  "description": "Triton Firewall Logger Agent",
+  "version": "1.0.0",
+  "author": "Joyent (joyent.com)",
+  "private": true,
+  "dependencies": {
+  },
+  "devDependencies": {
+  },
+  "scripts": {
+    "postinstall": "./npm/postinstall.sh",
+    "postuninstall": "./npm/postuninstall.sh"
+  },
+  "license": "MPL-2.0"
+}
diff --git a/smf/manifests/firewall-logger-agent-setup.xml.in b/smf/manifests/firewall-logger-agent-setup.xml.in
new file mode 100644
index 0000000..880de7e
--- /dev/null
+++ b/smf/manifests/firewall-logger-agent-setup.xml.in
@@ -0,0 +1,55 @@
+<?xml version="1.0"?>
+<!DOCTYPE service_bundle SYSTEM "/usr/share/lib/xml/dtd/service_bundle.dtd.1">
+<!--
+    This Source Code Form is subject to the terms of the Mozilla Public
+    License, v. 2.0. If a copy of the MPL was not distributed with this
+    file, You can obtain one at http://mozilla.org/MPL/2.0/.
+-->
+
+<!--
+    Copyright 2019 Joyent, Inc.
+-->
+
+<service_bundle type="manifest" name="firewall-logger-agent-setup">
+  <service name="smartdc/agent/firewall-logger-agent-setup" type="service" version="@@VERSION@@">
+
+    <create_default_instance enabled="@@ENABLED@@"/>
+    <single_instance/>
+
+    <dependency name="network" grouping="require_all" restart_on="error" type="service">
+      <service_fmri value="svc:/milestone/network:default"/>
+    </dependency>
+
+    <exec_method
+      type="method"
+      name="start"
+      exec="@@ROOT@@/smf/method/firewall-logger-agent-setup %m"
+      timeout_seconds="600">
+      <method_context>
+        <method_credential user="root" group="staff"/>
+        <method_environment>
+          <envvar name="PATH" value="/usr/bin:/usr/sbin"/>
+        </method_environment>
+      </method_context>
+    </exec_method>
+
+    <exec_method type="method" name="stop" exec=":true" timeout_seconds="60" />
+
+    <property_group name="startd" type="framework">
+      <propval name="duration" type="astring" value="transient" />
+      <propval name="ignore_error" type="astring" value="core,signal" />
+    </property_group>
+
+    <property_group name="application" type="application">
+    </property_group>
+
+    <stability value="Evolving"/>
+
+    <template>
+      <common_name>
+        <loctext xml:lang="C">Triton Firewall Logger Agent Setup</loctext>
+      </common_name>
+    </template>
+
+  </service>
+</service_bundle>
diff --git a/smf/manifests/firewall-logger-agent.xml b/smf/manifests/firewall-logger-agent.xml
new file mode 100644
index 0000000..58b254b
--- /dev/null
+++ b/smf/manifests/firewall-logger-agent.xml
@@ -0,0 +1,79 @@
+<?xml version="1.0"?>
+<!DOCTYPE service_bundle SYSTEM "/usr/share/lib/xml/dtd/service_bundle.dtd.1">
+<!--
+    This Source Code Form is subject to the terms of the Mozilla Public
+    License, v. 2.0. If a copy of the MPL was not distributed with this
+    file, You can obtain one at http://mozilla.org/MPL/2.0/.
+-->
+
+<!--
+    Copyright 2019 Joyent, Inc.
+-->
+
+<service_bundle type="manifest" name="firewall-logger-agent">
+  <service name="smartdc/agent/firewall-logger-agent" type="service" version="@@VERSION@@">
+
+    <create_default_instance enabled="@@ENABLED@@"/>
+    <single_instance/>
+
+    <dependency name="network" grouping="require_all" restart_on="error" type="service">
+      <service_fmri value="svc:/milestone/network:default"/>
+    </dependency>
+
+    <dependency name="filesystem" grouping="require_all" restart_on="error" type="service">
+      <service_fmri value="svc:/system/filesystem/local"/>
+    </dependency>
+
+    <dependency name="ipfilter" grouping="require_all" restart_on="none" type="service">
+      <service_fmri value="svc:/network/ipfilter:default"/>
+    </dependency>
+
+    <dependency name="vminfod" grouping="require_all" restart_on="none" type="service">
+      <service_fmri value="svc:/system/smartdc/vminfod:default"/>
+    </dependency>
+
+    <exec_method
+      type="method"
+      name="start"
+      exec="@@ROOT@@/bin/cfwlogd &amp;"
+      timeout_seconds="60">
+      <method_context>
+        <method_credential user="root" group="staff"/>
+        <method_environment>
+          <envvar name="PATH" value="@@PREFIX@@/bin:/usr/bin:/usr/sbin"/>
+          <envvar name='UMEM_OPTIONS' value='perthread_cache=64M'/>
+          <envvar name='RUST_BACKTRACE' value='full'/>
+          <envvar name='RUST_LOG' value='cfwlogd=debug'/>
+        </method_environment>
+      </method_context>
+    </exec_method>
+
+    <exec_method type="method" name="restart" exec=":kill" timeout_seconds="60">
+      <method_context>
+        <method_credential user="root" group="staff"/>
+      </method_context>
+    </exec_method>
+
+    <exec_method type="method" name="stop" exec=":kill" timeout_seconds="60">
+      <method_context>
+        <method_credential user="root" group="staff"/>
+      </method_context>
+    </exec_method>
+
+    <property_group name="startd" type="framework">
+      <propval name="ignore_error" type="astring" value="core,signal"/>
+    </property_group>
+
+    <property_group name="application" type="application">
+    </property_group>
+
+    <stability value="Evolving"/>
+
+    <template>
+      <common_name>
+        <loctext xml:lang="C">Triton Firewall Logger Agent</loctext>
+      </common_name>
+    </template>
+
+  </service>
+</service_bundle>
diff --git a/smf/manifests/firewall-logger-agent.xml.in b/smf/manifests/firewall-logger-agent.xml.in
new file mode 100644
index 0000000..58b254b
--- /dev/null
+++ b/smf/manifests/firewall-logger-agent.xml.in
@@ -0,0 +1,79 @@
+<?xml version="1.0"?>
+<!DOCTYPE service_bundle SYSTEM "/usr/share/lib/xml/dtd/service_bundle.dtd.1">
+<!--
+    This Source Code Form is subject to the terms of the Mozilla Public
+    License, v. 2.0. If a copy of the MPL was not distributed with this
+    file, You can obtain one at http://mozilla.org/MPL/2.0/.
+-->
+
+<!--
+    Copyright 2019 Joyent, Inc.
+-->
+
+<service_bundle type="manifest" name="firewall-logger-agent">
+  <service name="smartdc/agent/firewall-logger-agent" type="service" version="@@VERSION@@">
+
+    <create_default_instance enabled="@@ENABLED@@"/>
+    <single_instance/>
+
+    <dependency name="network" grouping="require_all" restart_on="error" type="service">
+      <service_fmri value="svc:/milestone/network:default"/>
+    </dependency>
+
+    <dependency name="filesystem" grouping="require_all" restart_on="error" type="service">
+      <service_fmri value="svc:/system/filesystem/local"/>
+    </dependency>
+
+    <dependency name="ipfilter" grouping="require_all" restart_on="none" type="service">
+      <service_fmri value="svc:/network/ipfilter:default"/>
+    </dependency>
+
+    <dependency name="vminfod" grouping="require_all" restart_on="none" type="service">
+      <service_fmri value="svc:/system/smartdc/vminfod:default"/>
+    </dependency>
+
+    <exec_method
+      type="method"
+      name="start"
+      exec="@@ROOT@@/bin/cfwlogd &amp;"
+      timeout_seconds="60">
+      <method_context>
+        <method_credential user="root" group="staff"/>
+        <method_environment>
+          <envvar name="PATH" value="@@PREFIX@@/bin:/usr/bin:/usr/sbin"/>
+          <envvar name='UMEM_OPTIONS' value='perthread_cache=64M'/>
+          <envvar name='RUST_BACKTRACE' value='full'/>
+          <envvar name='RUST_LOG' value='cfwlogd=debug'/>
+        </method_environment>
+      </method_context>
+    </exec_method>
+
+    <exec_method type="method" name="restart" exec=":kill" timeout_seconds="60">
+      <method_context>
+        <method_credential user="root" group="staff"/>
+      </method_context>
+    </exec_method>
+
+    <exec_method type="method" name="stop" exec=":kill" timeout_seconds="60">
+      <method_context>
+        <method_credential user="root" group="staff"/>
+      </method_context>
+    </exec_method>
+
+    <property_group name="startd" type="framework">
+      <propval name="ignore_error" type="astring" value="core,signal"/>
+    </property_group>
+
+    <property_group name="application" type="application">
+    </property_group>
+
+    <stability value="Evolving"/>
+
+    <template>
+      <common_name>
+        <loctext xml:lang="C">Triton Firewall Logger Agent</loctext>
+      </common_name>
+    </template>
+
+  </service>
+</service_bundle>
diff --git a/smf/method/firewall-logger-agent-setup b/smf/method/firewall-logger-agent-setup
new file mode 100755
index 0000000..03bf37f
--- /dev/null
+++ b/smf/method/firewall-logger-agent-setup
@@ -0,0 +1,52 @@
+#!/bin/bash
+#
+# This Source Code Form is subject to the terms of the Mozilla Public
+# License, v. 2.0. If a copy of the MPL was not distributed with this
+# file, You can obtain one at http://mozilla.org/MPL/2.0/.
+#
+
+#
+# Copyright 2019 Joyent, Inc.
+#
+
+#
+# Runs on node (CN + HN) boot to setup log rotation for firewall-logger agent logs.
+#
+
+set -o xtrace
+
+. /lib/svc/share/smf_include.sh
+. /lib/sdc/config.sh
+
+
+function setup_logadm {
+    # firewall-logger-agent logadm configuration:
+    logadm -v -w firewall_logger_logs -C 168 -S 1g -c -p 1h \
+        -t '/var/log/firewall-logger-agent/firewall-logger-agent_$nodename_%FT%H:%M:%S.log' \
+        /var/svc/log/*firewall-logger-agent*.log
+
+    # Move the smf_logs entry to run last (after the entries we just added) so
+    # that the default '-C' (from
+    # https://github.com/joyent/smartos-live/blob/master/overlay/generic/etc/logadm.conf)
+    # doesn't defeat our attempts to rotate out of "/var/svc/log".
+    /usr/sbin/logadm -v -r smf_logs
+    /usr/sbin/logadm -v -w smf_logs -C 8 -c -s 1m '/var/svc/log/*.log'
+}
+
+
+case "$1" in
+'start')
+    setup_logadm
+    ;;
+
+'stop')
+    echo "'stop' not yet supported" >&2
+    exit $SMF_EXIT_ERR_FATAL
+    ;;
+
+*)
+    echo "Usage: $0 { start | stop }"
+    exit $SMF_EXIT_ERR_FATAL
+    ;;
+esac
+exit $SMF_EXIT_OK
diff --git a/testutils/Cargo.toml b/testutils/Cargo.toml
new file mode 100644
index 0000000..0afbeea
--- /dev/null
+++ b/testutils/Cargo.toml
@@ -0,0 +1,11 @@
+[package]
+name = "testutils"
+version = "0.1.0"
+authors = ["Mike Zeller <mike@mikezeller.net>"]
+edition = "2018"
+
+[dependencies]
+chrono = "0.4.6"
+uuid = { version = "0.7.4", features = ["v4"] }
+vminfod = { path = "../vminfod" }
+rand = "0.6.5"
diff --git a/testutils/src/lib.rs b/testutils/src/lib.rs
new file mode 100644
index 0000000..0eec366
--- /dev/null
+++ b/testutils/src/lib.rs
@@ -0,0 +1,87 @@
+use chrono::{TimeZone, Utc};
+use rand::{thread_rng, Rng};
+use std::net::Ipv6Addr;
+use uuid::Uuid;
+use vminfod::Zone;
+
+// C representation of a cfw event
+#[derive(PartialEq, Debug)]
+#[repr(C)]
+pub struct Event {
+    pub event: u16,
+    pub length: u16,
+    pub zonedid: u32,
+    pub rule_id: u32,
+    pub source_port: u16,
+    pub destination_port: u16,
+    pub protocol: u8,
+    pub direction: u8,
+    pub reserved: [u8; 6],
+    pub source_ip: u128,
+    pub destination_ip: u128,
+    pub time_sec: i64,
+    pub time_usec: i64,
+    pub rule_uuid: [u8; 16],
+}
+
+impl Event {
+    pub fn as_bytes(&self) -> &[u8] {
+        unsafe {
+            ::std::slice::from_raw_parts(
+                (self as *const Self) as *const u8,
+                ::std::mem::size_of::<Self>(),
+            )
+        }
+    }
+}
+
+pub fn generate_event() -> Event {
+    let mut rng = thread_rng();
+
+    let ip_s: u128 = "::ffff:172.24.4.150".parse::<Ipv6Addr>().unwrap().into();
+    let ip_d: u128 = "::ffff:172.24.4.151".parse::<Ipv6Addr>().unwrap().into();
+    let port_s: u16 = rng.gen_range(1, 65535);
+    let port_d: u16 = rng.gen_range(1, 65535);
+    let now = chrono::offset::Utc::now();
+    // unix timeval only contains microseconds
+    let ts = Utc.timestamp(now.timestamp(), now.timestamp_subsec_micros() * 1000);
+    let uuid = Uuid::new_v4();
+
+    Event {
+        event: 1,
+        length: std::mem::size_of::<Event>() as u16,
+        zonedid: 16,
+        rule_id: rng.gen_range(0, u32::max_value()),
+        source_port: port_s.to_be(),
+        destination_port: port_d.to_be(),
+        protocol: 6,  // TCP
+        direction: 1, // In
+        reserved: [0; 6],
+        source_ip: ip_s.to_be(),
+        destination_ip: ip_d.to_be(),
+        time_sec: ts.timestamp(),
+        time_usec: i64::from(ts.timestamp_subsec_micros()),
+        rule_uuid: *uuid.as_bytes(),
+    }
+}
+
+pub fn generate_event_for_zone(z: &Zone) -> Event {
+    let mut e = generate_event();
+    e.zonedid = z.zonedid;
+    e
+}
+
+pub fn create_zone() -> Zone {
+    let mut rng = thread_rng();
+    Zone {
+        uuid: Uuid::new_v4().to_hyphenated().to_string(),
+        alias: Some("zone1".to_owned()),
+        owner_uuid: Uuid::new_v4().to_string(),
+        firewall_enabled: true,
+        zonedid: rng.gen_range(0, u32::max_value()),
+    }
+}
+
+pub fn generate_zones(n: usize) -> Vec<Zone> {
+    std::iter::repeat_with(create_zone).take(n).collect()
+}
diff --git a/tools/agent-prebuilt.sh b/tools/agent-prebuilt.sh
new file mode 100644
index 0000000..b11f710
--- /dev/null
+++ b/tools/agent-prebuilt.sh
@@ -0,0 +1,460 @@
+#!/bin/bash
+#
+# This Source Code Form is subject to the terms of the Mozilla Public
+# License, v. 2.0. If a copy of the MPL was not distributed with this
+# file, You can obtain one at http://mozilla.org/MPL/2.0/.
+#
+
+#
+# Copyright (c) 2019, Joyent, Inc.
+#
+
+#
+# Used by Makefile.agent_prebuilt.*, this script deals with managing the
+# cache of prebuilt Triton/Manta agent builds which are consumed by other
+# components. We don't expect developers to ever invoke this shell script
+# by hand. Instead, this script exists because it's easier to write/test
+# operations here than to try to embed lots of shell logic in a Makefile
+# target.
+#
+
+set -o errexit
+trap cleanup EXIT
+
+#
+# The options that this script takes correspond to the following
+# Makefile.agent_prebuilt.defs macros:
+#
+# -B AGENT_PREBUILT_AGENT_BRANCH    the branch of the agent sources to build
+# -b AGENT_PREBUILT_BRANCH          an alternate branch of the agent sources
+#                                   to build. The build will first try to
+#                                   checkout the repository at the -B branch,
+#                                   then the -b branch, then fall back to
+#                                   'master'
+# -c AGENT_PREBUILT_DIR             the top level cache where we clone/build
+#                                   agents
+# -d <name>_PREBUILT_ROOTDIR        where in the image the package resides
+# -p <name>_PREBUILT_TARBALL_PATTERN  a glob pattern to match the built or
+#                                     downloaded agent
+# -r <name>_PREBUILT_REPO           the local repository name
+# -t <name>_PREBUILT_AGENT_TARGETS  the make targets in that repository to build
+# -u <name>_PREBUILT_GIT_URL        the git repository containing the agent
+#                                   source
+# -U AGENT_PREBUILT_URL             the url to download prebuilt agent tarballs
+#                                   from
+#
+function usage {
+    echo "Usage: agent-prebuilt <options> <command>"
+    echo "COMMANDS:"
+    echo "  clone               clone the git repository to the given branch"
+    echo "  build               build the supplied -t targets"
+    echo "  clean               clean the git repository"
+    echo "  download            download from a http:// or file:/// "
+    echo "  extract             extract the tarball within an image proto dir"
+    echo "  show_tarball        prints the path of the latest tarball"
+    echo ""
+    echo "OPTIONS:"
+    echo "  -b <branch>         the branch to checkout and build"
+    echo "  -B <agent_branch>   the agent_branch to checkout and build"
+    echo "  -c <dir>            the location of the agent_cache"
+    echo "  -d <dir>            where in the image the package resides"
+    echo "  -p <pattern>        a glob pattern to match the built agent tarball"
+    echo "  -r <repo name>      the local repository directory name"
+    echo "  -t <target>         the make targets to build"
+    echo "  -u <url>            the git repository to clone"
+    echo "  -U <url>            a http:// URL or file:/// dir containing"
+    echo "                      prebuilt tarballs"
+    exit 2
+}
+
+function do_clone {
+    set +o errexit
+    git_exit=0
+    if [[ -d $agent_cache_dir/$repo_name ]]; then
+        cd $agent_cache_dir/$repo_name
+        if [[ $? -ne 0 ]]; then
+            echo "ERROR: unable to cd to $agent_cache_dir/$repo_name"
+            return 1
+        fi
+        # ensure there are no uncommitted changes before attempting to
+        # rebase
+        uncommitted=$(git status --porcelain)
+        if [[ -n "$uncommitted" ]]; then
+            echo "ERROR: uncommitted changes in $agent_cache_dir/$repo_name"
+            echo "Please commit these before attempting to rebase."
+            return 1
+        fi
+        git pull --rebase
+        if [[ $? -ne 0 ]]; then
+            echo "WARNING: Pulling from $git_url failed, which might be ok if"
+            echo "the branch the existing repository is checked out to doesn't"
+            echo "exist upstream."
+        fi
+        git checkout $agent_branch --
+        if [[ $? -ne 0 ]]; then
+            echo "Checking out $agent_branch failed, falling back to $branch"
+            git checkout $branch --
+        fi
+        if [[ $? -ne 0 ]]; then
+            # at this point, failures are really fatal.
+            set -o errexit
+            echo "Checking out $branch also failed, falling back to master"
+            git checkout master --
+            git_exit=$?
+        fi
+    else
+        cd $agent_cache_dir
+        if [[ $? -ne 0 ]]; then
+            echo "ERROR: unable to cd to $agent_cache_dir"
+            return 1
+        fi
+        git clone -b $agent_branch $git_url $repo_name
+        if [[ $? -ne 0 ]]; then
+            echo "Cloning branch $agent_branch failed, falling back to $branch"
+            git clone -b $branch $git_url $repo_name
+        fi
+        if [[ $? -ne 0 ]]; then
+            set -o errexit
+            echo "Cloning branch $branch also failed, falling back to master"
+            git clone -b master $git_url $repo_name
+            git_exit=$?
+        fi
+    fi
+    set -o errexit
+    return $git_exit
+}
+
+function do_build {
+    if [[ ! -d $agent_cache_dir/$repo_name ]]; then
+        echo "Cannot do a build when $repo_name dir is missing!"
+        return 1
+    fi
+    # this sets $LATEST_TARBALL as a side effect
+    get_tarball allow_empty
+    cd $agent_cache_dir/$repo_name
+    if [[ -n "$LATEST_TARBALL" ]]; then
+        # check it matches our hash
+        git_hash=$(git describe --all --long --dirty | awk -F'-g' '{print $NF}')
+        hash_present=$(echo $LATEST_TARBALL | grep "g${git_hash}" || true)
+        if [[ -n "$hash_present" ]]; then
+            echo "Latest tarball $LATEST_TARBALL seems fresh. Not rebuilding."
+            return 0
+        else
+            echo "Latest tarball is stale"
+            echo "  latest tarball: $LATEST_TARBALL"
+            echo "current git hash: $git_hash ($branch)"
+        fi
+    fi
+
+    #
+    # Our agents should be able to build and run anywhere,
+    # so relax the build checks for them. If any agents ever
+    # create dependencies against /opt/local, that would be
+    # bad.
+    #
+    ENGBLD_SKIP_VALIDATE_BUILDENV=true gmake $agent_targets
+    return $?
+}
+
+function do_clean {
+    rm -rf $agent_cache_dir/$repo_name
+    return 0
+}
+
+
+#
+# Determine the full path of the latest tarball, or the location we downloaded
+# it to if $agent_url is set, set as $LATEST_TARBALL.
+#
+function get_tarball {
+
+    if [[ -n "$1" ]]; then
+        allow_empty=true
+    fi
+
+    # 'basename', but using a bash builtin
+    file_pattern=${tarball_pattern##*/}
+    dir_pattern=$(dirname $tarball_pattern)
+    if [[ -z "$agent_url" ]]; then
+        latest_tarball_dir=$agent_cache_dir/$repo_name/$dir_pattern/
+    else
+        # downloaded tarballs are dumped at the top level of the agent_cache_dir
+        latest_tarball_dir=$agent_cache_dir
+    fi
+
+    # look for the agent_branch file
+    if [[ -n "$agent_branch" ]]; then
+        latest_tarball_file=$(
+            /usr/bin/ls -1 $latest_tarball_dir | grep $file_pattern \
+            2>/dev/null | grep $agent_branch | sort | tail -1)
+    fi
+    if [[ -z "$latest_tarball_file" ]]; then
+        # fall back to the branch file
+        latest_tarball_file=$(
+            /usr/bin/ls -1 $latest_tarball_dir | grep $file_pattern \
+            2>/dev/null | grep $branch | sort | tail -1)
+    fi
+    if [[ -z "$latest_tarball_file" ]]; then
+        # fall back to the master branch
+	latest_tarball_file=$(
+            /usr/bin/ls -1 $latest_tarball_dir | grep $file_pattern \
+            2>/dev/null | grep master | sort | tail -1)
+    fi
+    if [[ -z "$latest_tarball_file" ]]; then
+        if [[ -n "$allow_empty" ]]; then
+            LATEST_TARBALL=""
+            return
+        fi
+        echo "No (agent_branch) $agent_branch or (branch) $branch or master \
+            tarball for $tarball_pattern at $latest_tarball_dir"
+        exit 1
+    fi
+
+    LATEST_TARBALL=$latest_tarball_dir/$latest_tarball_file
+}
+
+#
+# Extract the agent into the image rooted at the current directory.
+#
+function do_extract {
+
+    # This sets $LATEST_TARBALL as a side effect
+    get_tarball
+
+    this_dir=$PWD
+    # if we specified a directory relative to the top of the image, make that
+    # and cd into it so the agent appears in the correct location.
+    if [[ -n "$root_dir" ]]; then
+        mkdir -p $root_dir
+        cd $root_dir
+    fi
+    echo "Extracting agent $LATEST_TARBALL to $PWD"
+    case $LATEST_TARBALL in \
+        *bz2) bunzip2 -c $LATEST_TARBALL | gtar xf - ;
+            ;;
+        *gz) gunzip -c $LATEST_TARBALL | gtar xf - ;
+            ;;
+        *)
+            echo "ERROR: unknown extension trying to extract $LATEST_TARBALL"
+            exit 1
+    esac
+    cd $this_dir
+}
+
+#
+# Download this agent from the agent_url directory, according to the
+# tarball_pattern supplied
+#
+function do_download {
+    file_pattern=$(basename $tarball_pattern)
+    set +o errexit
+    http=$(echo $agent_url | grep ^http)
+    set -o errexit
+
+    if [[ -n "$http" ]]; then
+        # look for an agent_branch url
+        agent_file=$(curl -sS --fail --connect-timeout 30 $agent_url |
+            grep 'href=' | cut -d'"' -f2 | grep "^${file_pattern}$" |
+            grep $agent_branch | tail -1)
+        if [[ -z "$agent_file" ]]; then
+            # fall back to the branch url
+            agent_file=$(curl -sS --fail --connect-timeout 30 $agent_url |
+                grep 'href=' | cut -d'"' -f2 | grep "^${file_pattern}$" |
+                grep $branch | tail -1)
+        fi
+        if [[ -z "$agent_file" ]]; then
+            echo "Unable to determine url for (agent_branch) $agent_branch \
+                or (branch) $branch $file_pattern at $agent_url"
+            exit 1
+        fi
+        echo "Downloading $agent_url/$agent_file"
+        curl -sS --connect-timeout 30 -o \
+            $agent_cache_dir/$agent_file $agent_url/$agent_file
+    else
+        # copy it, assuming $agent_url and $agent_cache_dir aren't identical.
+        # We don't have 'realpath' on all build systems, so make do with Python.
+        realpath_agent_url=$(python -c
+            "import os; print os.path.realpath('$agent_url')")
+        realpath_agent_cache_dir=$(python -c
+            "import os; print os.path.realpath('$agent_cache_dir')")
+        if [[ "$realpath_agent_url" == "$realpath_agent_cache_dir" ]]; then
+            echo "Identical paths for $agent_url and $agent_cache_dir. Skipping"
+            return
+        fi
+        file_pattern=$(basename $tarball_pattern)
+        # look for a agent_branch file
+        latest_tarball_file=$(
+            /usr/bin/ls -1 $agent_url | grep $file_pattern \
+            2>/dev/null | grep $agent_branch | sort | tail -1)
+        if [[ -z "$latest_tarball_file" ]]; then
+            # fall back to the branch file
+            latest_tarball_file=$(
+                /usr/bin/ls -1 $agent_url | grep $file_pattern \
+                2>/dev/null | grep $branch | sort | tail -1)
+        fi
+        if [[ -z "$latest_tarball_file" ]]; then
+            echo "Unable to find local file for (agent_branch) $agent_branch \
+                or (branch) $branch $agent_pattern at $agent_url"
+            exit 1
+        fi
+        latest_tarball_file=$agent_url/$latest_tarball_file
+        echo "Copying local $latest_tarball_file to $agent_cache_dir"
+        cp $latest_tarball_file $agent_cache_dir
+    fi
+}
+
+function cleanup {
+    if [[ -d $agent_cache_dir/$repo_name.lock ]]; then
+        rmdir $agent_cache_dir/$repo_name.lock
+    fi
+}
+
+#
+# Main
+#
+while getopts "B:b:c:d:hp:r:t:u:U:" opt; do
+    case "${opt}" in
+        b)
+            branch=$OPTARG
+            ;;
+        B)
+            agent_branch=$OPTARG
+            ;;
+        c)
+            agent_cache_dir=$OPTARG
+            mkdir -p $agent_cache_dir
+            ;;
+        d)
+            # it's ok for this to be empty, indicating the agent tarball
+            # delivers directories right up to the root of the image. Sanity
+            # check the variable, just to be on the safe side.
+            root_dir=$OPTARG
+            if [[ -n "$root_dir" ]]; then
+                case "$root_dir" in
+                    .* | /*)
+                        echo "Error: -d option should not start with . or /"
+                        exit 1
+                        ;;
+                esac
+            fi
+            ;;
+        h)
+            do_usage=true
+            ;;
+        p)
+            tarball_pattern="$OPTARG"
+            ;;
+        r)
+            repo_name=$OPTARG
+            ;;
+        t)
+            agent_targets="$OPTARG"
+            ;;
+        U)
+            # optional
+            agent_url=$OPTARG
+            ;;
+        u)
+            git_url=$OPTARG
+            ;;
+        *)
+            echo "Error: unknown option ${opt}"
+            usage
+    esac
+done
+shift $((OPTIND - 1))
+
+command=$1
+
+if [[ -z "$branch" ]]; then
+    branch=master
+fi
+
+if [[ -z "$agent_branch" ]]; then
+    agent_branch=$branch
+fi
+
+if [[ -z "$agent_cache_dir" ]]; then
+    echo "-c agent_cache_dir option must be supplied"
+    usage
+fi
+
+if [[ -z "$tarball_pattern" ]]; then
+    echo "-p tarball_pattern option must be supplied"
+    usage
+fi
+
+if [[ -z "$repo_name" ]]; then
+    echo "-r repo_name option must be supplied"
+    usage
+fi
+
+if [[ -z "$agent_targets" && "$command" == "build" ]]; then
+    echo "-t agent_targets option must be supplied"
+    usage
+fi
+
+if [[ -z "$git_url" && "$command" == "clone" ]]; then
+    echo "-u git_url option must be supplied"
+    usage
+fi
+
+if [[ -n "${do_usage}" ]]; then
+    usage
+fi
+
+if [[ -z "$1" ]]; then
+    echo "No command supplied"
+    usage
+fi
+
+# attempt to prevent two agent-prebuilt.sh scripts from operating on the
+# same repo at the same time. $agent_cache_dir exists at this point.
+unlocked="true"
+count=0
+set +o errexit
+while [[ -n "$unlocked" && $count -lt 600 ]]; do
+    if ! mkdir $agent_cache_dir/$repo_name.lock 2> /dev/null; then
+        echo "$agent_cache_dir/$repo_name.lock already held, sleeping $count..."
+        unlocked="locked"
+        count=$(( $count + 1 ))
+        sleep 1
+    else
+        unlocked=""
+    fi
+done
+set -o errexit
+
+if [[ -n "$unlocked" ]]; then
+    echo "Failed to unlock agent cache for $repo_name. Exiting now."
+    exit 1
+fi
+
+case "$command" in
+    clone)
+        do_clone
+        ;;
+    build)
+        do_build
+        ;;
+    clean)
+        do_clean
+        ;;
+    extract)
+        do_extract
+        ;;
+    show_tarball)
+        get_tarball
+        echo $LATEST_TARBALL
+        ;;
+    download)
+        do_download
+        ;;
+    *)
+        echo "Unrecognised command $1"
+        usage
+        ;;
+esac
+ret=$?
+cleanup
+exit $ret
diff --git a/tools/bashstyle b/tools/bashstyle
new file mode 100644
index 0000000..3ea6622
--- /dev/null
+++ b/tools/bashstyle
@@ -0,0 +1,175 @@
+#!/usr/bin/env node
+/*
+ * This Source Code Form is subject to the terms of the Mozilla Public
+ * License, v. 2.0. If a copy of the MPL was not distributed with this
+ * file, You can obtain one at http://mozilla.org/MPL/2.0/.
+ */
+
+/*
+ * Copyright (c) 2014, Joyent, Inc.
+ */
+
+/*
+ * bashstyle: check bash scripts for adherence to style guidelines, including:
+ *
+ *    o no lines longer than 80 characters
+ *    o file does not end with a blank line
+ *    o Do not use 'local' and var initialization *using a subshell* in the
+ *      same statement. See
+ *      <http://www.tldp.org/LDP/abs/html/localvar.html#EXITVALANOMALY01>
+ *      for why not. Arguably this belongs in a separate 'bashlint'.
+ *
+ * Future enhancements could include:
+ *    o indents consistent with respect to tabs, spaces
+ *    o indents consistently sized (all are some multiple of the smallest
+ *      indent, which must be a tab or 4 or 8 spaces)
+ */
+
+var VERSION = '2.0.0';
+
+var mod_assert = require('assert');
+var mod_fs = require('fs');
+
+var nerrors = 0;
+
+main();
+process.exit(0);
+
+function main()
+{
+	var files = process.argv.slice(2);
+
+	if (files.length === 0) {
+		console.error('usage: %s file1 [...]',
+		    process.argv.slice(0, 2).join(' '));
+		process.exit(2);
+	}
+
+	files.forEach(checkFile);
+
+	if (nerrors != 0)
+		process.exit(1);
+}
+
+function checkFile(filename)
+{
+	var text = mod_fs.readFileSync(filename, 'utf-8');
+	var lines = text.split('\n');
+	var i;
+	var styled = false;
+	var styleStart;
+
+	mod_assert.ok(lines.length > 0);
+
+	/*
+	 * Expand tabs in each line and check for long lines.
+	 */
+	for (i = 1; i <= lines.length; i++) {
+		var line = expandTabs(lines[i - 1]);
+
+		if (i > 1 && lines[i-2].match(/# BASHSTYLED/)) {
+			continue;
+		}
+
+		if (line.match(/# BEGIN BASHSTYLED/)) {
+			styleStart = i;
+			styled = true;
+		}
+
+		if (line.match(/# END BASHSTYLED/)) {
+			if (styled != true) {
+				nerrors++;
+				console.log('%s: %d: END BASHSTYLED ' +
+				    'w/o corresponding BEGIN', filename, i);
+			}
+			styled = false;
+		}
+
+		/*JSSTYLED*/
+		if (!styled && line.match(/^\s*local\s+(\w+)\s*=.*\$\(/)) {
+			nerrors++;
+			/*JSSTYLED*/
+			var m = line.match(/^\s*local\s+(\w+)\s*=/);
+			console.log('%s: %d: declaring and setting a "local" ' +
+				'var in the same statement ' +
+				'ignores a subshell return code ' +
+				'<http://www.tldp.org/LDP/abs/html/' +
+				'localvar.html#EXITVALANOMALY01>: ' +
+				'local %s=...',
+				filename, i, m[1]);
+		}
+
+		// Regexplanation: non-[, [, space (contents) space, ], non-]
+		// groups before and after brackets to ease search/replace.
+		if (!styled && line.match(/(^|[^\[])\[(\s.+\s)\]([^\]])/)) {
+			nerrors++;
+			console.log('%s: %d: prefer [[ to [ for tests.',
+			    filename, i);
+		}
+
+		if (!styled && line.length > 80) {
+			nerrors++;
+			console.log('%s: %d: line exceeds 80 columns',
+			    filename, i);
+		}
+
+		if (!styled && line.match(/\s+$/)) {
+			nerrors++;
+			console.log('%s: %d: line ends in whitespace',
+			    filename, i);
+		}
+	}
+
+	if (styled) {
+		nerrors++;
+		console.log('%s: %d: BEGIN BASHSTYLED that does not END',
+		            filename, styleStart);
+	}
+
+
+	/*
+	 * No sane editor lets you save a file without a newline at the
+	 * very end.
+	 */
+	if (lines[lines.length - 1].length !== 0) {
+		nerrors++;
+		console.log('%s: %d: file does not end with newline',
+			filename, lines.length);
+	}
+
+	/*
+	 * Since the file will always end with a newline, the last entry of
+	 * "lines" will actually be blank.
+	 */
+	if (lines.length > 1 && lines[lines.length - 2].length === 0) {
+		nerrors++;
+		console.log('%s: %d: file ends with a blank line',
+		    filename, lines.length - 1);
+	}
+}
+
+function expandTabs(text)
+{
+	var out = '';
+	var col = 0;
+	var j, k;
+
+	for (j = 0; j < text.length; j++) {
+		if (text[j] != '\t') {
+			out += text[j];
+			col++;
+			continue;
+		}
+
+		k = 8 - (col % 8);
+		col += k;
+
+		do {
+			out += ' ';
+		} while (--k > 0);
+
+		col += k;
+	}
+
+	return (out);
+}
diff --git a/tools/bits-upload.sh b/tools/bits-upload.sh
new file mode 100644
index 0000000..135591b
--- /dev/null
+++ b/tools/bits-upload.sh
@@ -0,0 +1,494 @@
+#!/bin/bash
+#
+# This Source Code Form is subject to the terms of the Mozilla Public
+# License, v. 2.0. If a copy of the MPL was not distributed with this
+# file, You can obtain one at http://mozilla.org/MPL/2.0/.
+#
+
+#
+# Copyright (c) 2019, Joyent, Inc.
+#
+
+#
+# Upload the given <bits_dir> directory to Manta or a "local" directory (likely
+# an NFS-mounted share if we're contributing to bits already stored there
+# by other builds on remote systems)
+# This creates a specific directory structure, consumed by the headnode builds:
+#
+# <remote>/<component>/latest-release -> <latest>
+# <remote>/component>/<component-branch-stamp>/<component>/... (files)
+#
+# which is documented at:
+# https://github.com/joyent/mountain-gorilla/blob/master/docs/index.md
+# (see "Bits directory structure")
+#
+
+#
+# It is unlikely that users will ever need to run this script by hand.
+# Users are more likely to run this as part of the 'bits-upload' or
+# 'bits-upload-latest' targets.
+#
+
+if [[ -n "$TRACE" ]]; then
+    export PS4='${BASH_SOURCE}:${LINENO}: '
+    set -o xtrace
+fi
+set -o errexit
+
+#
+# Uncomment the below to have manta-tools emit bunyan logs to stdout
+#
+# MANTA_VERBOSE=-v
+
+#
+# Whether we should overwrite previous uploads if the content is the same
+#
+BITS_UPLOAD_OVERWRITE=false
+
+#
+# Whether we should allow upload of bits marked with a '-dirty' $STAMP
+#
+BITS_UPLOAD_ALLOW_DIRTY=$ENGBLD_BITS_UPLOAD_ALLOW_DIRTY
+
+#
+# A path to our updates-imgadm command
+#
+UPDATES_IMGADM=/root/opt/imgapi-cli/bin/updates-imgadm
+
+PATH=$PATH:/root/opt/node_modules/manta/bin:/opt/tools/bin
+
+function fatal {
+    echo "$(basename $0): error: $1"
+    exit 1
+}
+
+function errexit {
+    [[ $1 -ne 0 ]] || exit 0
+    fatal "error exit status $1 at line $2"
+}
+
+trap 'errexit $? $LINENO' EXIT
+
+function usage {
+    echo "Usage: bits-upload.sh [options] [subdirs...]"
+    echo "OPTIONS"
+    echo "  -b <branch>         the branch use"
+    echo "  -B <try_branch>     the try_branch use"
+    echo "  -d <upload_base_dir> destination path name in manta or a local path"
+    echo "  -D <bits_dir>       local dir with the bits to upload (default: ./bits) "
+    echo "  -L                  indicate the -d arg is a local path"
+    echo "  -n <name>           the name of component to upload"
+    echo "  -p                  also publish images to updates.joyent.com"
+    echo "  -t <timestamp>      the timestamp (optional, derived otherwise)"
+    echo ""
+    echo "Upload bits to Manta or a local destination from <bits_dir>"
+    echo ""
+    echo "The upload_base_dir is presumed to be either a subdir of"
+    echo "\${MANTA_USER}/stor or if it starts with '/', a path under"
+    echo "\${MANTA_USER}. If Using -L, the -d argument should be an"
+    echo "absolute path."
+    exit 2
+}
+
+#
+# Maintain a global associative array mapping uploaded file basenames
+# to their corresponding Manta paths. This assumes basenames are unique.
+#
+declare -A STORED_MANTA_PATHS
+
+#
+# A simple wrapper to emit manta command-lines before running them.
+#
+function manta_run {
+    echo $@
+    "$@"
+    return $?
+}
+
+#
+# Upload build artifacts to Manta. There's some duplication in the logic
+# here and local_upload.
+#
+function manta_upload {
+
+    if [[ -z "$MANTA_KEY_ID" ]]; then
+        export MANTA_KEY_ID=$(
+            ssh-keygen -E md5 -l -f ~/.ssh/id_rsa.pub | \
+            awk '{sub("^MD5:", "", $2); print $2}')
+    fi
+    if [[ -z "$MANTA_URL" ]]; then
+        export MANTA_URL=https://us-east.manta.joyent.com
+    fi
+    if [[ -z "$MANTA_USER" ]]; then
+        export MANTA_USER="Joyent_Dev";
+    fi
+
+    if [[ ${UPLOAD_BASE_DIR:0:1} != '/' ]]; then
+        # if it starts with a / we assume it's /stor/<something> or
+        # /public/<something> if not, we prepend /stor
+        UPLOAD_BASE_DIR="/stor/${UPLOAD_BASE_DIR}"
+    fi
+
+    MANTA_DESTDIR=/${MANTA_USER}${UPLOAD_BASE_DIR}/${UPLOAD_SUBDIR}
+    echo "Uploading bits to ${MANTA_DESTDIR} "
+    if [[ -z "$SUBS" ]]; then
+        manta_run mmkdir ${MANTA_VERBOSE} -p ${MANTA_DESTDIR}
+    fi
+
+    for sub in $SUBS; do
+        manta_run mmkdir ${MANTA_VERBOSE} -p ${MANTA_DESTDIR}/${sub#${BITS_DIR}}
+    done
+
+    md5sums=""
+    # now we can upload the files
+    for file in $FILES; do
+        manta_object=${MANTA_DESTDIR}/${file#$BITS_DIR}
+        # md5sum comes from the coreutils package
+        local_md5_line=$(md5sum ${file})
+        local_md5=$(echo "${local_md5_line}" | cut -d ' ' -f1)
+        manta_md5=$(mmd5 ${manta_object} | cut -d ' ' -f1)
+
+        if [[ -n ${manta_md5} && ${manta_md5} != ${local_md5} ]]; then
+            fatal "${manta_object} exists but MD5 does not match ${file}"
+        fi
+
+        if [[ -z ${manta_md5} ]]; then
+            # file doesn't exist, upload it
+            manta_run mput ${MANTA_VERBOSE} -f ${file} ${manta_object}
+            [[ $? == 0 ]] || fatal "Failed to upload ${file} to ${manta_object}"
+        elif [[ "$BITS_UPLOAD_OVERWRITE" == "false" ]]; then
+            echo "${manta_object} already exists and matches local file,"
+            echo "Skipping upload."
+        fi
+        md5sums="${md5sums}${local_md5_line}\n"
+
+        # save the file to our global assoc-array of {filename: manta path}
+        # used later when mapping manifests to image file URLs.
+        STORED_MANTA_PATHS[$(basename $file)]=${manta_object}
+
+        # Store a full URL if it appears to be a public resource, otherwise
+        # just save the manta path.
+        set +o errexit
+        echo $manta_object | grep -q /public/
+        if [[ $? -eq 0 ]]; then
+            echo ${MANTA_URL}${manta_object} >> ${BITS_DIR}/artifacts.txt
+        else
+            echo $manta_object >> ${BITS_DIR}/artifacts.txt
+        fi
+        set -o errexit
+    done
+
+    # upload the md5sums
+    echo -e $md5sums | \
+        manta_run mput ${MANTA_VERBOSE} -H "content-type: text/plain" \
+            ${MANTA_DESTDIR}/md5sums.txt
+        echo ${MANTA_DESTDIR}/md5sums.txt >> ${BITS_DIR}/artifacts.txt
+
+    # now update the branch latest link
+    echo "${MANTA_DESTDIR}" | \
+        manta_run mput ${MANTA_VERBOSE} -H "content-type: text/plain" \
+            /${MANTA_USER}${UPLOAD_BASE_DIR}/${UPLOAD_BRANCH}-latest
+    echo /${MANTA_USER}${UPLOAD_BASE_DIR}/${UPLOAD_BRANCH}-latest >> \
+        ${BITS_DIR}/artifacts.txt
+
+    # If this is a bi-weekly release branch, also update latest-release link
+    if [[ $UPLOAD_BRANCH =~ ^release- ]]; then
+        echo "${MANTA_DESTDIR}" | \
+            manta_run mput ${MANTA_VERBOSE} -H "content-type: text/plain" \
+                /${MANTA_USER}${UPLOAD_BASE_DIR}/latest-release
+        echo /${MANTA_USER}${UPLOAD_BASE_DIR}/latest-release >> \
+            ${BITS_DIR}/artifacts.txt
+    fi
+
+    echo "Uploaded to ${MANTA_DESTDIR}"
+}
+
+#
+# Copy build artifacts to a local or NFS-mounted filesystem.
+# There's some duplication in the logic here and manta_upload. Note that
+# the <branch>-latest object created is now a symlink rather than an
+# object containing the latest path.
+#
+function local_upload {
+
+    LOCAL_DESTDIR=${UPLOAD_BASE_DIR}/${UPLOAD_SUBDIR}
+    for sub in $SUBS; do
+        mkdir -p ${LOCAL_DESTDIR}/${sub#${BITS_DIR}}
+    done
+
+    md5sums=""
+    for file in $FILES; do
+        remote_object=${LOCAL_DESTDIR}/${file#$BITS_DIR}
+
+        local_md5_line=$(md5sum ${file})
+        local_md5=$(echo "${local_md5_line}" | cut -d ' ' -f1)
+        if [[ -f ${remote_object} ]]; then
+            remote_md5=$(md5sum ${remote_object} | cut -d ' ' -f1)
+        else
+            remote_md5=""
+        fi
+
+        if [[ -n ${remote_md5} && ${remote_md5} != ${local_md5} ]]; then
+            fatal "${remote_object} exists but MD5 does not match ${file}"
+        fi
+
+        if [[ -z ${remote_md5} ]]; then
+            # file doesn't exist, upload it
+            cp ${file} ${remote_object}
+            [[ $? == 0 ]] || \
+                fatal "Failed to upload ${file} to ${remote_object}"
+        else
+            echo "${remote_object} already exists and matches local file,"
+            echo "skipping upload."
+        fi
+        md5sums="${md5sums}${local_md5_line}\n"
+        echo $remote_object >> ${BITS_DIR}/artifacts.txt
+    done
+
+    # upload the md5sums
+    echo -e $md5sums > ${LOCAL_DESTDIR}/md5sums.txt
+
+    # now update the branch latest link
+    if [[ -L ${UPLOAD_BASE_DIR}/${UPLOAD_BRANCH}-latest ]]; then
+        unlink ${UPLOAD_BASE_DIR}/${UPLOAD_BRANCH}-latest
+    fi
+    (cd $UPLOAD_BASE_DIR ; ln -s ${UPLOAD_SUBDIR} ${UPLOAD_BRANCH}-latest)
+
+    # If this is a bi-weekly release branch, also update latest-release link
+    if [[ $UPLOAD_BRANCH =~ ^release- ]]; then
+        if [[ -L ${UPLOAD_BASE_DIR}/latest-release ]]; then
+            unlink ${UPLOAD_BASE_DIR}/latest-release
+        fi
+        (cd ${UPLOAD_BASE_DIR} ; ln -s ${UPLOAD_SUBDIR} latest-release)
+    fi
+}
+
+#
+# Look for build artifacts to operate on.
+#
+function find_upload_bits {
+    if [[ -z "$SUBDIRS" ]]; then
+        SUBS=$(find $BITS_DIR -type d)
+        FILES=$(find $BITS_DIR -type f)
+    else
+        for subdir in ${SUBDIRS}; do
+            if [[ -d $BITS_DIR/$subdir ]]; then
+                SUBS="$SUBS $(find $BITS_DIR/$subdir -type d)"
+                FILES="$FILES $(find $BITS_DIR/$subdir -type f)"
+            fi
+        done
+    fi
+}
+
+#
+# Publish build artifacts to updates.joyent.com.
+#
+function publish_to_updates {
+    local manta_path
+    local msigned_url
+    local compression_flag
+
+    echo "Publishing updates to updates.joyent.com"
+    for file in ${FILES}; do
+        set +o errexit
+        echo ${file} | grep -q '.*manifest$'
+        if [[ $? -ne 0 ]]; then
+            set -o errexit
+            continue
+        fi
+        set -o errexit
+
+        MF=${file}
+        IMAGEFILE=$(echo ${MF} | sed -e 's/\..*manifest$/.zfs.gz/g')
+        compression_flag=""
+
+        # Some payloads are not zfs-based, look for likely alternatives.
+        # This assumes that a single directory with <file>.manifest
+        # contains only one of [<file>.zfs.gz, <file>.sh, <file>.tgz,
+	# <file>.tar.gz]
+        if [[ ! -f "${IMAGEFILE}" ]]; then
+            IMAGEFILE=$(echo ${MF} | sed -e 's/\..*manifest$/.sh/g')
+            compression_flag="--compression=none"
+        fi
+        if [[ ! -f "${IMAGEFILE}" ]]; then
+            IMAGEFILE=$(echo ${MF} | sed -e 's/\..*manifest$/.tgz/g')
+            compression_flag=""
+        fi
+        if [[ ! -f "${IMAGEFILE}" ]]; then
+            IMAGEFILE=$(echo ${MF} | sed -e 's/\..*manifest$/.tar.gz/g')
+            compression_flag=""
+        fi
+
+        if [[ ! -f ${IMAGEFILE} ]]; then
+            echo "Unable to determine image file for ${MF}."
+            echo "Skipping publishing ${MF} to updates.joyent.com"
+            continue
+        fi
+
+        UUID=$(json -f ${MF} uuid)
+        if [[ -z "${UUID}" ]]; then
+            echo "Unable to determine UUID of ${MF}."
+            echo "Skipping publishing ${MF} to updates.joyent.com"
+            continue
+        fi
+
+        # The default 1hr expiry for msign is sufficient, since we're going
+        # to be accessing this URL almost immediately.
+        manta_path=${STORED_MANTA_PATHS[$(basename $IMAGEFILE)]}
+        msigned_url=$(msign $manta_path)
+
+        # Compute values for channel, user and identity
+        if [[ -z "$UPDATES_IMGADM_CHANNEL" ]]; then
+
+            if [[ ! -z "$TRY_BRANCH" ]]; then
+                BRANCH_NAME=${TRY_BRANCH}
+            else
+                BRANCH_NAME=${BRANCH}
+            fi
+            if [[ -z "$TRY_BRANCH" && "$(echo ${BRANCH} \
+                    | grep '^release-[0-9]\{8\}$' || true)" ]]; then
+                export UPDATES_IMGADM_CHANNEL=staging
+            else
+                if [[ "${BRANCH_NAME}" == "master" ]]; then
+                    export UPDATES_IMGADM_CHANNEL=dev
+                else
+                    export UPDATES_IMGADM_CHANNEL=experimental
+                fi
+            fi
+        fi
+
+        if [[ -z "$UPDATES_IMGADM_USER" ]]; then
+            export UPDATES_IMGADM_USER=mg
+        fi
+
+        if [[ -z "$UPDATES_IMGADM_IDENTITY" ]]; then
+            export UPDATES_IMGADM_IDENTITY=~/.ssh/automation.id_rsa
+        fi
+        echo "Using the following environment variables for updates-imgadm:"
+        echo "UPDATES_IMGADM_CHANNEL=$UPDATES_IMGADM_CHANNEL"
+        echo "UPDATES_IMGADM_USER=$UPDATES_IMGADM_USER"
+        echo "UPDATES_IMGADM_IDENTITY=$UPDATES_IMGADM_IDENTITY"
+        echo Running: ${UPDATES_IMGADM} import -m ${MF} -f "${msigned_url}" ${compression_flag}
+        ${UPDATES_IMGADM} import -m ${MF} -f "${msigned_url}" ${compression_flag}
+    done
+}
+
+
+#
+# Main
+#
+BITS_DIR="bits"
+while getopts "B:b:d:D:Ln:pt:" opt; do
+    case "${opt}" in
+        b)
+            BRANCH=$OPTARG
+            ;;
+        B)
+            TRY_BRANCH=$OPTARG
+            ;;
+        d)
+            UPLOAD_BASE_DIR=$OPTARG
+            ;;
+        D)
+            BITS_DIR=$OPTARG
+            ;;
+        L)
+            USE_LOCAL=true
+            ;;
+        n)
+            NAME=$OPTARG
+            ;;
+        p)
+            PUBLISH_UPDATES=true
+            ;;
+        t)
+            TIMESTAMP=$OPTARG
+            ;;
+        *)
+            echo "Error: Unknown argument ${opt}"
+            usage
+    esac
+done
+shift $((OPTIND - 1))
+
+if [[ -z "${BRANCH}" ]]; then
+    echo "Missing -b argument for branch"
+    usage
+fi
+
+if [[ -z "$UPLOAD_BASE_DIR" ]]; then
+    echo "Missing -d argument for upload_base_dir"
+    usage
+fi
+
+if [[ -z "$NAME" ]]; then
+    fatal "Missing -d argument for name"
+fi
+
+SUBDIRS=$*
+
+UPLOAD_BRANCH=$TRY_BRANCH
+if [[ -z "$UPLOAD_BRANCH" ]]; then
+    UPLOAD_BRANCH=$BRANCH
+fi
+
+start_time=$(date +%s)
+
+# we keep a file containing a list of uploads for this
+# session, useful to include as part of build artifacts.
+if [[ -f $BITS_DIR/artifacts.txt ]]; then
+    rm -f $BITS_DIR/artifacts.txt
+fi
+
+find_upload_bits
+
+if [[ -z "$TIMESTAMP" ]]; then
+    LATEST_BUILD_STAMP=$BITS_DIR/$NAME/latest-build-stamp
+    # Pull the latest timestamp from the bits dir instead.
+    if [[ -f $LATEST_BUILD_STAMP ]]; then
+        TIMESTAMP=$(cat $LATEST_BUILD_STAMP)
+    else
+        echo "Missing timestamp, and no contents in $LATEST_BUILD_STAMP"
+        echo "Did the 'prepublish' Makefile target run?"
+        fatal "Unable to derive latest timestamp from files in $BITS_DIR"
+    fi
+fi
+
+#
+# Attempting to upload bits from a workspace marked dirty isn't
+# allowed, because then we have no sure way to get from the development
+# bits to the corresponding source commit. Arguably, this _might_ be
+# feasible for pure-javascript-only components. Developers who absolutely
+# must upload bits marked '-dirty' can always upload them manually, or
+# use the BITS_UPLOAD_ALLOW_DIRTY escape hatch in case of emergency.
+#
+set +o errexit
+echo $TIMESTAMP | grep -q '\-dirty$'
+if [[ $? -eq 0 && -z "$BITS_UPLOAD_ALLOW_DIRTY" ]]; then
+    fatal "Bits timestamp $TIMESTAMP marked 'dirty': not uploading"
+fi
+set -o errexit
+
+UPLOAD_SUBDIR=$TIMESTAMP
+
+if [[ -n "$USE_LOCAL" ]]; then
+    if [[ -n "$PUBLISH_TO_UPDATES" ]]; then
+        fatal "-p requires uploading to Manta, and is incompatible with -L"
+    fi
+    local_upload
+else
+    manta_upload
+fi
+
+if [[ -n "$PUBLISH_UPDATES" ]]; then
+    publish_to_updates
+fi
+
+end_time=$(date +%s)
+elapsed=$((${end_time} - ${start_time}))
+if [[ -n "$USE_LOCAL" ]]; then
+    desc="(path=${UPLOAD_BASE_DIR}/${UPLOAD_SUBDIR})"
+else
+    desc="(Manta path=/${MANTA_USER}${UPLOAD_BASE_DIR}/${UPLOAD_SUBDIR})."
+fi
+echo "Upload took ${elapsed} seconds $desc"
diff --git a/tools/buildimage-extract-pkg.sh b/tools/buildimage-extract-pkg.sh
new file mode 100644
index 0000000..c71cfa7
--- /dev/null
+++ b/tools/buildimage-extract-pkg.sh
@@ -0,0 +1,35 @@
+#!/bin/bash
+#
+# This Source Code Form is subject to the terms of the Mozilla Public
+# License, v. 2.0. If a copy of the MPL was not distributed with this
+# file, You can obtain one at http://mozilla.org/MPL/2.0/.
+#
+
+#
+# Copyright (c) 2019, Joyent, Inc.
+#
+
+#
+# This script simply extracts the .tar.gz or .tar.bz package given as $1 to the
+# current directory.
+#
+
+ARCHIVE=$1
+
+# Uncomment this for debugging.
+# VERBOSE=v
+
+case $ARCHIVE in
+    *tar.bz2|*.tar.bz)
+        TAR_ARGS="jx${VERBOSE}f"
+        ;;
+    *.tar.gz|*.tgz)
+        TAR_ARGS="zx${VERBOSE}f"
+        ;;
+    *)
+        echo "Error: buildimage-extract-pkg: Unknown archive file $ARCHIVE"
+        exit 1
+esac
+
+gtar ${TAR_ARGS} $ARCHIVE
+exit $?
diff --git a/tools/download_ctftools b/tools/download_ctftools
new file mode 100644
index 0000000..3f6b633
--- /dev/null
+++ b/tools/download_ctftools
@@ -0,0 +1,90 @@
+#!/bin/bash
+#
+# This Source Code Form is subject to the terms of the Mozilla Public
+# License, v. 2.0. If a copy of the MPL was not distributed with this
+# file, You can obtain one at http://mozilla.org/MPL/2.0/.
+#
+
+#
+# Copyright (c) 2018, Joyent, Inc.
+#
+
+#
+# This program will download a prebuilt copy of the CTF tools from Manta.
+# These tools have been built using a technique to ensure they can be run on a
+# sufficiently old platform.  More details are available at:
+#
+#	https://github.com/jclulow/ctftools
+#
+
+MANTA_URL='https://us-east.manta.joyent.com'
+BASE_PATH='/Joyent_Dev/public/ctftools'
+TARBALL='ctftools.20141030T081701Z.9543159.tar.gz'
+
+if [[ -z $1 ]]; then
+	printf 'ERROR: usage: download_ctftools <destination dir>\n' 2>&1
+	exit 1
+fi
+
+TOP=$(cd "$(dirname "$0")/.." && pwd)
+CACHE_DIR=$1
+TMPDIR="$TOP/$CACHE_DIR"
+CTFDIR="$TOP/$CACHE_DIR/ctftools"
+
+if ! /usr/bin/mkdir -p "$TMPDIR"; then
+	printf 'ERROR: could not mkdir "%s"\n' "$TMPDIR" >&2
+	exit 1
+fi
+
+if [[ ! -f "$TMPDIR/$TARBALL" ]]; then
+	#
+	# Try to download the tarball from Manta.
+	#
+	start_time=$SECONDS
+	rm -f "$TMPDIR/$TARBALL.tmp"
+	while :; do
+		if (( $SECONDS > start_time + 120 )); then
+			printf 'ERROR: timed out trying to download tools\n' >&2
+			exit 1
+		fi
+
+		if ! curl -sSf -o "$TMPDIR/$TARBALL.tmp" \
+		    "$MANTA_URL/$BASE_PATH/$TARBALL"; then
+			printf 'WARNING: download failure (retrying)\n' >&2
+			sleep 5
+			continue
+		fi
+
+		if ! /usr/bin/gzip -t "$TMPDIR/$TARBALL.tmp"; then
+			printf 'WARNING: download gzip invalid (retrying)\n' >&2
+			sleep 5
+			continue
+		fi
+
+		if ! /usr/bin/mv "$TMPDIR/$TARBALL.tmp" "$TMPDIR/$TARBALL"; then
+			printf 'ERROR: could not move tarball into place\n' >&2
+			exit 1
+		fi
+
+		break
+	done
+fi
+
+#
+# Extract the tarball
+#
+if ! /usr/bin/mkdir -p "$CTFDIR"; then
+	printf 'ERROR: could not mkdir "%s"\n' "$CTFDIR" >&2
+	exit 1
+fi
+
+if ! cd "$CTFDIR"; then
+	exit 1
+fi
+
+if ! /usr/bin/tar xfz "$TMPDIR/$TARBALL"; then
+	printf 'ERROR: could not extract tarball\n' >&2
+	exit 1
+fi
+
+exit 0
diff --git a/tools/download_go b/tools/download_go
new file mode 100644
index 0000000..69e9a3e
--- /dev/null
+++ b/tools/download_go
@@ -0,0 +1,126 @@
+#!/bin/bash
+#
+# This Source Code Form is subject to the terms of the Mozilla Public
+# License, v. 2.0. If a copy of the MPL was not distributed with this
+# file, You can obtain one at http://mozilla.org/MPL/2.0/.
+#
+
+#
+# Copyright (c) 2018, Joyent, Inc.
+#
+
+#
+# This program will download a Go toolchain for a particular system.  It
+# currently expects to find illumos builds of the toolchain on the Joyent
+# download server, where we will be placing them alongside sdcnode, etc.
+# The program is designed specifically to be used as part of a make target.
+#
+# We may revisit this once the Go project makes official builds for illumos
+# systems available in the future.
+#
+# NOTE: This program comes from the "eng" repo. It's designed to be dropped
+# into other repos as-is without requiring any modifications. If you find
+# yourself changing this file, you should instead update the original copy in
+# eng.git and then update your repo to use the new version.
+#
+
+#
+# This program accepts four arguments, in the following order:
+#
+#	GOVERSION	The version of the Go toolchain to use; e.g., "1.9.2"
+#	GOOS		The build machine operating system; e.g., "solaris"
+#	GOARCH		The build machine CPU architecture; e.g., "amd64"
+#	OUTDIR		The local directory into which the downloaded tar
+#			file will be placed.
+#
+# The program will use the provided arguments to find and download an archive
+# of the Go toolchain for use on the build machine.  The archive will be named
+# for a combination of the provided arguments; e.g.,
+# "go1.9.2.solaris-amd64.tar.bz2".  A target symbolic link will also be
+# created, with just the version number in the name; e.g., "go-1.9.2.tar.bz2".
+# If the archive could not be downloaded, an error message will be printed and
+# the output file and target link will be unaffected.
+#
+
+BASEURL='https://download.joyent.com/pub/build/go/adhoc/'
+GOVERSION=$1
+GOOS=$2
+GOARCH=$3
+OUTDIR=$4
+
+if [[ -z $GOVERSION || -z $GOOS || -z $GOARCH || -z $OUTDIR ]]; then
+	printf 'ERROR: usage: download_go GOVERSION GOOS GOARCH OUTDIR\n' 2>&1
+	exit 1
+fi
+
+if [[ ! -d $OUTDIR ]]; then
+	printf 'ERROR: output directory "%s" does not exist\n' "$OUTDIR" 2>&1
+	exit 1
+fi
+
+TARGET="go-$GOVERSION.tar.bz2"
+
+#
+# Download the index page which lists the current set of available go
+# builds:
+#
+if ! list=$(curl -sSfL "$BASEURL") || [[ -z "$list" ]]; then
+	printf 'ERROR: could not download index page\n' >&2
+	exit 1
+fi
+
+#
+# Using only commonly found household items, extract the full name of the
+# go tar archive we need.  This program needs to be able to operate in a
+# minimally populated build zone, so we avoid using anything beyond basic
+# UNIX tools like "awk".
+#
+# One word to describe this process might be "brittle".
+#
+if ! name=$(/usr/bin/awk -v "v=$GOVERSION" -v "o=$GOOS" -v "a=$GOARCH" -F\" '
+    BEGIN { pattern = "^go"v"."o"-"a".tar.bz2$"; }
+    $1 == "<a href=" && $2 ~ pattern { print $2 }' <<< "$list") ||
+    [[ -z "$name" ]]; then
+	printf 'ERROR: could not locate file name in index page\n' >&2
+	printf '\t(Does Go version %s (%s-%s) exist?)\n' \
+	    "$GOVERSION" "$GOOS" "$GOARCH" >&2
+	exit 1
+fi
+
+
+#
+# If the full file name of the latest go build does not exist, download it now
+# to a temporary file.  If it succeeds, move it into place.
+#
+output_file="$OUTDIR/$name"
+if [[ ! -f $output_file ]]; then
+	printf 'Downloading Go: %s\n' "$BASEURL$name"
+
+	temp_file="$OUTDIR/.tmp.$name.$$"
+	rm -f "$temp_file"
+
+	if ! curl -sSf -o "$temp_file" "$BASEURL$name"; then
+		printf 'ERROR: could not download go\n' >&2
+		rm -f "$temp_file"
+		exit 1
+	fi
+
+	if ! mv "$temp_file" "$output_file"; then
+		printf 'ERROR: could not move tar file into place\n' >&2
+		rm -f "$temp_file"
+		exit 1
+	fi
+fi
+
+#
+# Make sure the target link points at the correct file:
+#
+rm -f "$OUTDIR/$TARGET"
+if ! ln -s "$name" "$OUTDIR/$TARGET"; then
+	printf 'ERROR: could not create target link\n' >&2
+	exit 1
+fi
+
+exit 0
+
+# vim: set ts=8 sts=8 sw=8 noet:
diff --git a/tools/jsl.node.conf b/tools/jsl.node.conf
new file mode 100644
index 0000000..c44e8ee
--- /dev/null
+++ b/tools/jsl.node.conf
@@ -0,0 +1,139 @@
+#
+# Configuration File for JavaScript Lint 
+#
+# This configuration file can be used to lint a collection of scripts, or to enable
+# or disable warnings for scripts that are linted via the command line.
+#
+
+### Warnings
+# Enable or disable warnings based on requirements.
+# Use "+WarningName" to display or "-WarningName" to suppress.
+#
++ambiguous_else_stmt          # the else statement could be matched with one of multiple if statements (use curly braces to indicate intent
++ambiguous_nested_stmt        # block statements containing block statements should use curly braces to resolve ambiguity
++ambiguous_newline            # unexpected end of line; it is ambiguous whether these lines are part of the same statement
++anon_no_return_value         # anonymous function does not always return value
++assign_to_function_call      # assignment to a function call
+-block_without_braces         # block statement without curly braces
++comma_separated_stmts        # multiple statements separated by commas (use semicolons?)
++comparison_type_conv         # comparisons against null, 0, true, false, or an empty string allowing implicit type conversion (use === or !==)
++default_not_at_end           # the default case is not at the end of the switch statement
++dup_option_explicit          # duplicate "option explicit" control comment
++duplicate_case_in_switch     # duplicate case in switch statement
++duplicate_formal             # duplicate formal argument {name}
++empty_statement              # empty statement or extra semicolon
++identifier_hides_another     # identifer {name} hides an identifier in a parent scope
+-inc_dec_within_stmt          # increment (++) and decrement (--) operators used as part of greater statement
++incorrect_version            # Expected /*jsl:content-type*/ control comment. The script was parsed with the wrong version.
++invalid_fallthru             # unexpected "fallthru" control comment
++invalid_pass                 # unexpected "pass" control comment
++jsl_cc_not_understood        # couldn't understand control comment using /*jsl:keyword*/ syntax
++leading_decimal_point        # leading decimal point may indicate a number or an object member
++legacy_cc_not_understood     # couldn't understand control comment using /*@keyword@*/ syntax
++meaningless_block            # meaningless block; curly braces have no impact
++mismatch_ctrl_comments       # mismatched control comment; "ignore" and "end" control comments must have a one-to-one correspondence
++misplaced_regex              # regular expressions should be preceded by a left parenthesis, assignment, colon, or comma
++missing_break                # missing break statement
++missing_break_for_last_case  # missing break statement for last case in switch
++missing_default_case         # missing default case in switch statement
++missing_option_explicit      # the "option explicit" control comment is missing
++missing_semicolon            # missing semicolon
++missing_semicolon_for_lambda # missing semicolon for lambda assignment
++multiple_plus_minus          # unknown order of operations for successive plus (e.g. x+++y) or minus (e.g. x---y) signs
++nested_comment               # nested comment
++no_return_value              # function {name} does not always return a value
++octal_number                 # leading zeros make an octal number
++parseint_missing_radix       # parseInt missing radix parameter
++partial_option_explicit      # the "option explicit" control comment, if used, must be in the first script tag
++redeclared_var               # redeclaration of {name}
++trailing_comma_in_array      # extra comma is not recommended in array initializers
++trailing_decimal_point       # trailing decimal point may indicate a number or an object member
++undeclared_identifier        # undeclared identifier: {name}
++unreachable_code             # unreachable code
+-unreferenced_argument        # argument declared but never referenced: {name}
+-unreferenced_function        # function is declared but never referenced: {name}
++unreferenced_variable        # variable is declared but never referenced: {name}
++unsupported_version          # JavaScript {version} is not supported
++use_of_label                 # use of label
++useless_assign               # useless assignment
++useless_comparison           # useless comparison; comparing identical expressions
+-useless_quotes               # the quotation marks are unnecessary
++useless_void                 # use of the void type may be unnecessary (void is always undefined)
++var_hides_arg                # variable {name} hides argument
++want_assign_or_call          # expected an assignment or function call
++with_statement               # with statement hides undeclared variables; use temporary variable instead
+
+
+### Output format
+# Customize the format of the error message.
+#    __FILE__ indicates current file path
+#    __FILENAME__ indicates current file name
+#    __LINE__ indicates current line
+#    __COL__ indicates current column
+#    __ERROR__ indicates error message (__ERROR_PREFIX__: __ERROR_MSG__)
+#    __ERROR_NAME__ indicates error name (used in configuration file)
+#    __ERROR_PREFIX__ indicates error prefix
+#    __ERROR_MSG__ indicates error message
+#
+# For machine-friendly output, the output format can be prefixed with
+# "encode:". If specified, all items will be encoded with C-slashes.
+#
+# Visual Studio syntax (default):
++output-format __FILE__(__LINE__): __ERROR__
+# Alternative syntax:
+#+output-format __FILE__:__LINE__: __ERROR__
+
+
+### Context
+# Show the in-line position of the error.
+# Use "+context" to display or "-context" to suppress.
+#
++context
+
+
+### Control Comments
+# Both JavaScript Lint and the JScript interpreter confuse each other with the syntax for
+# the /*@keyword@*/ control comments and JScript conditional comments. (The latter is
+# enabled in JScript with @cc_on@). The /*jsl:keyword*/ syntax is preferred for this reason,
+# although legacy control comments are enabled by default for backward compatibility.
+#
+-legacy_control_comments
+
+
+### Defining identifiers
+# By default, "option explicit" is enabled on a per-file basis.
+# To enable this for all files, use "+always_use_option_explicit"
+-always_use_option_explicit
+
+# Define certain identifiers of which the lint is not aware.
+# (Use this in conjunction with the "undeclared identifier" warning.)
+#
+# Common uses for webpages might be:
++define __dirname
++define clearInterval
++define clearTimeout
++define console
++define exports
++define global
++define module
++define process
++define require
++define setInterval
++define setImmediate
++define setTimeout
++define Buffer
++define JSON
++define Math
+
+### JavaScript Version
+# To change the default JavaScript version:
+#+default-type text/javascript;version=1.5
+#+default-type text/javascript;e4x=1
+
+### Files
+# Specify which files to lint
+# Use "+recurse" to enable recursion (disabled by default).
+# To add a set of files, use "+process FileName", "+process Folder\Path\*.js",
+# or "+process Folder\Path\*.htm".
+#
+
diff --git a/tools/jsl.web.conf b/tools/jsl.web.conf
new file mode 100644
index 0000000..b135000
--- /dev/null
+++ b/tools/jsl.web.conf
@@ -0,0 +1,137 @@
+#
+# Configuration File for JavaScript Lint 
+# Developed by Matthias Miller (http://www.JavaScriptLint.com)
+#
+# This configuration file can be used to lint a collection of scripts, or to enable
+# or disable warnings for scripts that are linted via the command line.
+#
+
+### Warnings
+# Enable or disable warnings based on requirements.
+# Use "+WarningName" to display or "-WarningName" to suppress.
+#
++ambiguous_else_stmt          # the else statement could be matched with one of multiple if statements (use curly braces to indicate intent
++ambiguous_nested_stmt        # block statements containing block statements should use curly braces to resolve ambiguity
++ambiguous_newline            # unexpected end of line; it is ambiguous whether these lines are part of the same statement
++anon_no_return_value         # anonymous function does not always return value
++assign_to_function_call      # assignment to a function call
+-block_without_braces         # block statement without curly braces
++comma_separated_stmts        # multiple statements separated by commas (use semicolons?)
++comparison_type_conv         # comparisons against null, 0, true, false, or an empty string allowing implicit type conversion (use === or !==)
++default_not_at_end           # the default case is not at the end of the switch statement
++dup_option_explicit          # duplicate "option explicit" control comment
++duplicate_case_in_switch     # duplicate case in switch statement
++duplicate_formal             # duplicate formal argument {name}
++empty_statement              # empty statement or extra semicolon
++identifier_hides_another     # identifer {name} hides an identifier in a parent scope
++inc_dec_within_stmt          # increment (++) and decrement (--) operators used as part of greater statement
++incorrect_version            # Expected /*jsl:content-type*/ control comment. The script was parsed with the wrong version.
++invalid_fallthru             # unexpected "fallthru" control comment
++invalid_pass                 # unexpected "pass" control comment
++jsl_cc_not_understood        # couldn't understand control comment using /*jsl:keyword*/ syntax
++leading_decimal_point        # leading decimal point may indicate a number or an object member
++legacy_cc_not_understood     # couldn't understand control comment using /*@keyword@*/ syntax
++meaningless_block            # meaningless block; curly braces have no impact
++mismatch_ctrl_comments       # mismatched control comment; "ignore" and "end" control comments must have a one-to-one correspondence
++misplaced_regex              # regular expressions should be preceded by a left parenthesis, assignment, colon, or comma
++missing_break                # missing break statement
++missing_break_for_last_case  # missing break statement for last case in switch
++missing_default_case         # missing default case in switch statement
++missing_option_explicit      # the "option explicit" control comment is missing
++missing_semicolon            # missing semicolon
++missing_semicolon_for_lambda # missing semicolon for lambda assignment
++multiple_plus_minus          # unknown order of operations for successive plus (e.g. x+++y) or minus (e.g. x---y) signs
++nested_comment               # nested comment
++no_return_value              # function {name} does not always return a value
++octal_number                 # leading zeros make an octal number
++parseint_missing_radix       # parseInt missing radix parameter
++partial_option_explicit      # the "option explicit" control comment, if used, must be in the first script tag
++redeclared_var               # redeclaration of {name}
++trailing_comma_in_array      # extra comma is not recommended in array initializers
++trailing_decimal_point       # trailing decimal point may indicate a number or an object member
++undeclared_identifier        # undeclared identifier: {name}
++unreachable_code             # unreachable code
++unreferenced_argument        # argument declared but never referenced: {name}
++unreferenced_function        # function is declared but never referenced: {name}
++unreferenced_variable        # variable is declared but never referenced: {name}
++unsupported_version          # JavaScript {version} is not supported
++use_of_label                 # use of label
++useless_assign               # useless assignment
++useless_comparison           # useless comparison; comparing identical expressions
++useless_quotes               # the quotation marks are unnecessary
++useless_void                 # use of the void type may be unnecessary (void is always undefined)
++var_hides_arg                # variable {name} hides argument
++want_assign_or_call          # expected an assignment or function call
++with_statement               # with statement hides undeclared variables; use temporary variable instead
+
+
+### Output format
+# Customize the format of the error message.
+#    __FILE__ indicates current file path
+#    __FILENAME__ indicates current file name
+#    __LINE__ indicates current line
+#    __COL__ indicates current column
+#    __ERROR__ indicates error message (__ERROR_PREFIX__: __ERROR_MSG__)
+#    __ERROR_NAME__ indicates error name (used in configuration file)
+#    __ERROR_PREFIX__ indicates error prefix
+#    __ERROR_MSG__ indicates error message
+#
+# For machine-friendly output, the output format can be prefixed with
+# "encode:". If specified, all items will be encoded with C-slashes.
+#
+# Visual Studio syntax (default):
++output-format __FILE__(__LINE__): __ERROR__
+# Alternative syntax:
+#+output-format __FILE__:__LINE__: __ERROR__
+
+
+### Context
+# Show the in-line position of the error.
+# Use "+context" to display or "-context" to suppress.
+#
++context
+
+
+### Control Comments
+# Both JavaScript Lint and the JScript interpreter confuse each other with the syntax for
+# the /*@keyword@*/ control comments and JScript conditional comments. (The latter is
+# enabled in JScript with @cc_on@). The /*jsl:keyword*/ syntax is preferred for this reason,
+# although legacy control comments are enabled by default for backward compatibility.
+#
+-legacy_control_comments
+
+
+### Defining identifiers
+# By default, "option explicit" is enabled on a per-file basis.
+# To enable this for all files, use "+always_use_option_explicit"
++always_use_option_explicit
+
+# Define certain identifiers of which the lint is not aware.
+# (Use this in conjunction with the "undeclared identifier" warning.)
+#
+# Common uses for webpages might be:
++define	JSON
++define Math
++define $
++define XMLHttpRequest
++define alert
++define clearInterval
++define clearTimeout
++define confirm
++define document
++define setInterval
++define setTimeout
++define window
+
+### JavaScript Version
+# To change the default JavaScript version:
+#+default-type text/javascript;version=1.5
+#+default-type text/javascript;e4x=1
+
+### Files
+# Specify which files to lint
+# Use "+recurse" to enable recursion (disabled by default).
+# To add a set of files, use "+process FileName", "+process Folder\Path\*.js",
+# or "+process Folder\Path\*.htm".
+#
+
diff --git a/tools/jsstyle.conf b/tools/jsstyle.conf
new file mode 100644
index 0000000..2d13d5d
--- /dev/null
+++ b/tools/jsstyle.conf
@@ -0,0 +1,14 @@
+#
+# This Source Code Form is subject to the terms of the Mozilla Public
+# License, v. 2.0. If a copy of the MPL was not distributed with this
+# file, You can obtain one at http://mozilla.org/MPL/2.0/.
+#
+
+#
+# Copyright (c) 2017, Joyent, Inc.
+#
+
+unparenthesized-return=1
+indent=tab
+line-length=80
+literal-string-quote=single
diff --git a/tools/mkrepo b/tools/mkrepo
new file mode 100644
index 0000000..c06d75a
--- /dev/null
+++ b/tools/mkrepo
@@ -0,0 +1,121 @@
+#!/usr/bin/env node
+// -*- mode: js -*-
+/*
+ * This Source Code Form is subject to the terms of the Mozilla Public
+ * License, v. 2.0. If a copy of the MPL was not distributed with this
+ * file, You can obtain one at http://mozilla.org/MPL/2.0/.
+ */
+
+/*
+ * Copyright (c) 2019, Joyent, Inc.
+ */
+
+var child_process = require('child_process');
+var fs = require('fs');
+var path = require('path');
+
+
+
+///--- Globals
+
+var DIRS = [
+    'deps',
+    'docs',
+    'docs/media',
+    'lib',
+    'smf',
+    'smf/manifests',
+    'test',
+    'tools'
+];
+
+var SUBMODULES = {
+    'eng': 'https://github.com/joyent/eng.git',
+    'javascriptlint': 'https://github.com/davepacheco/javascriptlint.git',
+    'jsstyle': 'https://github.com/davepacheco/jsstyle.git',
+    'restdown': 'https://github.com/trentm/restdown.git'
+};
+
+
+
+///--- Internal Functions
+
+function usage(code, message) {
+    if (message)
+        console.error(message);
+
+    console.error('usage: %s [repo ...]', path.basename(process.argv[1]));
+    process.exit(code);
+}
+
+
+function ensureDirectoryNotExists(dir) {
+    try {
+        var stats = fs.statSync(dir);
+        usage(1, dir + ' already exists');
+    } catch (e) {
+        return false;
+    }
+}
+
+
+function cp(src, dest) {
+    fs.createReadStream(src).pipe(fs.createWriteStream(dest));
+}
+
+
+function exec(cmd, dir, cb) {
+    child_process.exec(cmd, {cwd: dir}, function (err, stdout, stderr) {
+        if (err)
+            process.exit(err.code || 1);
+
+        if (typeof (cb) === 'function')
+            return cb(null);
+    });
+}
+
+
+function mkdir(d) {
+    fs.mkdirSync(d, '0750');
+}
+
+function gitify(dir, repo) {
+    exec('git init', dir, function () {
+        exec('git remote add origin git@github.com:joyent/' + repo + '.git',
+	    dir);
+
+        Object.keys(SUBMODULES).forEach(function (k) {
+            // stub out the git submodule call
+            console.error('Cloning into deps/' + k + '...');
+            exec('git submodule add ' + SUBMODULES[k] + ' ./deps/' + k, dir);
+        });
+    });
+}
+
+
+
+///--- Mainline
+
+if (process.argv.length < 3)
+    usage(1, 'repo required');
+
+process.argv.slice(2).forEach(function (arg) {
+    var repo = path.resolve(arg);
+    ensureDirectoryNotExists(repo);
+    mkdir(repo);
+    DIRS.concat('.').forEach(function (d) {
+        var dir = repo + '/' + d;
+        if (d != '.')
+            mkdir(dir);
+
+        fs.readdirSync('./' + d).forEach(function (f) {
+            var src = './' + d + '/' + f;
+            var dest = dir + '/' + f;
+            if (fs.statSync(src).isFile() && !/^\..*/.test(f))
+                cp(src, dest);
+        });
+    });
+
+    cp('./.gitignore', repo + '/.gitignore');
+    gitify(repo, arg);
+});
diff --git a/tools/runtests.in b/tools/runtests.in
new file mode 100644
index 0000000..2d72b6f
--- /dev/null
+++ b/tools/runtests.in
@@ -0,0 +1,136 @@
+#!/bin/bash
+#
+# This Source Code Form is subject to the terms of the Mozilla Public
+# License, v. 2.0. If a copy of the MPL was not distributed with this
+# file, You can obtain one at http://mozilla.org/MPL/2.0/.
+#
+
+#
+# Copyright (c) 2014, Joyent, Inc.
+#
+
+#
+# Run the TODONAME tests.
+# Run `./runtests -h` for usage info.
+#
+
+if [ "$TRACE" != "" ]; then
+    export PS4='${BASH_SOURCE}:${LINENO}: ${FUNCNAME[0]:+${FUNCNAME[0]}(): }'
+    set -o xtrace
+fi
+set -o errexit
+set -o pipefail
+
+
+
+#---- guard
+
+if [[ ! -f "/lib/sdc/.sdc-test-no-production-data" ]]; then
+    cat <<EOF
+To run this test you must create the file:
+
+    /lib/sdc/.sdc-test-no-production-data
+
+after ensuring you have no production data on this SDC.
+EOF
+    exit 2
+fi
+
+
+
+#---- config
+
+NAME=TODONAME
+TOP=$(cd $(dirname $0)/../; pwd)
+
+
+
+#---- support functions
+
+function fatal
+{
+    echo "$(basename $0): fatal error: $*"
+    exit 1
+}
+
+function usage
+{
+    echo "Usage:"
+    echo "  runtests [OPTIONS...]"
+    echo ""
+    echo "Options:"
+    echo "  -f FILTER   Filter pattern (substring match) for test files to run."
+}
+
+
+
+#---- mainline
+
+start_time=$(date +%s)
+
+# Options.
+opt_test_pattern=
+while getopts "hf:" opt
+do
+    case "$opt" in
+        h)
+            usage
+            exit 0
+            ;;
+        f)
+            opt_test_pattern=$OPTARG
+            ;;
+        *)
+            usage
+            exit 1
+            ;;
+    esac
+done
+
+OUTPUT_DIR=/var/tmp/${NAME}test
+echo "# Create output dir ($OUTPUT_DIR)."
+rm -rf $OUTPUT_DIR
+mkdir -p $OUTPUT_DIR
+
+
+
+#---- start tests
+
+# TODO
+# Project specific setup and test running goes here.
+# - TAP output should be redirected or tee'd to $OUTPUT_DIR/*.tap
+#   This is used (a) to summarize below and (b) for the Jenkins Job TAP
+#   parsing/reporting.
+# - If reasonable, use $opt_test_pattern to filter the set of test files run.
+#
+# Some examples:
+# - https://mo.joyent.com/amon/blob/master/test/runtests
+# - https://mo.joyent.com/smartos-live/blob/master/src/vm/runtests
+# TODO
+
+
+
+#---- summary
+
+echo ""
+echo "# test results:"
+
+end_time=$(date +%s)
+elapsed=$((${end_time} - ${start_time}))
+
+tests=$(grep "^# tests [0-9]" $OUTPUT_DIR/*.tap | cut -d ' ' -f3 | xargs | tr ' ' '+' | bc)
+passed=$(grep "^# pass  [0-9]" $OUTPUT_DIR/*.tap | tr -s ' ' | cut -d ' ' -f3 | xargs | tr ' ' '+' | bc)
+[[ -z ${tests} ]] && tests=0
+[[ -z ${passed} ]] && passed=0
+fail=$((${tests} - ${passed}))
+
+echo "# Completed in ${elapsed} seconds."
+echo -e "# \033[32mPASS: ${passed} / ${tests}\033[39m"
+if [[ ${fail} -gt 0 ]]; then
+    echo -e "# \033[31mFAIL: ${fail} / ${tests}\033[39m"
+fi
+echo ""
+
+if [[ ${tests} != ${passed} ]]; then
+    exit 1
+fi
diff --git a/tools/service_bundle.dtd.1 b/tools/service_bundle.dtd.1
new file mode 100644
index 0000000..e5c2380
--- /dev/null
+++ b/tools/service_bundle.dtd.1
@@ -0,0 +1,1091 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<!--
+ Copyright (c) 2004, 2010, Oracle and/or its affiliates. All rights reserved.
+
+ CDDL HEADER START
+
+ The contents of this file are subject to the terms of the
+ Common Development and Distribution License (the "License").
+ You may not use this file except in compliance with the License.
+
+ You can obtain a copy of the license at usr/src/OPENSOLARIS.LICENSE
+ or http://www.opensolaris.org/os/licensing.
+ See the License for the specific language governing permissions
+ and limitations under the License.
+
+ When distributing Covered Code, include this CDDL HEADER in each
+ file and include the License file at usr/src/OPENSOLARIS.LICENSE.
+ If applicable, add the following below this CDDL HEADER, with the
+ fields enclosed by brackets "[]" replaced with your own identifying
+ information: Portions Copyright [yyyy] [name of copyright owner]
+
+ CDDL HEADER END
+-->
+
+<!--
+  Service description DTD
+
+    Most attributes are string values (or an individual string from a
+    restricted set), but attributes with a specific type requirement are
+    noted in the comment describing the element.
+-->
+
+<!--
+  XInclude support
+
+    A series of service bundles may be composed via the xi:include tag.
+    smf(5) tools enforce that all bundles be of the same type.
+-->
+
+<!--
+     These entities are used for the property, propval and property_group
+     elements, that require type attributes for manifest, while for profiles
+     the type attributes are only implied.
+-->
+
+<!ENTITY % profile "IGNORE">
+<!ENTITY % manifest "INCLUDE">
+
+<!ELEMENT xi:include
+  (xi:fallback)
+  >
+<!ATTLIST xi:include
+  href CDATA #REQUIRED
+  parse (xml|text) "xml"
+  encoding CDATA #IMPLIED
+  xmlns:xi CDATA #FIXED "http://www.w3.org/2001/XInclude"
+  >
+
+<!ELEMENT xi:fallback
+  ANY
+  >
+<!ATTLIST xi:fallback
+  xmlns:xi CDATA #FIXED "http://www.w3.org/2001/XInclude"
+  >
+
+<!--
+  stability
+
+    This element associates an SMI stability level with the parent
+    element.  See attributes(5) for an explanation of interface
+    stability levels.
+
+    Its attribute is
+
+	value	The stability level of the parent element.
+-->
+
+<!ELEMENT stability EMPTY>
+
+<!ATTLIST stability
+	value		( Standard | Stable | Evolving | Unstable |
+			External | Obsolete ) #REQUIRED >
+
+<!-- Property value lists -->
+
+<!--
+  value_node
+
+    This element represents a single value within any of the typed
+    property value lists.
+
+    Its attribute is
+
+	value	The value for this node in the list.
+-->
+
+<!ELEMENT value_node EMPTY>
+
+<!ATTLIST value_node
+	value CDATA #REQUIRED>
+
+<!--
+  count_list
+  integer_list
+  opaque_list
+  host_list
+  hostname_list
+  net_address_list
+  net_address_v4_list
+  net_address_v6_list
+  time_list
+  astring_list
+  ustring_list
+  boolean_list
+  fmri_list
+  uri_list
+
+    These elements represent the typed lists of values for a property.
+    Each contains one or more value_node elements representing each
+    value on the list.
+
+    None of these elements has attributes.
+-->
+
+<!ELEMENT count_list
+	( value_node+ )>
+
+<!ATTLIST count_list>
+
+<!ELEMENT integer_list
+	( value_node+ )>
+
+<!ATTLIST integer_list>
+
+<!ELEMENT opaque_list
+	( value_node+ )>
+
+<!ATTLIST opaque_list>
+
+<!ELEMENT host_list
+	( value_node+ )>
+
+<!ATTLIST host_list>
+
+<!ELEMENT hostname_list
+	( value_node+ )>
+
+<!ATTLIST hostname_list>
+
+<!ELEMENT net_address_list
+	( value_node+ )>
+
+<!ATTLIST net_address_list>
+
+<!ELEMENT net_address_v4_list
+	( value_node+ )>
+
+<!ATTLIST net_address_v4_list>
+
+<!ELEMENT net_address_v6_list
+	( value_node+ )>
+
+<!ATTLIST net_address_v6_list>
+
+<!ELEMENT time_list
+	( value_node+ )>
+
+<!ATTLIST time_list>
+
+<!ELEMENT astring_list
+	( value_node+ )>
+
+<!ATTLIST astring_list>
+
+<!ELEMENT ustring_list
+	( value_node+ )>
+
+<!ATTLIST ustring_list>
+
+<!ELEMENT boolean_list
+	( value_node+ )>
+
+<!ATTLIST boolean_list>
+
+<!ELEMENT fmri_list
+	( value_node+ )>
+
+<!ATTLIST fmri_list>
+
+<!ELEMENT uri_list
+	( value_node+ )>
+
+<!ATTLIST uri_list>
+
+<!-- Properties and property groups -->
+
+<!--
+   property
+
+     This element is for a singly or multiply valued property within a
+     property group.  It contains an appropriate value list element,
+     which is expected to be consistent with the type attribute.
+
+     Its attributes are
+
+	name	The name of this property.
+
+	type	The data type for this property.
+
+	override These values should replace values already in the
+		repository.
+-->
+
+<![%profile;[
+<!ELEMENT property
+	( count_list | integer_list | opaque_list | host_list | hostname_list |
+	net_address_list | net_address_v4_list | net_address_v6_list |
+	time_list | astring_list | ustring_list | boolean_list | fmri_list |
+	uri_list )? >
+
+<!ATTLIST property
+	name		CDATA #REQUIRED
+	type		( count | integer | opaque | host | hostname |
+			net_address | net_address_v4 | net_address_v6 | time |
+			astring | ustring | boolean | fmri | uri ) #IMPLIED
+	override	( true | false ) "false" >
+]]>
+	
+<![%manifest;[
+<!ELEMENT property
+	( count_list | integer_list | opaque_list | host_list | hostname_list |
+	net_address_list | net_address_v4_list | net_address_v6_list |
+	time_list | astring_list | ustring_list | boolean_list | fmri_list |
+	uri_list )? >
+
+<!ATTLIST property
+	name		CDATA #REQUIRED
+	type		( count | integer | opaque | host | hostname |
+			net_address | net_address_v4 | net_address_v6 | time |
+			astring | ustring | boolean | fmri | uri ) #REQUIRED
+	override	( true | false ) "false" >
+]]>
+
+<!--
+   propval
+
+     This element is for a singly valued property within a property
+     group.  List-valued properties must use the property element above.
+
+     Its attributes are
+
+	name	The name of this property.
+
+	type	The data type for this property.
+
+	value	The value for this property.  Must match type
+		restriction of type attribute.
+
+	override This value should replace any values already in the
+		repository.
+-->
+
+<![%profile;[
+<!ELEMENT propval EMPTY>
+
+<!ATTLIST propval
+	name		CDATA #REQUIRED
+	type		( count | integer | opaque | host | hostname |
+			net_address | net_address_v4 | net_address_v6 | time |
+			astring | ustring | boolean | fmri | uri ) #IMPLIED
+	value		CDATA #REQUIRED
+	override	( true | false ) "false" >
+]]>
+
+<![%manifest;[
+<!ELEMENT propval EMPTY>
+
+<!ATTLIST propval
+	name		CDATA #REQUIRED
+	type		( count | integer | opaque | host | hostname |
+			net_address | net_address_v4 | net_address_v6 | time |
+			astring | ustring | boolean | fmri | uri ) #REQUIRED
+	value		CDATA #REQUIRED
+	override	( true | false ) "false" >
+]]>
+
+<!--
+  property_group
+
+    This element is for a set of related properties on a service or
+    instance.  It contains an optional stability element, as well as
+    zero or more property-containing elements.
+
+    Its attributes are
+
+	name	The name of this property group.
+
+	type	A category for this property group.  Groups of type
+		"framework", "implementation" or "template" are primarily
+		of interest to the service management facility, while
+		groups of type "application" are expected to be only of
+		interest to the service to which this group is attached.
+		Other types may be introduced using the service symbol
+		namespace conventions.
+
+	delete	If in the repository, this property group should be removed.
+-->
+
+<![%profile;[
+<!ELEMENT property_group
+	( stability?, ( propval | property )* )>
+
+<!ATTLIST property_group
+	name		CDATA #REQUIRED
+	type		CDATA #IMPLIED
+	delete		( true | false ) "false" >
+]]>
+
+<![%manifest;[
+<!ELEMENT property_group
+	( stability?, ( propval | property )* )>
+
+<!ATTLIST property_group
+	name		CDATA #REQUIRED
+	type		CDATA #REQUIRED
+	delete		( true | false ) "false" >
+]]>
+
+<!--
+  service_fmri
+
+    This element defines a reference to a service FMRI (for either a
+    service or an instance).
+
+    Its attribute is
+
+	value	The FMRI.
+-->
+
+<!ELEMENT service_fmri EMPTY>
+
+<!ATTLIST service_fmri
+	value		CDATA #REQUIRED>
+
+<!-- Dependencies -->
+
+<!--
+  dependency
+
+    This element identifies a group of FMRIs upon which the service is
+    in some sense dependent.  Its interpretation is left to the
+    restarter to which a particular service instance is delegated.  It
+    contains a group of service FMRIs, as well as a block of properties.
+
+    Its attributes are
+
+	name	The name of this dependency.
+
+	grouping The relationship between the various FMRIs grouped
+		here; "require_all" of the FMRIs to be online, "require_any"
+		of the FMRIs to be online, or "exclude_all" of the FMRIs
+		from being online or in maintenance for the dependency to
+		be satisfied.  "optional_all" dependencies are satisfied
+		when all of the FMRIs are either online or unable to come
+		online (because they are disabled, misconfigured, or one
+		of their dependencies is unable to come online).
+
+	restart_on The type of events from the FMRIs that the service should
+		be restarted for.  "error" restarts the service if the
+		dependency is restarted due to hardware fault.  "restart"
+		restarts the service if the dependency is restarted for
+		any reason, including hardware fault.  "refresh" restarts
+		the service if the dependency is refreshed or restarted for
+		any reason.  "none" will never restart the service due to
+		dependency state changes.
+
+	type	The type of dependency: on another service ('service'), on
+		a filesystem path ('path'), or another dependency type.
+
+	delete	This dependency should be deleted.
+-->
+
+<!ELEMENT dependency
+	( service_fmri*, stability?, ( propval | property )* ) >
+
+<!ATTLIST dependency
+	name		CDATA #REQUIRED
+	grouping	( require_all | require_any | exclude_all |
+			optional_all ) #REQUIRED
+	restart_on	( error | restart | refresh | none ) #REQUIRED
+	type		CDATA #REQUIRED
+	delete		( true | false ) "false" >
+
+<!-- Dependents -->
+
+<!--
+  dependent
+
+    This element identifies a service which should depend on this service.  It
+    corresponds to a dependency in the named service.  The grouping and type
+    attributes of that dependency are implied to be "require_all" and
+    "service", respectively.
+
+    Its attributes are
+
+	name	The name of the dependency property group to create in the
+		dependent entity.
+
+	grouping The grouping relationship of the dependency property
+		group to create in the dependent entity.  See "grouping"
+		attribute on the dependency element.
+
+	restart_on The type of events from this service that the named service
+		should be restarted for.
+
+	delete	True if this dependent should be deleted.
+
+	override Whether to replace an existing dependent of the same name.
+
+-->
+
+<!ELEMENT dependent
+	( service_fmri, stability?, ( propval | property )* ) >
+
+<!ATTLIST dependent
+	name		CDATA #REQUIRED
+	grouping	( require_all | require_any | exclude_all |
+			optional_all) #REQUIRED
+	restart_on	( error | restart | refresh | none) #REQUIRED
+	delete		( true | false ) "false"
+	override	( true | false ) "false" >
+
+<!-- Method execution context, security profile, and credential definitions -->
+
+<!--
+  envvar
+
+    An environment variable. It has two attributes:
+
+	name	The name of the environment variable.
+	value	The value of the environment variable.
+-->
+
+<!ELEMENT envvar EMPTY>
+
+<!ATTLIST envvar
+	name		CDATA #REQUIRED
+	value		CDATA #REQUIRED >
+
+<!--
+  method_environment
+
+    This element defines the environment for a method. It has no
+    attributes, and one or more envvar child elements.
+-->
+
+<!ELEMENT method_environment (envvar+) >
+
+<!ATTLIST method_environment>
+
+<!--
+  method_profile
+
+    This element indicates which exec_attr(5) profile applies to the
+    method context being defined.
+
+    Its attribute is
+
+	name	The name of the profile.
+-->
+
+<!ELEMENT method_profile EMPTY>
+
+<!ATTLIST method_profile
+	name		CDATA #REQUIRED >
+
+<!--
+  method_credential
+
+    This element specifies credential attributes for the execution
+    method to use.
+
+    Its attributes are
+
+	user	The user ID, in numeric or text form.
+
+	group	The group ID, in numeric or text form.  If absent or
+		":default", the group associated with the user in the
+		passwd database.
+
+	supp_groups Supplementary group IDs to be associated with the
+		method, separated by commas or spaces.  If absent or
+		":default", initgroups(3C) will be used.
+
+	privileges An optional string specifying the privilege set.
+
+	limit_privileges An optional string specifying the limit
+		privilege set.
+-->
+
+<!ELEMENT method_credential EMPTY>
+
+<!ATTLIST method_credential
+	user		CDATA #REQUIRED
+	group		CDATA #IMPLIED
+	supp_groups	CDATA #IMPLIED
+	privileges	CDATA #IMPLIED
+	limit_privileges CDATA #IMPLIED >
+
+<!--
+  method_context
+
+    This element combines credential and resource management attributes
+    for execution methods.  It may contain a method_environment, or
+    a method_profile or method_credential element.
+
+    Its attributes are
+
+	working_directory The home directory to launch the method from.
+		":default" can be used as a token to indicate use of the
+		user specified by the credential or profile specified.
+
+	project	The project ID, in numeric or text form.  ":default" can
+		be used as a token to indicate use of the project
+		identified by getdefaultproj(3PROJECT) for the non-root
+		user specified by the credential or profile specified.
+		If the user is root, ":default" designates the project
+		the restarter is running in.
+
+	resource_pool The resource pool name to launch the method on.
+		":default" can be used as a token to indicate use of the
+		pool specified in the project(4) entry given in the
+		"project" attribute above.
+-->
+<!ELEMENT method_context
+	( (method_profile | method_credential)?, method_environment? ) >
+
+<!ATTLIST method_context
+	working_directory	CDATA #IMPLIED
+	project			CDATA #IMPLIED
+	resource_pool		CDATA #IMPLIED >
+
+<!-- Restarter delegation, methods, and monitors -->
+
+<!--
+  exec_method
+
+    This element describes one of the methods used by the designated
+    restarter to act on the service instance.  Its interpretation is
+    left to the restarter to which a particular service instance is
+    delegated.  It contains a set of attributes, an optional method
+    context, and an optional stability element for the optional
+    properties that can be included.
+
+    Its attributes are
+
+	type	The type of method, either "method" or "monitor".
+
+	name	Name of this execution method.  The method names are
+		usually a defined interface of the restarter to which an
+		instance of this service is delegated.
+
+	exec	The string identifying the action to take.  For
+		svc.startd(1M), this is a string suitable to pass to
+		exec(2).
+
+	timeout_seconds [integer] Duration, in seconds, to wait for this
+		method to complete.  A '0' or '-1' denotes an infinite
+		timeout.
+
+	delete	If in the repository, the property group for this method
+		should be removed.
+-->
+
+<!ELEMENT exec_method
+	( method_context?, stability?, ( propval | property )* ) >
+
+<!ATTLIST exec_method
+	type		( method | monitor ) #REQUIRED
+	name		CDATA #REQUIRED
+	exec		CDATA #REQUIRED
+	timeout_seconds	CDATA #REQUIRED
+	delete		( true | false ) "false" >
+
+<!--
+  restarter
+
+    A flag element identifying the restarter to which this service or
+    service instance is delegated.  Contains the FMRI naming the
+    delegated restarter.
+
+    This element has no attributes.
+-->
+
+<!ELEMENT restarter
+	( service_fmri ) >
+
+<!ATTLIST restarter>
+
+<!--
+  Templates
+-->
+
+<!--
+  doc_link
+
+    The doc_link relates a resource described by the given URI to the
+    service described by the containing template.  The resource is
+    expected to be a documentation or elucidatory reference of some
+    kind.
+
+    Its attributes are
+
+      name      A label for this resource.
+
+      uri       A URI to the resource.
+-->
+
+<!ELEMENT doc_link EMPTY>
+
+<!ATTLIST doc_link
+	name		CDATA #REQUIRED
+	uri		CDATA #REQUIRED >
+
+<!--
+  manpage
+
+    The manpage element connects the reference manual page to the
+    template's service.
+
+    Its attributes are
+
+      title     The manual page title.
+
+      section   The manual page's section.
+
+      manpath   The MANPATH environment variable, as described in man(1)
+                that is required to reach the named manual page
+-->
+
+<!ELEMENT manpage EMPTY>
+
+<!ATTLIST manpage
+	title		CDATA #REQUIRED
+	section		CDATA #REQUIRED
+	manpath		CDATA ":default" >
+
+<!--
+  documentation
+
+    The documentation element groups an arbitrary number of doc_link
+    and manpage references.
+
+    It has no attributes.
+-->
+
+<!ELEMENT documentation
+	( doc_link | manpage )* >
+
+<!ATTLIST documentation>
+
+<!--
+  loctext
+
+    The loctext element is a container for localized text.
+
+    Its sole attribute is
+
+	xml:lang The name of the locale, in the form accepted by LC_ALL,
+		etc.  See locale(5).
+-->
+<!ELEMENT loctext
+        (#PCDATA) >
+
+<!ATTLIST loctext
+        xml:lang	CDATA #REQUIRED >
+
+<!--
+  description
+
+    The description holds a set of potentially longer, localized strings that
+    consist of a short description of the service.
+
+    The description has no attributes.
+-->
+<!ELEMENT description
+        ( loctext+ ) >
+
+<!ATTLIST description>
+
+<!--
+  common_name
+
+    The common_name holds a set of short, localized strings that
+    represent a well-known name for the service in the given locale.
+
+    The common_name has no attributes.
+-->
+<!ELEMENT common_name
+        ( loctext+ ) >
+
+<!ATTLIST common_name>
+
+<!--
+  units
+
+    The units a numerical property is expressed in.
+-->
+
+<!ELEMENT units
+	( loctext+ ) >
+
+<!ATTLIST units>
+
+<!--
+  visibility
+
+    Expresses how a property is typically accessed.  This isn't
+    intended as access control, but as an indicator as to how a
+    property is used.
+
+    Its attributes are:
+
+      value     'hidden', 'readonly', or 'readwrite' indicating that
+		the property should be hidden from the user, shown but
+		read-only, or modifiable.
+-->
+
+<!ELEMENT visibility EMPTY>
+
+<!ATTLIST visibility
+	value	( hidden | readonly | readwrite ) #REQUIRED >
+
+<!--
+  value
+
+    Describes a legal value for a property value, and optionally contains a
+    human-readable name and description for the specified property
+    value.
+
+    Its attributes are:
+
+      name	A string representation of the value.
+-->
+
+<!ELEMENT value
+	( common_name?, description? ) >
+
+<!ATTLIST value
+	name	CDATA #REQUIRED >
+
+<!--
+  values
+
+    Human-readable names and descriptions for valid values of a property.
+-->
+
+<!ELEMENT values
+	(value+) >
+
+<!ATTLIST values>
+
+<!--
+  cardinality
+
+    Places a constraint on the number of values the property can take
+    on.
+
+    Its attributes are:
+	min	minimum number of values
+	max	maximum number of values
+
+    Both attributes are optional.  If min is not specified, it defaults to
+    0.  If max is not specified it indicates an unlimited number of values.
+    If neither is specified this indicates 0 or more values.
+-->
+
+<!ELEMENT cardinality EMPTY>
+
+<!ATTLIST cardinality
+	min	CDATA "0"
+	max	CDATA "18446744073709551615">
+
+<!--
+  internal_separators
+
+    Indicates the separators used within a property's value used to
+    separate the actual values.  Used in situations where multiple
+    values are packed into a single property value instead of using a
+    multi-valued property.
+-->
+
+<!ELEMENT internal_separators
+	(#PCDATA) >
+
+<!ATTLIST internal_separators>
+
+<!--
+  range
+
+    Indicates a range of possible integer values.
+
+    Its attributes are:
+
+      min	The minimum value of the range (inclusive).
+      max	The maximum value of the range (inclusive).
+-->
+
+<!ELEMENT range EMPTY>
+
+<!ATTLIST range
+	min	CDATA #REQUIRED
+	max	CDATA #REQUIRED >
+
+<!--
+  constraints
+
+    Provides a set of constraints on the values a property can take on.
+-->
+
+<!ELEMENT constraints
+	( value*, range* ) >
+<!ATTLIST constraints>
+
+<!--
+  include_values
+
+    Includes an entire set of values in the choices block.
+
+    Its attributes are:
+
+	type    Either "constraints" or "values", indicating an
+		inclusion of all values allowed by the property's
+		constraints or all values for which there are
+		human-readable names and descriptions, respectively.
+-->
+
+<!ELEMENT include_values EMPTY>
+
+<!ATTLIST include_values
+	type	( constraints | values ) #REQUIRED >
+
+<!--
+  choices
+
+    Provides a set of common choices for the values a property can take
+    on.  Useful in those cases where the possibilities are unenumerable
+    or merely inconveniently legion, and a manageable subset is desired
+    for presentation in a user interface.
+-->
+
+<!ELEMENT choices
+	( value*, range*, include_values* ) >
+
+<!ATTLIST choices>
+
+<!--
+  prop_pattern
+
+
+    The prop_pattern describes one property of the enclosing property group
+    pattern.
+
+    Its attributes are:
+
+	name    The property's name.
+	type    The property's type.
+	required
+		If the property group is present, this property is required.
+
+	type can be omitted if required is false.
+-->
+
+<!ELEMENT prop_pattern
+	( common_name?, description?, units?, visibility?, cardinality?,
+	  internal_separators?, values?, constraints?, choices? ) >
+
+<!ATTLIST prop_pattern
+	name		CDATA	#REQUIRED
+	type		( count | integer | opaque | host | hostname |
+			net_address | net_address_v4 | net_address_v6 | time |
+			astring | ustring | boolean | fmri | uri ) #IMPLIED
+	required	( true | false )	"false" >
+
+<!--
+  pg_pattern
+
+    The pg_pattern describes one property group.
+    Depending on the element's attributes, these descriptions may apply
+    to just the enclosing service/instance, instances of the enclosing
+    service, delegates of the service (assuming it is a restarter), or
+    all services.
+
+    Its attributes are:
+
+	name    The property group's name.  If not specified, it
+		matches all property groups with the specified type.
+	type    The property group's type.  If not specified, it
+		matches all property groups with the specified name.
+	required
+		If the property group is required.
+	target	The scope of the pattern, which may be all, delegate,
+		instance, or this.  'all' is reserved for framework use
+		and applies the template to all services on the system.
+		'delegate' is reserved for restarters, and means the
+		template applies to all services which use the restarter.
+		'this' would refer to the defining service or instance.
+		'instance' can only be used in a service's template block,
+		and means the definition applies to all instances of this
+		service.
+
+-->
+
+<!ELEMENT pg_pattern
+	( common_name?, description?, prop_pattern* ) >
+
+<!ATTLIST pg_pattern
+	name		CDATA	""
+	type		CDATA	""
+	required	( true | false )	"false"
+	target		( this | instance | delegate | all )	"this" >
+
+<!--
+  template
+
+    The template contains a collection of metadata about the service.
+    It contains a localizable string that serves as a common,
+    human-readable name for the service.  (This name should be less than
+    60 characters in a single byte locale.)  The template may optionally
+    contain a longer localizable description of the service, a
+    collection of links to documentation, either in the form of manual
+    pages or in the form of URI specifications to external documentation
+    sources (such as docs.sun.com).
+
+    The template has no attributes.
+-->
+<!ELEMENT template
+        ( common_name, description?, documentation?, pg_pattern* ) >
+
+<!ATTLIST template>
+
+<!-- Notification Parameters -->
+
+<!ELEMENT paramval EMPTY>
+
+<!ATTLIST paramval
+	name		CDATA #REQUIRED
+	value		CDATA #REQUIRED>
+
+<!ELEMENT parameter
+	( value_node* )>
+
+<!ATTLIST parameter
+	name		CDATA #REQUIRED>
+
+<!ELEMENT event EMPTY>
+
+<!ATTLIST event
+	value		CDATA #REQUIRED>
+
+<!ELEMENT type
+	( ( parameter | paramval )* )>
+
+<!ATTLIST type
+	name		CDATA #REQUIRED
+	active		( true | false ) "true" >
+
+<!--
+  notification parameters
+
+    This element sets the notification parameters for Software Events and
+    Fault Management problem lifecycle events.
+-->
+
+<!ELEMENT notification_parameters
+	( event, type+ )>
+
+<!ATTLIST notification_parameters>
+
+<!-- Services and instances -->
+
+<!--
+  create_default_instance
+
+    A flag element indicating that an otherwise empty default instance
+    of this service (named "default") should be created at install, with
+    its enabled property set as given.
+
+    Its attribute is
+
+	enabled	[boolean] The initial value for the enabled state of
+		this instance.
+-->
+
+<!ELEMENT create_default_instance EMPTY >
+
+<!ATTLIST create_default_instance
+	enabled		( true | false ) #REQUIRED >
+
+<!--
+  single_instance
+
+    A flag element stating that this service can only have a single
+    instance on a particular system.
+-->
+
+<!ELEMENT single_instance EMPTY>
+
+<!ATTLIST single_instance>
+
+<!--
+  instance
+
+    The service instance is the object representing a software component
+    that will run on the system if enabled.  It contains an enabled
+    element, a set of dependencies on other services, potentially
+    customized methods or configuration data, an optional method
+    context, and a pointer to its restarter.  (If no restarter is
+    specified, the master restarter, svc.startd(1M), is assumed to be
+    responsible for the service.)
+
+    Its attributes are
+
+	name	The canonical name for this instance of the service.
+
+	enabled	[boolean] The initial value for the enabled state of
+		this instance.
+-->
+
+<!ELEMENT instance
+	( restarter?, dependency*, dependent*, method_context?,
+	exec_method*, notification_parameters*, property_group*,
+	template? ) >
+
+<!ATTLIST instance
+	name		CDATA #REQUIRED
+	enabled		( true | false ) #REQUIRED >
+
+<!--
+  service
+
+    The service contains the set of instances defined by default for
+    this service, an optional method execution context, any default
+    methods, the template, and various restrictions or advice applicable
+    at installation.  The method execution context and template elements
+    are required for service_bundle documents with type "manifest", but
+    are optional for "profile" or "archive" documents.
+
+    Its attributes are
+
+	name	The canonical name for the service.
+
+	version	[integer] The integer version for this service.
+
+	type	Whether this service is a simple service, a delegated
+		restarter, or a milestone (a synthetic service that
+		collects a group of dependencies).
+-->
+
+<!ELEMENT service
+	( create_default_instance?, single_instance?, restarter?,
+	dependency*, dependent*, method_context?, exec_method*,
+	notification_parameters*, property_group*, instance*,
+	stability?, template? ) >
+
+<!ATTLIST service
+	name		CDATA #REQUIRED
+	version		CDATA #REQUIRED
+	type		( service | restarter | milestone ) #REQUIRED >
+
+<!--
+  service_bundle
+
+    The bundle possesses two attributes:
+
+	type	How this file is to be understood by the framework (or
+		used in a non-framework compliant way). Standard types
+		are 'archive', 'manifest', and 'profile'.
+	
+	name	A name for the bundle.  Manifests should be named after
+		the package which delivered them; profiles should be
+		named after the "feature set nickname" they intend to
+		enable.
+-->
+
+<!ELEMENT service_bundle
+	( service_bundle* | service* | xi:include* )>
+
+<!ATTLIST service_bundle
+	type		CDATA #REQUIRED
+	name		CDATA #REQUIRED>
diff --git a/tools/validate-buildenv.sh b/tools/validate-buildenv.sh
new file mode 100644
index 0000000..7aa1f68
--- /dev/null
+++ b/tools/validate-buildenv.sh
@@ -0,0 +1,727 @@
+#!/bin/bash
+#
+# This Source Code Form is subject to the terms of the Mozilla Public
+# License, v. 2.0. If a copy of the MPL was not distributed with this
+# file, You can obtain one at http://mozilla.org/MPL/2.0/.
+#
+
+#
+# Copyright (c) 2019, Joyent, Inc.
+#
+
+#
+# Check if the current build machine is supported for building this component
+# and that the build environment seems sane.
+#
+# Ideally, rather than checking the sanity of any random shell environment,
+# we'd have a way to fully specify the build environment, sanitizing it so that
+# only specific user-set environment variables are allowed. That is not done
+# here.
+#
+# This is specifically a bash script because we're using its associative array
+# support.
+#
+# Meant to be run from the top level of a git repository before commencing a
+# build. It checks the following:
+#
+# - the pkgsrc version is compatible with one derived from $NODE_PREBUILT_IMAGE
+#   or $BASE_IMAGE_UUID
+# - our devzone has a delegated dataset
+# - the list of pkgsrc packages match the ones installed on jenkins-agent images
+# - the RBAC profiles(1) of the user, looking for 'Primary Administrator' or
+#   uid=0
+# - the build environment has a $PATH with /opt/local/bin before /usr/bin et al
+# - our build platform for this component, BUILD_PLATFORM, matches uname -vish
+# - several non-pkgsrc programs needed by the build are availabe on the $PATH
+# - git submodules for this repository, if present, are up to date
+#
+
+#
+# For the NODE_PREBUILT_IMAGE checks, we use this list from
+# From https://download.joyent.com/pub/build/sdcnode/README.html
+# the following images versions are supported:
+#
+#    sdc-smartos@1.6.3: fd2cc906-8938-11e3-beab-4359c665ac99
+#    sdc-multiarch@13.3.1: b4bdc598-8939-11e3-bea4-8341f6861379
+#    sdc-base@14.2.0: de411e86-548d-11e4-a4b7-3bb60478632a
+#    sdc-minimal-multiarch-lts@15.4.1: 18b094b0-eb01-11e5-80c1-175dac7ddf02
+#    triton-origin-multiarch-15.4.1@1.0.1: 04a48d7d-6bb5-4e83-8c3b-e60a99e0f48f
+#    minimal-multiarch@18.1.0: 1ad363ec-3b83-11e8-8521-2f68a4a34d5d
+#    triton-origin-multiarch-18.1.0: b6ea7cb4-6b90-48c0-99e7-1d34c2895248
+#
+# In the future, we would prefer if the pkgsrc versions were declared
+# directly in Makefiles without needing this lookup. (see TOOLS-2038)
+#
+
+#
+# NOTE If you modify this file, be sure to check that
+# jenkins-agent.git:/toolbox/auto-user-script.sh is in sync with the set
+# of pkgsrc packages installed per-pkgsrc version. It's not wonderful that
+# these lists are duplicated here :-/
+#
+
+if [[ -n "$ENGBLD_SKIP_VALIDATE_BUILDENV" ]]; then
+    echo "\$ENGBLD_SKIP_VALIDATE_BUILDENV set - not running build environment checks!"
+    exit 0
+fi
+
+if [[ $(uname -s) != "SunOS" ]]; then
+    echo "Only illumos build machines are supported."
+    echo "Set \$ENGBLD_SKIP_VALIDATE_BUILDENV in the environment"
+    echo "to override this."
+    echo "Exiting now."
+    exit 1
+fi
+
+# Used to cross-check declared NODE_PREBUILT_IMAGE to pkgsrc version.
+declare -A PKGSRC_MAP=(
+    [fd2cc906-8938-11e3-beab-4359c665ac99]=2011Q4
+    [b4bdc598-8939-11e3-bea4-8341f6861379]=2013Q3
+    [de411e86-548d-11e4-a4b7-3bb60478632a]=2014Q2
+    [18b094b0-eb01-11e5-80c1-175dac7ddf02]=2015Q4
+    [04a48d7d-6bb5-4e83-8c3b-e60a99e0f48f]=2015Q4
+    [1ad363ec-3b83-11e8-8521-2f68a4a34d5d]=2018Q1
+    [b6ea7cb4-6b90-48c0-99e7-1d34c2895248]=2018Q1
+)
+
+# Used to provide useful error messages to the user, mapping the
+# NODE_PREBUILT_IMAGE uuid to the human-friendly image name.
+declare -A SDC_MAP=(
+    [fd2cc906-8938-11e3-beab-4359c665ac99]=sdc-smartos@1.6.3
+    [b4bdc598-8939-11e3-bea4-8341f6861379]=sdc-multiarch@13.3.1
+    [de411e86-548d-11e4-a4b7-3bb60478632a]=sdc-base@14.2.0
+    [18b094b0-eb01-11e5-80c1-175dac7ddf02]=sdc-minimal-multiarch-lts@15.4.1
+    [04a48d7d-6bb5-4e83-8c3b-e60a99e0f48f]=triton-origin-multiarch-15.4.1@1.0.1
+    [1ad363ec-3b83-11e8-8521-2f68a4a34d5d]=minimal-multiarch@18.1.0
+    [b6ea7cb4-6b90-48c0-99e7-1d34c2895248]=triton-origin-multiarch-18.1.0@1.0.1
+)
+
+# Used to provide useful error messages to the user, mapping the NODE_PREBUILT
+# image uuid to a corresponding jenkins-agent image uuid.
+# Jenkins agent images are built by https://github.com/joyent/jenkins-agent
+declare -A JENKINS_AGENT_MAP=(
+    [fd2cc906-8938-11e3-beab-4359c665ac99]=956f365d-2444-4163-ad48-af2f377726e0
+    [b4bdc598-8939-11e3-bea4-8341f6861379]=7b1ac281-3fe4-4cf7-858c-2ff73ec64f4e
+    [de411e86-548d-11e4-a4b7-3bb60478632a]=83708aad-20a8-45ff-bfc0-e53de610e418
+    [18b094b0-eb01-11e5-80c1-175dac7ddf02]=1356e735-456e-4886-aebd-d6677921694c
+    [04a48d7d-6bb5-4e83-8c3b-e60a99e0f48f]=1356e735-456e-4886-aebd-d6677921694c
+    [1ad363ec-3b83-11e8-8521-2f68a4a34d5d]=8b297456-1619-4583-8a5a-727082323f77
+    [b6ea7cb4-6b90-48c0-99e7-1d34c2895248]=8b297456-1619-4583-8a5a-727082323f77
+)
+
+# For each pkgsrc version, set a list of packages that must be present
+PKGSRC_PKGS_2011Q4="
+    gcc-compiler
+    gcc-runtime
+    gcc-tools
+    cscope
+    gmake
+    scmgit-base
+    python26
+    png
+    GeoIP
+    GeoLiteCity
+    ghostscript
+    zookeeper-client
+    binutils
+    postgresql91-client-9.1.2
+    gsharutils
+    cdrtools
+    coreutils
+    pigz"
+
+PKGSRC_PKGS_2013Q3="
+    grep
+    build-essential
+    python27
+    py27-expat
+    coreutils
+    gsharutils
+    pigz"
+
+PKGSRC_PKGS_2014Q2="
+    grep
+    build-essential
+    python27
+    py27-expat
+    coreutils
+    gsharutils
+    pigz"
+
+PKGSRC_PKGS_2015Q4="
+    grep
+    build-essential
+    python27
+    py27-expat
+    coreutils
+    gsed
+    gsharutils
+    flex
+    pcre-8.41
+    pigz"
+
+PKGSRC_PKGS_2018Q1="
+    grep
+    build-essential
+    python27
+    py27-expat
+    coreutils
+    gsed
+    gsharutils
+    flex
+    pcre-8.42
+    pigz"
+
+UPDATES_URL="https://updates.joyent.com?channel=experimental"
+UPDATES_IMG_URL="https://updates.joyent.com/images/"
+
+#
+# Determine the pkgsrc release of this build machine and the sdcnode prebuilt
+# image, as declared by this component's Makefile. These variables get used by
+# other functions in this script, globals for now.
+#
+function get_pkgsrc_sdcnode_versions {
+
+    REQUIRED_IMAGE=$(
+        make -s --no-print-directory print-BASE_IMAGE_UUID 2> /dev/null |
+            cut -d= -f2)
+    PKGSRC_RELEASE=$(
+        grep ^release: /etc/pkgsrc_version | cut -d: -f2 | sed -e 's/ //g')
+
+    # If there's no BASE_IMAGE_UUID, use NODE_PREBUILT_IMAGE instead.
+    # In either case, what we really want to know is whether this build machine
+    # is running software compatible with the runtime that gets used for
+    # this component in Triton/Manta. This relies on the special 'print-VAR'
+    # target from Makefile.targ
+    if [[ -z "$REQUIRED_IMAGE" ]]; then
+        REQUIRED_IMAGE=$(
+            make -s --no-print-directory \
+                print-NODE_PREBUILT_IMAGE 2> /dev/null | cut -d= -f2)
+    fi
+
+}
+
+#
+# Check that this environment has a $PATH that finds /opt/local/bin before
+# /usr/bin, /bin and /opt/tools/bin. This is important for two reasons:
+#
+# 1. several of the Manta/Triton Makefiles have assumptions on the behaviour
+#    of some UNIX utilities that differ between GNU and Illumos variants
+#    (e.g. the way cp(1) treats symlinks)
+# 2. we compile with gcc 4.9.3 and not yet 7.3.x (which is in /opt/tools)
+#
+# Eventually, we may want to validate other aspects of the build environment,
+# or provide a way for the build to obtain a santized build environment, but
+# let's start here, since we know that failing this test definitely breaks
+# builds.
+#
+# Return 0 if it looks like $PATH is correctly set.
+#
+function validate_build_path {
+    echo $PATH | awk '{
+        pathlen = split($0, path_arr, ":");
+        found_optlocalbin = "false";
+        ret = 0;
+        for (i = 1; i <= pathlen; i++) {
+            if (path_arr[i] == "/opt/local/bin") {
+                found_optlocalbin = "true";
+                continue;
+            }
+            if ((path_arr[i] == "/usr/sbin" ||
+                    path_arr[i] == "/usr/bin" ||
+                    path_arr[i] == "/bin" ||
+                    path_arr[i] == "/sbin" ||
+                    path_arr[i] == "/opt/tools/bin") &&
+                        found_optlocalbin == "false") {
+                ret = 1;
+                break;
+            }
+        }
+        if (found_optlocalbin == "false")
+            ret = 1;
+
+        exit ret;
+    }'
+    result=$?
+    if [[ "$result" -ne 0 ]]; then
+        echo "Error: unexpected \$PATH"
+        echo ""
+        echo "The \$PATH in this shell environment has /bin, /usr/bin, /sbin"
+        echo "/usr/sbin or /opt/tools/bin appearing before /opt/local/bin."
+        echo ""
+        echo "Several of the Manta/Triton component Makefiles contain"
+        echo "assumptions on the GNU implementations of UNIX utilities."
+        echo ""
+        echo "/opt/tools/bin contains compilers that are not currently"
+        echo "supported by the build."
+        echo "The \$PATH should be changed so that /opt/local/bin appears"
+        echo "before /usr/bin, /usr/sbin, /opt/tools/bin and /bin."
+        echo ""
+        echo "The current path is:"
+        echo $PATH
+    fi
+    return $result
+}
+
+#
+# Check that this build machine is running a pkgsrc version appropriate for
+# building this component. Dispense advice on how to obtain jenkins-agent
+# images if this machine is not appropriate.
+#
+function validate_pkgsrc_version {
+
+    # Add an escape hatch.
+    if [[ -n "$ENGBLD_SKIP_VALIDATE_BUILD_PKGSRC" ]]; then
+        echo "Skipping pkgsrc build machine validity tests."
+        return 0
+    fi
+
+    #
+    # We use $REQUIRED_IMAGE as a key to determine the correct build machine
+    # for a given workspace. This works fine for most current consumers of
+    # eng.git, as they either set it, or set $NODE_PREBUILT_IMAGE. If having
+    # neither of these becomes common for future consumers of eng.git, we may
+    # want to change this code, perhaps to hardcode a default minimum
+    # PKGSRC_RELEASE for the build machine, omitting the warning below.
+    #
+    if [[ -z "$REQUIRED_IMAGE" ]]; then
+        echo "Info: No apparent NODE_PREBUILT_IMAGE or BASE_IMAGE_UUID value."
+        echo "Perhaps this build machine will work?"
+        echo ""
+        return 0
+    fi
+
+    if [[ -z "${PKGSRC_MAP[$REQUIRED_IMAGE]}" ]]; then
+        echo "Error: unable to map $REQUIRED_IMAGE to a pkgsrc version"
+        echo "changes needed to 'validate-build-platform.sh'?"
+        echo ""
+        return 1
+    fi
+
+    if [[ "$PKGSRC_RELEASE" != "${PKGSRC_MAP[$REQUIRED_IMAGE]}" ]]; then
+
+        local SDC_IMAGE_NAME="${SDC_MAP[${REQUIRED_IMAGE}]}"
+        local JENKINS_IMAGE="${JENKINS_AGENT_MAP[${REQUIRED_IMAGE}]}"
+
+        local JENK_IMG_URL=${UPDATES_IMG_URL}${JENKINS_IMAGE}
+        echo "This build machine should not be used to build this component."
+        echo ""
+        echo "expected pkgsrc version ${PKGSRC_MAP[$REQUIRED_IMAGE]}"
+        echo " running pkgsrc version ${PKGSRC_RELEASE} "
+        echo ""
+        echo "This component should build on an image based on $SDC_IMAGE_NAME"
+        echo "The following jenkins-agent image will work: ${JENKINS_IMAGE}"
+        echo ""
+        echo "To retrieve this image on Triton, use:"
+        echo "    sdc-imgadm import -S '${UPDATES_URL}' ${JENKINS_IMAGE}"
+        echo ""
+        echo "on SmartOS, use:"
+        echo "    imgadm import -S '${UPDATES_URL}' ${JENKINS_IMAGE}"
+        echo ""
+        echo "or import by hand, with:"
+        echo "    curl -k -o img.manifest '${JENK_IMG_URL}?channel=experimental'"
+        echo "    curl -k -o img.gz '${JENK_IMG_URL}/file?channel=experimental'"
+        echo "and then"
+        echo "    imgadm install -m img.manifest -f img.gz"
+        echo ""
+        return 1
+    else
+        # The build machine pkgsrc version is valid for building this component.
+        return 0
+    fi
+}
+
+#
+# Return 0 if uid==0 or the current user has the 'Primary Administrator'
+# profile, returning 1 otherwise. Checking for uid==0 shouldn't really imply
+# privilege, but that's the current reality in illumos (sorry casper!)
+#
+function validate_rbac_profile {
+    /usr/bin/id | grep -q uid=0
+    if [[ $? -eq 0 ]]; then
+        return 0
+    fi
+
+    /usr/bin/profiles | grep -q 'Primary Administrator'
+    if [[ $? -eq 0 ]]; then
+        return 0
+    fi
+
+    echo "The current user should have the 'Primary Administrator' profile"
+    echo "which is needed to perform some parts of the build, e.g."
+    echo "'buildimage'."
+    echo "To configure this, as root, run:"
+    echo "    usermod -P 'Primary Administrator' $USER"
+    echo ""
+    return 1
+}
+
+#
+# Return 0 if it looks like we have a delegated dataset for this VM
+#
+function validate_delegated_dataset {
+    zonename=$(/usr/bin/zonename)
+    # it seems unlikely that someone's building in a gz, but it should be fine.
+    if [[ "$zonename" == "global" ]]; then
+        return 0
+    fi
+
+    has_delegated_ds=$(zfs list -H -o name zones/$zonename/data 2>/dev/null)
+    if [[ -z "$has_delegated_ds" ]]; then
+        local djc_base="https://docs.joyent.com/private-cloud/instances/"
+        echo "The current devzone does not have a delegated zfs dataset,"
+        echo "which is required for 'buildimage' to function."
+        echo "Please recreate this devzone, ensuring it has a delegated ds."
+        echo ""
+        echo "To do this, when using vmadm in SmartOS or sdc-vmapi in"
+        echo "Triton, add:"
+        echo "    'delegate_dataset': true,"
+        echo "to the json configuration. If using the Triton admin interface,"
+        echo "select 'Delegate Dataset' when provisioning the instance."
+        echo "For more information, see:"
+        echo "$djc_base/delegated-data-sets"
+        echo ""
+        return 1
+    fi
+    return 0
+}
+
+#
+# Return 0 if $BUILD_PLATFORM in the component's Makefile matches the timestamp
+# encoded in uname -s output. We allow this check to be overridden
+# independently, since in development, not building on the official platform
+# image is common.
+#
+function validate_build_platform {
+
+    if [[ -n "$ENGBLD_SKIP_VALIDATE_BUILD_PLATFORM" ]]; then
+        return 0
+    fi
+
+    current_platform=$(/usr/bin/uname -v | sed -e 's/.*_//')
+    component_platform=$(
+        make -s --no-print-directory print-BUILD_PLATFORM 2> /dev/null |
+            cut -d= -f2)
+
+    # some components do not set a required build platform.
+    if [[ -z "$component_platform" ]]; then
+        return 0
+    fi
+
+    # this seems unlikely.
+    if [[ -z "$current_platform" ]]; then
+        echo "WARNING: unable to determine current build platform!"
+        return 1
+    fi
+
+    if [[ "$component_platform" != "$current_platform" ]]; then
+        echo "The current platform image, $current_platform, is not valid."
+        echo "This component should instead be built on $component_platform"
+        echo ""
+        echo "To disable this check, set "
+        echo "\$ENGBLD_SKIP_VALIDATE_BUILD_PLATFORM in the environment."
+        echo ""
+        return 1
+    fi
+    return 0
+}
+
+#
+# Emit a line of the form "<pkgsrc release> <one word description>"
+#
+function print_required_pkgsrc_version {
+    if [[ -n "$REQUIRED_IMAGE" ]]; then
+        echo "${PKGSRC_MAP[$REQUIRED_IMAGE]} ${SDC_MAP[$REQUIRED_IMAGE]} ${JENKINS_AGENT_MAP[$REQUIRED_IMAGE]}"
+        exit 0
+    else
+        exit 1
+    fi
+}
+
+#
+# Check that a list of pkgsrc packages appropriate to this release are
+# installed.For now, we don't care about version numbers.
+#
+function validate_pkgsrc_pkgs {
+
+    # Add an escape hatch.
+    if [[ -n "$ENGBLD_SKIP_VALIDATE_BUILD_PKGSRC" ]]; then
+        echo "Skipping pkgsrc package version validity tests."
+        return 0
+    fi
+
+    if [[ -z "$PKGSRC_RELEASE" ]]; then
+        echo "Unable to determine pkgsrc release"
+        return 1
+    fi
+
+    PKGSRC_VAR_NAME=PKGSRC_PKGS_${PKGSRC_RELEASE}
+    EXPECTED_PKGS=${!PKGSRC_VAR_NAME}
+    PKG_LIST_FILE=$(mktemp /tmp/validate_build_pkgsrc.XXXXXX)
+    /opt/local/bin/pkgin list | cut -f 1 > $PKG_LIST_FILE
+
+    MISSING_PKGS=""
+    for pkg in ${EXPECTED_PKGS}; do
+        FOUND=""
+        grep -q "$pkg-[0-9].*" $PKG_LIST_FILE
+        if [[ $? -eq 0 ]]; then
+            FOUND=true
+        fi
+        grep -q "$pkg " $PKG_LIST_FILE
+        if [[ $? -eq 0 ]]; then
+            FOUND=true
+        fi
+        if [[ -z "$FOUND" ]]; then
+            MISSING_PKGS="$MISSING_PKGS $pkg"
+        fi
+    done
+    rm $PKG_LIST_FILE
+
+    if [[ -n "$MISSING_PKGS" ]]; then
+        echo "The following packages should be installed for $PKGSRC_RELEASE:"
+        echo "$MISSING_PKGS"
+
+        # add a special-case for scmgit on smartos 1.6.3, where the version
+        # from pkgsrc isn't modern enough. In particular, our tarball supports
+        # TLSv1.2. Newer jenkins-agent images have fixed this by symlinking
+        # to the version of git from /opt/tools instead, but warn just in case.
+        if [[ "$PKGSRC_RELEASE" == "2011Q4" ]]; then
+            JDEV="https://us-east.manta.joyent.com/Joyent_Dev/public/bits/"
+            MODERN_GIT_TARBALL="modern-git-20170223a.tar.gz"
+            echo "Note: the version of scmgit from pkgsrc may be too old."
+            echo "Please verify that /opt/local/bin/git (which may be a "
+            echo "symlink) is at least at version 2.12.0, or download the"
+            echo "newer version from $JDEV/$MODERN_GIT_TARBALL"
+        fi
+
+        return 1
+    fi
+    return 0
+}
+
+#
+# Check that this system has /opt/tools/bin. Note that /opt/tools delivers
+# its own pkgsrc contents and is a wholly separate pkgsrc installation than
+# /opt/local/bin.
+#
+function validate_opt_tools {
+    if [[ ! -f /opt/tools/bin/pkgin ]]; then
+
+        local JENKINS_IMAGE="${JENKINS_AGENT_MAP[${REQUIRED_IMAGE}]}"
+        echo "This build zone is missing /opt/tools/bin, which is"
+        echo "needed in order to run certain parts of the build, notably"
+        echo "the 'buildimage' target."
+        echo ""
+        echo "All modern jenkins-agent images contain these, so using"
+        echo "this image will work as a build zone: $JENKINS_IMAGE"
+        echo ""
+        echo "Alternatively, you can install the current pkgsrc bootstrap bits"
+        echo "which deliver /opt/tools. Run the following:"
+        echo ""
+        BOOTSTRAP_URL="https://pkgsrc.joyent.com/packages/SmartOS/bootstrap/"
+        BOOTSTRAP_TAR="bootstrap-2018Q3-tools.tar.gz"
+        BOOTSTRAP_SHA="2244695a8ec0960e26c6f83cbe159a5269033d6a"
+        echo "    curl -k -o /var/tmp/${BOOTSTRAP_TAR} \\"
+        echo "        ${BOOTSTRAP_URL}/${BOOTSTRAP_TAR}"
+        echo ""
+        echo " ( verify that the SHA-1 sum of the tar file is $BOOTSTRAP_SHA )"
+        echo ""
+        echo "    tar -zxpf /var/tmp/${BOOTSTRAP_TAR} -C /"
+        echo "    rm /var/tmp/${BOOTSTRAP_TAR}"
+        echo ""
+        return 1
+    else
+        # Validate we have an expected set of packages
+        PKG_LIST_FILE=$(mktemp /tmp/validate_build_pkgsrc.XXXXXX)
+        /opt/tools/bin/pkgin list | cut -f 1 > $PKG_LIST_FILE
+
+        EXPECTED_PKGS="curl
+            git-base
+            git-docs
+            git-contrib
+            openjdk8
+            nodejs-6.14.4
+            npm"
+
+        MISSING_PKGS=""
+        for pkg in ${EXPECTED_PKGS}; do
+            FOUND=""
+            grep -q "$pkg-[0-9].*" $PKG_LIST_FILE
+            if [[ $? -eq 0 ]]; then
+                FOUND=true
+            fi
+            grep -q "$pkg " $PKG_LIST_FILE
+            if [[ $? -eq 0 ]]; then
+                FOUND=true
+            fi
+            if [[ -z "$FOUND" ]]; then
+                MISSING_PKGS="$MISSING_PKGS $pkg"
+            fi
+        done
+        rm $PKG_LIST_FILE
+
+        if [[ -n "$MISSING_PKGS" ]]; then
+            echo "The following packages must be installed in /opt/tools:"
+            echo ""
+            echo "$MISSING_PKGS"
+            echo ""
+            echo "Use /opt/tools/bin/pkgin in <package name> ..."
+            echo ""
+            return 1
+        fi
+    fi
+    return 0
+}
+
+#
+# Check that several programs needed by the build which aren't available from
+# pkgsrc are available somewhere on the path. Trust that the versions present
+# are sufficient.
+#
+function validate_non_pkgsrc_bins {
+    REQUIRED_PROGS="mmd5
+        mmkdir
+        mput
+        msign
+        updates-imgadm"
+
+    MISSING_PROGS=""
+
+    for prog in ${REQUIRED_PROGS}; do
+        command -v $prog > /dev/null
+        if [[ $? -ne 0 ]]; then
+            MISSING_PROGS="$prog $MISSING_PROGS"
+        fi
+    done
+
+    if [[ -n "$MISSING_PROGS" ]]; then
+        echo "The following programs were not found in \$PATH:"
+        echo ""
+        echo "$MISSING_PROGS"
+        echo ""
+        echo "These should be installed by hand. If we're running on a "
+        echo "'jenkins-agent' image, they may be found in /root/bin and "
+        echo "this should be added to \$PATH."
+        echo ""
+        echo "Otherwise, the following command will install the required"
+        echo "programs:"
+        echo ""
+        echo "    npm install manta imgapi-cli"
+        echo ""
+        return 1
+    fi
+    return 0
+}
+
+#
+# Validate that git submodules, if present, match the versions recorded in
+# the top-level git repository. Submodules that are not initialized are not
+# checked. We assume that the component's use of the 'deps/%/.git' make target
+# will initialize missing submodules as needed.
+# This check does not catch submodules containing uncommitted local changes.
+# For that case, we just end up issuing a warning via the verify_clean_repo(..)
+# check.
+#
+function validate_submodules {
+    if [[ -n "$ENGBLD_SKIP_VALIDATE_SUBMODULES" ]]; then
+        return 0
+    fi
+    # current git seems not to properly show submodules with merge conflicts,
+    # (^U, below) but we'll keep this in case it gets fixed in the future.
+    MODIFIED_SUBMODULES=$(git submodule | grep -e ^+ -e ^U | cut -d' ' -f 2)
+    if [[ -n "$MODIFIED_SUBMODULES" ]]; then
+        echo "The following submodules are not checked out to the versions"
+        echo "recorded in this repository:"
+        echo ""
+        for module in $MODIFIED_SUBMODULES; do
+            echo $module
+        done
+        echo ""
+        echo "To fix this, please run the following command:"
+        echo ""
+        echo "git submodule update"
+        echo ""
+        echo "If this was intentional (e.g. you're making changes to the"
+        echo "submodule, but have not yet staged those changes) and to disable"
+        echo "this check, set \$ENGBLD_SKIP_VALIDATE_SUBMODULES"
+        echo "in the environment."
+        echo ""
+        return 1
+    fi
+    return 0
+}
+
+#
+# Issue a warning to the developer if their workspace contains uncommitted
+# changes, which would result in bits-upload.sh not posting any built bits
+# to Manta or updates.joyent.com
+#
+function verify_clean_repo {
+    HAS_DIRTY=$(git describe --all --long --dirty | grep '\-dirty$')
+    if [[ -n "$HAS_DIRTY" ]]; then
+        echo "WARNING: this workspace contains uncommitted changes,"
+        echo "which means that any build artifacts will not be uploaded by"
+        echo "bits-upload.sh to either Manta or updates.joyent.com"
+    fi
+}
+
+function usage {
+    echo "Usage: validate-build-platform [-h] [-r]"
+    echo "  -h       print usage"
+    echo "  -r       only print required pkgsrc release and description"
+    exit 2
+}
+
+#
+# Main
+#
+while getopts "rh" opt; do
+    case "${opt}" in
+        r)
+            do_required_version=true
+            ;;
+        h)
+            do_usage=true
+            ;;
+    esac
+done
+
+if [[ -n "${do_usage}" ]]; then
+    usage
+fi
+
+get_pkgsrc_sdcnode_versions
+
+if [[ -n "${do_required_version}" ]]; then
+    print_required_pkgsrc_version
+    exit $?
+else
+    RESULT=0
+    validate_pkgsrc_version
+    RESULT=$(( $RESULT + $? ))
+    validate_pkgsrc_pkgs
+    RESULT=$(( $RESULT + $? ))
+    validate_rbac_profile
+    RESULT=$(( $RESULT + $? ))
+    validate_delegated_dataset
+    RESULT=$(( $RESULT + $? ))
+    validate_build_path
+    RESULT=$(( $RESULT + $? ))
+    validate_build_platform
+    RESULT=$(( $RESULT + $? ))
+    validate_opt_tools
+    RESULT=$(( $RESULT + $? ))
+    validate_non_pkgsrc_bins
+    RESULT=$(( $RESULT + $? ))
+    validate_submodules
+    RESULT=$(( $RESULT + $? ))
+    # this doesn't contribute to success/failure, but warns
+    # developers that '-dirty' repositories will result in
+    # bits-upload not posting to Manta/updates.joyent.com.
+    verify_clean_repo
+    if [[ "$RESULT" -gt 0 ]]; then
+        exit 1
+    else
+        exit 0
+    fi
+fi
diff --git a/vminfod/Cargo.toml b/vminfod/Cargo.toml
new file mode 100644
index 0000000..05b48d3
--- /dev/null
+++ b/vminfod/Cargo.toml
@@ -0,0 +1,14 @@
+[package]
+name = "vminfod"
+version = "0.1.0"
+authors = ["Mike Zeller <mike@mikezeller.net>"]
+edition = "2018"
+
+[dependencies]
+hyper = "0.12.25"
+futures = "0.1.25"
+serde = { version = "1.0.89", features = ["derive"] }
+serde_json = "1.0.39"
+crossbeam-channel = "0.3.8"
+tokio = "0.1.17"
+log = "0.4.6"
diff --git a/vminfod/examples/print-events.rs b/vminfod/examples/print-events.rs
new file mode 100644
index 0000000..945aca0
--- /dev/null
+++ b/vminfod/examples/print-events.rs
@@ -0,0 +1,9 @@
+fn main() {
+    // starts a new thread that sends events back over a channel
+    let (rx, _vminfod_handle) = vminfod::start_vminfod_stream();
+
+    // do something with each event
+    for event in rx.iter() {
+        println!("{:#?}", event);
+    }
+}
diff --git a/vminfod/src/client.rs b/vminfod/src/client.rs
new file mode 100644
index 0000000..25d0305
--- /dev/null
+++ b/vminfod/src/client.rs
@@ -0,0 +1,77 @@
+// This Source Code Form is subject to the terms of the Mozilla Public
+// License, v. 2.0. If a copy of the MPL was not distributed with this
+// file, You can obtain one at http://mozilla.org/MPL/2.0/.
+
+// Copyright 2019 Joyent, Inc.
+
+use crate::linefeed::Lines;
+use crate::VminfodEvent;
+use crossbeam_channel::Sender;
+use futures::{Future, Stream};
+use hyper::{Body, Client as HyperClient, Request};
+use tokio::runtime::current_thread::Runtime;
+
+use std::string::FromUtf8Error;
+
+#[derive(Debug)]
+enum Error {
+    Hyper(hyper::Error),
+    Serde(serde_json::error::Error),
+    FromUtf8(FromUtf8Error),
+}
+
+impl From<serde_json::error::Error> for Error {
+    fn from(err: serde_json::error::Error) -> Error {
+        Error::Serde(err)
+    }
+}
+
+impl From<FromUtf8Error> for Error {
+    fn from(err: FromUtf8Error) -> Error {
+        Error::FromUtf8(err)
+    }
+}
+
+pub(crate) struct Client {
+    sender: Sender<VminfodEvent>,
+}
+
+impl Client {
+    pub(crate) fn new(sender: Sender<VminfodEvent>) -> Self {
+        Client { sender }
+    }
+
+    pub(crate) fn run(&self) {
+        let req = Request::builder()
+            .method("GET")
+            .header(
+                "User-Agent",
+                "cfwlogd - VminfodWatcher (firewall-logger-agent)",
+            )
+            .uri("http://127.0.0.1:9090/events")
+            .body(Body::from(""))
+            .expect("invalid hyper request params");
+
+        let tx = self.sender.clone();
+        let client = HyperClient::new();
+        let connection = client
+            .request(req)
+            .map_err(|e| {
+                error!("failed to connect to vminfod: {}", e);
+            })
+            .and_then(|res| {
+                Lines::new(res.into_body().map_err(Error::Hyper))
+                    .for_each(move |line| {
+                        let event: VminfodEvent = serde_json::from_str(&line)?;
+                        tx.send(event).unwrap();
+                        Ok(())
+                    })
+                    .map_err(|e| error!("vminfod event stream closed: {:#?}", e))
+            });
+
+        // The vminfod stream is processed by the current thread rather than a pool of threads
+        let mut rt = Runtime::new().expect("failed to create vminfod tokio runtime");
+        rt.spawn(connection);
+        rt.run().expect("failed to run vminfod tokio runtime");
+    }
+}
diff --git a/vminfod/src/lib.rs b/vminfod/src/lib.rs
new file mode 100644
index 0000000..2b0e30d
--- /dev/null
+++ b/vminfod/src/lib.rs
@@ -0,0 +1,88 @@
+// This Source Code Form is subject to the terms of the Mozilla Public
+// License, v. 2.0. If a copy of the MPL was not distributed with this
+// file, You can obtain one at http://mozilla.org/MPL/2.0/.
+
+// Copyright 2019 Joyent, Inc.
+
+pub mod client;
+pub mod linefeed;
+
+use std::thread;
+
+#[macro_use]
+extern crate log;
+
+use client::Client;
+use serde::Deserialize;
+
+#[derive(Deserialize, Debug)]
+#[serde(tag = "type")]
+pub enum VminfodEvent {
+    #[serde(rename = "ready")]
+    Ready(ReadyEvent),
+    #[serde(rename = "create")]
+    Create(CreateEvent),
+    #[serde(rename = "modify")]
+    Modify(ModifyEvent),
+    #[serde(rename = "delete")]
+    Delete(DeleteEvent),
+}
+
+#[derive(Deserialize, Debug)]
+pub struct ReadyEvent {
+    pub vms: String,
+}
+
+#[derive(Deserialize, Debug)]
+pub struct CreateEvent {
+    pub vm: Zone,
+}
+
+#[derive(Deserialize, Debug)]
+pub struct ModifyEvent {
+    pub vm: Zone,
+    pub changes: Vec<Changes>,
+}
+
+#[derive(Deserialize, Debug)]
+pub struct DeleteEvent {
+    pub zonename: String,
+    pub uuid: String,
+}
+
+#[derive(Deserialize, Debug)]
+pub struct Zone {
+    pub uuid: String,
+    pub alias: Option<String>,
+    pub owner_uuid: String,
+    pub firewall_enabled: bool,
+    pub zonedid: u32,
+}
+
+#[derive(Deserialize, Debug)]
+pub struct Changes {
+    pub path: Vec<Option<String>>,
+}
+
+/// Starts a new thread that runs a tokio executor/runtime responsible for watching a vminfod event
+/// stream and sending corresponding events back over the receive half of a channel
+pub fn start_vminfod_stream() -> (
+    crossbeam_channel::Receiver<VminfodEvent>,
+    thread::JoinHandle<()>,
+) {
+    // We allow up to 10 events to be buffered
+    const NUM_EVENTS_BUFFERED: usize = 10;
+
+    let (tx, rx) = crossbeam_channel::bounded(NUM_EVENTS_BUFFERED);
+
+    (
+        rx,
+        thread::Builder::new()
+            .name("vminfod_client".to_string())
+            .spawn(move || {
+                let c = Client::new(tx);
+                c.run();
+            })
+            .expect("vminfod client thread spawn failed."),
+    )
+}
diff --git a/vminfod/src/linefeed.rs b/vminfod/src/linefeed.rs
new file mode 100644
index 0000000..d369105
--- /dev/null
+++ b/vminfod/src/linefeed.rs
@@ -0,0 +1,131 @@
+// This Source Code Form is subject to the terms of the Mozilla Public
+// License, v. 2.0. If a copy of the MPL was not distributed with this
+// file, You can obtain one at http://mozilla.org/MPL/2.0/.
+
+// Copyright 2019 Joyent, Inc.
+
+use std::mem::replace;
+use std::string::FromUtf8Error;
+
+use futures::stream::Fuse;
+use futures::{try_ready, Async, Poll, Stream};
+
+/// The following code is a modified version of:
+/// https://play.rust-lang.org/?gist=971e438cabd6f91efb76b7e45b15edf3&version=stable
+///
+/// The original author of that snippet is:
+/// https://github.com/hyperium/hyper/issues/1335#issuecomment-331682968
+
+#[derive(Debug)]
+pub struct Lines<S: Stream> {
+    buffered: Option<Vec<u8>>,
+    stream: Fuse<S>,
+}
+
+impl<S: Stream> Lines<S> {
+    pub fn new(stream: S) -> Lines<S> {
+        Lines {
+            buffered: None,
+            stream: stream.fuse(),
+        }
+    }
+
+    fn process(&mut self, flush: bool) -> Option<Result<String, FromUtf8Error>> {
+        let buffered = replace(&mut self.buffered, None);
+        if let Some(ref buffer) = buffered {
+            let mut split = buffer.splitn(2, |c| *c == b'\n');
+            if let Some(first) = split.next() {
+                if let Some(second) = split.next() {
+                    replace(&mut self.buffered, Some(second.to_vec()));
+                    return Some(String::from_utf8(first.to_vec()));
+                } else if flush {
+                    return Some(String::from_utf8(first.to_vec()));
+                }
+            }
+        }
+        replace(&mut self.buffered, buffered);
+        None
+    }
+}
+
+impl<S> Stream for Lines<S>
+where
+    S: Stream,
+    S::Item: AsRef<[u8]>,
+    S::Error: From<FromUtf8Error>,
+{
+    type Item = String;
+    type Error = S::Error;
+
+    fn poll(&mut self) -> Poll<Option<String>, S::Error> {
+        // It's important that we loop here so that we only return Async::NotReady when our inner
+        // Stream does.  Otherwise the current task will never be polled again and things will
+        // stall
+        loop {
+            match try_ready!(self.stream.poll()) {
+                // We got a chunk of data from the inner stream
+                Some(chunk) => {
+                    if let Some(ref mut buffer) = self.buffered {
+                        buffer.extend(chunk.as_ref());
+                    } else {
+                        self.buffered = Some(chunk.as_ref().to_vec());
+                    }
+                    match self.process(false) {
+                        Some(Ok(line)) => return Ok(Async::Ready(Some(line))),
+                        Some(Err(err)) => return Err(err.into()),
+                        None => (),
+                    }
+                }
+                // The inner stream has finished
+                None => match self.process(true) {
+                    Some(Ok(line)) => return Ok(Async::Ready(Some(line))),
+                    Some(Err(err)) => return Err(err.into()),
+                    None => return Ok(Async::Ready(None)),
+                },
+            }
+        }
+    }
+}
+
+#[cfg(test)]
+mod linefeed_tests {
+    use super::*;
+    use futures::stream::iter_ok;
+    use futures::Async;
+
+    #[test]
+    // test that `Lines` is able to buffer and split by new lines
+    fn test_lines() {
+        let chunks = vec![
+            "hello",
+            " world\n",
+            "good\nbye\n",
+            "world\n",
+            "escaped\\n\n",
+        ];
+        let stream = iter_ok::<_, FromUtf8Error>(chunks);
+        let mut lines = Lines::new(stream);
+
+        // iter_ok gives us an iterator that is always Ready
+        assert_eq!(
+            lines.poll().unwrap(),
+            Async::Ready(Some("hello world".to_string()))
+        );
+        assert_eq!(
+            lines.poll().unwrap(),
+            Async::Ready(Some("good".to_string()))
+        );
+        assert_eq!(lines.poll().unwrap(), Async::Ready(Some("bye".to_string())));
+        assert_eq!(
+            lines.poll().unwrap(),
+            Async::Ready(Some("world".to_string()))
+        );
+        assert_eq!(
+            lines.poll().unwrap(),
+            Async::Ready(Some("escaped\\n".to_string()))
+        );
+        assert_eq!(lines.poll().unwrap(), Async::Ready(Some("".to_string())));
+        assert_eq!(lines.poll().unwrap(), Async::Ready(None));
+    }
+
+}
